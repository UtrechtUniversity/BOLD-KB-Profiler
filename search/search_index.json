{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BOLD \u22ee Knowledge Graph Exploration and Analysis platform Features \u2022 Quick Installation \u2022 Documentation \u2022 Demo Features Seamless import of Knowledge Bases from LOD Cloud and Triply DB Interact with external SPARQL endpoints Create persistent reports and share them with others Run SPARQL or pre-built analysis queries Explore knowledge graph with interactive visualizations Pick unseen terms with fuzzy search Demo A live demo of BOLD can be found here . Log in with the following credentials: * Username: demo * Password: demodemo Documentation Visit BOLD documentation for more information. Quick Installation Note: StarDog is free but requires a license. Request a free license at stardog.com . Place the license at dev/stardog/stardog-license-key.bin in the project root. You can quickly spin up a BOLD instance using Docker . Create a docker-compose.yml file with the following contents: version : '3' services : bold : image : egordm/bold:latest ports : - 8000:8000 volumes : - ./storage:/storage - ./backend/.env:/app/.env networks : - bold-net links : - postgres - stardog depends_on : - postgres - stardog postgres : image : egordm/postgres-multidb:latest environment : POSTGRES_USER : root POSTGRES_PASSWORD : helloworld POSTGRES_MULTIPLE_DATABASES : test,develop,production ports : - 5432:5432 volumes : - data-postgres:/var/lib/postgresql/data networks : - bold-net stardog : image : stardog/stardog:7.9.1-java11-preview userns_mode : host ports : - 5820:5820 volumes : - data-stardog:/var/opt/stardog - ./dev/stardog/stardog-license-key.bin:/var/opt/stardog/stardog-license-key.bin - ./storage/import:/var/data/import - ./storage/downloads:/var/data/downloads - ./storage/export:/var/data/export environment : STARDOG_SERVER_JAVA_ARGS : \"-Xmx8g -Xms8g -XX:MaxDirectMemorySize=12g\" networks : - bold-net volumes : data-stardog : data-postgres : networks : bold-net : Then run docker-compose up -d to start the container. You can now access BOLD at http://localhost:8000 .","title":"Home"},{"location":"#features","text":"Seamless import of Knowledge Bases from LOD Cloud and Triply DB Interact with external SPARQL endpoints Create persistent reports and share them with others Run SPARQL or pre-built analysis queries Explore knowledge graph with interactive visualizations Pick unseen terms with fuzzy search","title":"Features"},{"location":"#demo","text":"A live demo of BOLD can be found here . Log in with the following credentials: * Username: demo * Password: demodemo","title":"Demo"},{"location":"#documentation","text":"Visit BOLD documentation for more information.","title":"Documentation"},{"location":"#quick-installation","text":"Note: StarDog is free but requires a license. Request a free license at stardog.com . Place the license at dev/stardog/stardog-license-key.bin in the project root. You can quickly spin up a BOLD instance using Docker . Create a docker-compose.yml file with the following contents: version : '3' services : bold : image : egordm/bold:latest ports : - 8000:8000 volumes : - ./storage:/storage - ./backend/.env:/app/.env networks : - bold-net links : - postgres - stardog depends_on : - postgres - stardog postgres : image : egordm/postgres-multidb:latest environment : POSTGRES_USER : root POSTGRES_PASSWORD : helloworld POSTGRES_MULTIPLE_DATABASES : test,develop,production ports : - 5432:5432 volumes : - data-postgres:/var/lib/postgresql/data networks : - bold-net stardog : image : stardog/stardog:7.9.1-java11-preview userns_mode : host ports : - 5820:5820 volumes : - data-stardog:/var/opt/stardog - ./dev/stardog/stardog-license-key.bin:/var/opt/stardog/stardog-license-key.bin - ./storage/import:/var/data/import - ./storage/downloads:/var/data/downloads - ./storage/export:/var/data/export environment : STARDOG_SERVER_JAVA_ARGS : \"-Xmx8g -Xms8g -XX:MaxDirectMemorySize=12g\" networks : - bold-net volumes : data-stardog : data-postgres : networks : bold-net : Then run docker-compose up -d to start the container. You can now access BOLD at http://localhost:8000 .","title":"Quick Installation"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines If you are a first time contributor, start by reading this fantastic guide . Read the docs Get familliar with the stack Backend: Django Celery Stardog Frontend: React React Router MUI Development setup For development purposes, you need to have a working Python and Rust installation. Set up local databases (See database setup ) Install Rust and Python Build BOLD cli tools: make release-tools Install Poetry Install necessary dependencies: poetry install Run worker application: make start_worker Run backend server: make start_backend Run docs server: make start_docs Open the BOLD web interface: http://localhost:8000/ For further information proceed to Architechture guide .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"If you are a first time contributor, start by reading this fantastic guide .","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#read-the-docs","text":"","title":"Read the docs"},{"location":"CONTRIBUTING/#get-familliar-with-the-stack","text":"","title":"Get familliar with the stack"},{"location":"CONTRIBUTING/#backend","text":"Django Celery Stardog","title":"Backend:"},{"location":"CONTRIBUTING/#frontend","text":"React React Router MUI","title":"Frontend:"},{"location":"CONTRIBUTING/#development-setup","text":"For development purposes, you need to have a working Python and Rust installation. Set up local databases (See database setup ) Install Rust and Python Build BOLD cli tools: make release-tools Install Poetry Install necessary dependencies: poetry install Run worker application: make start_worker Run backend server: make start_backend Run docs server: make start_docs Open the BOLD web interface: http://localhost:8000/ For further information proceed to Architechture guide .","title":"Development setup"},{"location":"architecture/","text":"BOLD Architecture This document is currently aimed at being a quick introduction to help a potentially new contributor understand how to navigate this project. The BOLD architecture is designed to be scaleable and to handle datasets of all sizes. Therefore we keep the data and the code separate. Additionally, we define asynchronous execution architecture to allow for parallel execution of queries, dataset imports and fault tolerance. A high level overview of the architecture can be seen in the following figure. In the above image we see the described separation. Keeping the analysis and visualization extensible is another important goal of the BOLD platform. To achieve this we aim to separete the analysis and visualization from the execution logic by strictly assigning these roles to the frontend and backend, respectively. The backend is responsible for the execution of queries and the data management. It recieves an execution plan in form a report and report cells containing sparql queries and executes them in parallel given the dataset. Similarly, the import logic does not work with third party services, but with rdf or sparql endpoint urls directly. The data is either stored in a self-hosted local data cluster or is accessed in a read-only fashion from a external data clusters using SPARQL and elasticsearch endpoints. State of the workers is decoupled from the backend server and vice-versa for fault tolerance. Import Flow While the import flow varies depending on the dataset and configuration, the general flow can be seen in the figure below. The dataset urls are retrieved from a third party service ( Linked Open Data Cloud or TriplyDB ) and sent to the backend for import. The backend stores the import metadata and schedules an asynchronous import job which is passed to one of the worker nodes (see celery ). The worker node is responsible for importing the dataset (if applicable) and importing it into the graph database ( StarDog ). Once the import is complete a search index is created by collecting all the entities from the dataset. Additionally, various queries are run to compute the dataset statistics and check the health of the dataset. Finally, the job results are persisted and the webserver is notified. Report Flow The report flow is inspired by the jupyter notebooks workflow. We have chosen to use our own workflow to allow easy addition of custom widgets, fuzzy term searching, self-hosting, and custom dataset import. Flow for a simple report cell execution is shown in the figure below. Report executions consists of two parts. (1) We save the report to run the up-to-date cells. (2) We execute the cells in parallel. Each cell execution job is passed onto a worker node (see celery ). The tasks query graph database and search the database. Once the result is known, it is saved to the database and the webserver (as well as client) is notified.","title":"Architecture"},{"location":"architecture/#bold-architecture","text":"This document is currently aimed at being a quick introduction to help a potentially new contributor understand how to navigate this project. The BOLD architecture is designed to be scaleable and to handle datasets of all sizes. Therefore we keep the data and the code separate. Additionally, we define asynchronous execution architecture to allow for parallel execution of queries, dataset imports and fault tolerance. A high level overview of the architecture can be seen in the following figure. In the above image we see the described separation. Keeping the analysis and visualization extensible is another important goal of the BOLD platform. To achieve this we aim to separete the analysis and visualization from the execution logic by strictly assigning these roles to the frontend and backend, respectively. The backend is responsible for the execution of queries and the data management. It recieves an execution plan in form a report and report cells containing sparql queries and executes them in parallel given the dataset. Similarly, the import logic does not work with third party services, but with rdf or sparql endpoint urls directly. The data is either stored in a self-hosted local data cluster or is accessed in a read-only fashion from a external data clusters using SPARQL and elasticsearch endpoints. State of the workers is decoupled from the backend server and vice-versa for fault tolerance.","title":"BOLD Architecture"},{"location":"architecture/#import-flow","text":"While the import flow varies depending on the dataset and configuration, the general flow can be seen in the figure below. The dataset urls are retrieved from a third party service ( Linked Open Data Cloud or TriplyDB ) and sent to the backend for import. The backend stores the import metadata and schedules an asynchronous import job which is passed to one of the worker nodes (see celery ). The worker node is responsible for importing the dataset (if applicable) and importing it into the graph database ( StarDog ). Once the import is complete a search index is created by collecting all the entities from the dataset. Additionally, various queries are run to compute the dataset statistics and check the health of the dataset. Finally, the job results are persisted and the webserver is notified.","title":"Import Flow"},{"location":"architecture/#report-flow","text":"The report flow is inspired by the jupyter notebooks workflow. We have chosen to use our own workflow to allow easy addition of custom widgets, fuzzy term searching, self-hosting, and custom dataset import. Flow for a simple report cell execution is shown in the figure below. Report executions consists of two parts. (1) We save the report to run the up-to-date cells. (2) We execute the cells in parallel. Each cell execution job is passed onto a worker node (see celery ). The tasks query graph database and search the database. Once the result is known, it is saved to the database and the webserver (as well as client) is notified.","title":"Report Flow"},{"location":"deployment/","text":"Deployment It is advised to deploy BOLD using preconfigured docker containers. Note: StarDog is free but requires a license. Request a free license at stardog.com . Place the license at dev/stardog/stardog-license-key.bin in the project root. Create a docker-compose.yml file with the following contents: version : '3' services : bold : image : egordm/bold:latest ports : - 8000:8000 volumes : - ./storage:/storage - ./backend/.env:/app/.env networks : - bold-net links : - postgres - stardog depends_on : - postgres - stardog postgres : image : egordm/postgres-multidb:latest environment : POSTGRES_USER : root POSTGRES_PASSWORD : helloworld POSTGRES_MULTIPLE_DATABASES : test,develop,production ports : - 5432:5432 volumes : - data-postgres:/var/lib/postgresql/data networks : - bold-net stardog : image : stardog/stardog:7.9.1-java11-preview userns_mode : host ports : - 5820:5820 volumes : - data-stardog:/var/opt/stardog - ./dev/stardog/stardog-license-key.bin:/var/opt/stardog/stardog-license-key.bin - ./storage/import:/var/data/import - ./storage/downloads:/var/data/downloads - ./storage/export:/var/data/export environment : STARDOG_SERVER_JAVA_ARGS : \"-Xmx8g -Xms8g -XX:MaxDirectMemorySize=12g\" networks : - bold-net volumes : data-stardog : data-postgres : networks : bold-net : Create a backend/.env file with the following contents: OPENAPI_KEY=\"(optional) key for the openai api\" DEBUG=off STARDOG_ENABLE=on Change STARDOG_ENABLE to off if you don't want to use stardog. Then run docker-compose up -d to start the container. You can now access BOLD at http://localhost:8000 . System Requirements The server requirements are mostly bound by the Stardog database. You can choose to not use the Stardog database, but you will not be able to import the full datasets (only external SPARQL endpoints are allowed). Moreover you can decide to run Stardog on a different machine. You must have twice the amount of storage your datasets require. (YAGO is 60Gb thus 120Gb) You must allocate at least 2 cores for the server. Memory requirements are found below: Number of Triples Total System Memory 100 million 8G 1 billion 32G 10 billion 128G 25 billion 256G 50 billion 512G","title":"Deployment"},{"location":"deployment/#deployment","text":"It is advised to deploy BOLD using preconfigured docker containers. Note: StarDog is free but requires a license. Request a free license at stardog.com . Place the license at dev/stardog/stardog-license-key.bin in the project root. Create a docker-compose.yml file with the following contents: version : '3' services : bold : image : egordm/bold:latest ports : - 8000:8000 volumes : - ./storage:/storage - ./backend/.env:/app/.env networks : - bold-net links : - postgres - stardog depends_on : - postgres - stardog postgres : image : egordm/postgres-multidb:latest environment : POSTGRES_USER : root POSTGRES_PASSWORD : helloworld POSTGRES_MULTIPLE_DATABASES : test,develop,production ports : - 5432:5432 volumes : - data-postgres:/var/lib/postgresql/data networks : - bold-net stardog : image : stardog/stardog:7.9.1-java11-preview userns_mode : host ports : - 5820:5820 volumes : - data-stardog:/var/opt/stardog - ./dev/stardog/stardog-license-key.bin:/var/opt/stardog/stardog-license-key.bin - ./storage/import:/var/data/import - ./storage/downloads:/var/data/downloads - ./storage/export:/var/data/export environment : STARDOG_SERVER_JAVA_ARGS : \"-Xmx8g -Xms8g -XX:MaxDirectMemorySize=12g\" networks : - bold-net volumes : data-stardog : data-postgres : networks : bold-net : Create a backend/.env file with the following contents: OPENAPI_KEY=\"(optional) key for the openai api\" DEBUG=off STARDOG_ENABLE=on Change STARDOG_ENABLE to off if you don't want to use stardog. Then run docker-compose up -d to start the container. You can now access BOLD at http://localhost:8000 .","title":"Deployment"},{"location":"deployment/#system-requirements","text":"The server requirements are mostly bound by the Stardog database. You can choose to not use the Stardog database, but you will not be able to import the full datasets (only external SPARQL endpoints are allowed). Moreover you can decide to run Stardog on a different machine. You must have twice the amount of storage your datasets require. (YAGO is 60Gb thus 120Gb) You must allocate at least 2 cores for the server. Memory requirements are found below: Number of Triples Total System Memory 100 million 8G 1 billion 32G 10 billion 128G 25 billion 256G 50 billion 512G","title":"System Requirements"},{"location":"installation/","text":"Installation This document is describes how to install BOLD for typical usage. If you want to install BOLD for development purposes, we refer you to the CONTRIBUTING section. The BOLD platform depends on postgresql and stardog databases for knowledge graph and state storage. In the following steps we discuss their setup as well as necessary steps to get BOLD up and running. Database Setup If you have already postgresql and stardog databases installed, you can skip this step. In this step we discuss their setup as docker containers. Note: StarDog is free but requires a license. Request a free license at stardog.com . Place the license as dev/stardog/stardog-license-key.bin in the project root. Install Docker and Docker Compose Build the Docker images: docker-compose build Start the database services: docker-compose up Docker setup If you want to install BOLD for development purposes, we refer you to the CONTRIBUTING section. In this step we describe steps on how to run BOLD as a docker container.' Build docker images: docker-compose -f docker-compose.full.yml build Start BOLD and the relevant services: docker-compose -f docker-compose.full.yml up Open the BOLD web interface: http://127.0.0.1:8000/ Build docker image from source (optional) Build docker image from source if you want to run newest or modified version of BOLD. In this step we describe steps on how to run BOLD as a docker container.' Build docker images: docker-compose -f docker-compose.yml -f docker-compose.standalone.yml build Start BOLD and the relevant services: docker-compose -f docker-compose.yml -f docker-compose.standalone.yml up Open the BOLD web interface: http://127.0.0.1:8000/ Enabling GPT code completion To enable code completion with GPT you need an OpenAI API key. If you don't have one, you can request one at openai.com . Once you have a key, create file backend/.env and add the following line: OPENAPI_KEY=<your-key> Configuration You can configure your bold installation by creating backend/.env file and setting following variables: DEBUG : Set to False to disable debug mode. STARDOG_ENABLE : Set to False to disable local dataset downloads and search indexing. This is useful if you don't have a stardog instance running. DJANGO_SUPERUSER_USERNAME : Set to the username of the superuser. (default: admin ) DJANGO_SUPERUSER_PASSWORD : Set to the password of the superuser. (default: admin ) DJANGO_SUPERUSER_EMAIL : Set to the email of the superuser.","title":"Installation"},{"location":"installation/#installation","text":"This document is describes how to install BOLD for typical usage. If you want to install BOLD for development purposes, we refer you to the CONTRIBUTING section. The BOLD platform depends on postgresql and stardog databases for knowledge graph and state storage. In the following steps we discuss their setup as well as necessary steps to get BOLD up and running.","title":"Installation"},{"location":"installation/#database-setup","text":"If you have already postgresql and stardog databases installed, you can skip this step. In this step we discuss their setup as docker containers. Note: StarDog is free but requires a license. Request a free license at stardog.com . Place the license as dev/stardog/stardog-license-key.bin in the project root. Install Docker and Docker Compose Build the Docker images: docker-compose build Start the database services: docker-compose up","title":"Database Setup"},{"location":"installation/#docker-setup","text":"If you want to install BOLD for development purposes, we refer you to the CONTRIBUTING section. In this step we describe steps on how to run BOLD as a docker container.' Build docker images: docker-compose -f docker-compose.full.yml build Start BOLD and the relevant services: docker-compose -f docker-compose.full.yml up Open the BOLD web interface: http://127.0.0.1:8000/","title":"Docker setup"},{"location":"installation/#build-docker-image-from-source-optional","text":"Build docker image from source if you want to run newest or modified version of BOLD. In this step we describe steps on how to run BOLD as a docker container.' Build docker images: docker-compose -f docker-compose.yml -f docker-compose.standalone.yml build Start BOLD and the relevant services: docker-compose -f docker-compose.yml -f docker-compose.standalone.yml up Open the BOLD web interface: http://127.0.0.1:8000/","title":"Build docker image from source (optional)"},{"location":"installation/#enabling-gpt-code-completion","text":"To enable code completion with GPT you need an OpenAI API key. If you don't have one, you can request one at openai.com . Once you have a key, create file backend/.env and add the following line: OPENAPI_KEY=<your-key>","title":"Enabling GPT code completion"},{"location":"installation/#configuration","text":"You can configure your bold installation by creating backend/.env file and setting following variables: DEBUG : Set to False to disable debug mode. STARDOG_ENABLE : Set to False to disable local dataset downloads and search indexing. This is useful if you don't have a stardog instance running. DJANGO_SUPERUSER_USERNAME : Set to the username of the superuser. (default: admin ) DJANGO_SUPERUSER_PASSWORD : Set to the password of the superuser. (default: admin ) DJANGO_SUPERUSER_EMAIL : Set to the email of the superuser.","title":"Configuration"},{"location":"structure/","text":"Project Structure This document is aimed to be a quick introduction to help a potentially new contributor understand how to navigate this project. File Organization The projects files are organized in the following way: backend : Contains the Django backend and worker source code. dev : Contains the development scripts and docker configs. docs : Contains the mkdocs documentation. frontend : Contains the react frontend source code. storage : Is the directory where the BOLD runtime files are stored. tools : Contains rust based BOLD cli tools for RDF preprocessing and search indexing. Backend Structure The BOLD backend is split into multiple modules that are responsible for different parts of the BOLD system. backend : Contains the Django and celery configuration. datasets : Is responsible for all dataset related operations such as import and management. frontend : Contains static files generated by the react frontend. reports : Is responsible for all report related operations such as management and cell execution. shared : Contains shared utility code used by all the modules. tasks : Contains framework for asynchronous execution and tracking of tasks. users : Contains all user related operations such as authentication and management. Frontend Structure The BOLD frontend follows standard react and typescript conventions for code organization. components : Contains all the UI components which can be used in any context. containers : Contains all the UI containers which combine multiple components into combined user interfaces. hooks : Contains hooks for data from state extraction. pages : Holds complete pages used in navigation and frontend. providers : Contains all the data providers and hooks used for managing state of the application. services : Holds service apis for the backend, LODC and TriplyDB. theme : Contains the mui theme configuration. types : Contains typescript type definitions for data structures used in the application. utils : Contains utility functions. App.tsx : The main entry point for the application. Models Dataset - The dataset model is used to manage the data sources and local datasets. It stores handles to local and remote resources. Report - The report model is used to manage the reports. It stores the cells and the results of the cells in the notebook json field. Task - The task model represents a scheduled task. It stores a handle to the celery task and the information about the task status.","title":"Structure"},{"location":"structure/#project-structure","text":"This document is aimed to be a quick introduction to help a potentially new contributor understand how to navigate this project.","title":"Project Structure"},{"location":"structure/#file-organization","text":"The projects files are organized in the following way: backend : Contains the Django backend and worker source code. dev : Contains the development scripts and docker configs. docs : Contains the mkdocs documentation. frontend : Contains the react frontend source code. storage : Is the directory where the BOLD runtime files are stored. tools : Contains rust based BOLD cli tools for RDF preprocessing and search indexing.","title":"File Organization"},{"location":"structure/#backend-structure","text":"The BOLD backend is split into multiple modules that are responsible for different parts of the BOLD system. backend : Contains the Django and celery configuration. datasets : Is responsible for all dataset related operations such as import and management. frontend : Contains static files generated by the react frontend. reports : Is responsible for all report related operations such as management and cell execution. shared : Contains shared utility code used by all the modules. tasks : Contains framework for asynchronous execution and tracking of tasks. users : Contains all user related operations such as authentication and management.","title":"Backend Structure"},{"location":"structure/#frontend-structure","text":"The BOLD frontend follows standard react and typescript conventions for code organization. components : Contains all the UI components which can be used in any context. containers : Contains all the UI containers which combine multiple components into combined user interfaces. hooks : Contains hooks for data from state extraction. pages : Holds complete pages used in navigation and frontend. providers : Contains all the data providers and hooks used for managing state of the application. services : Holds service apis for the backend, LODC and TriplyDB. theme : Contains the mui theme configuration. types : Contains typescript type definitions for data structures used in the application. utils : Contains utility functions. App.tsx : The main entry point for the application.","title":"Frontend Structure"},{"location":"structure/#models","text":"Dataset - The dataset model is used to manage the data sources and local datasets. It stores handles to local and remote resources. Report - The report model is used to manage the reports. It stores the cells and the results of the cells in the notebook json field. Task - The task model represents a scheduled task. It stores a handle to the celery task and the information about the task status.","title":"Models"},{"location":"user_manual/","text":"User Manual This document is aimed at new users who want learn or get an overview of the BOLD platform features. Definitions Dataset A BOLD dataset represents an (imported) RDF dataset. The dataset consists of a SPARQL endpoint and a search index. Both can point to either a local of a remote resource. The datasets can be shared between users and reports. Report A BOLD report represents a collection of cells that contain SPARQL queries and widgets. A report also persists all the query and widget results so that they can be reviewed later. Report Cell A report cell contains either SPARQL queries or a configurable widget which generates queries for the database. Task A BOLD task represents a collection of work that can be scheduled and assigned to a worker. Tasks are meant to be used for long-running tasks and run in parallel to avoid blocking the main server. Basic Navigation Navigate through the app by using the sidebar. Reports - Page to manage the reports. Datasets - Page to manage the datasets. Tasks - Page to view scheduled/completed tasks. LODC - Browse the LODC datasets TriplyDB - Browse the TriplyDB datasets Filter the entities by defining custom filters and select specific columns to display. Task bar The task bar is located in the bottom right corner of the screen and shows the progress of currently running tasks. You can expand it to view the details of the running or completed tasks. Importing Datasets The datasets can be imported a multitude of sources. There are two main ways to import datasets in BOLD. RDF Dataset - Import an RDF dataset by importing it into a local dataset and creating a search index. This requires the machine to have sufficient memory and storage to store the dataset. SPARQL Endpoint - Create a dataset from SPARQL endpoint and using an external search index. This does not require machine to have enough storage to store the dataset as it is accessed directly from the endpoint. Importing from Linked Open Data Cloud Import a dataset from (linked open data cloud)[https://lod-cloud.net/datasets]. Since LODC stores links to external resources check whether the file is accessible before importing it by clicking the download link. Importing from Triply DB Import a dataset from (Triply DB)[https://triplydb.com/]. When importing a SPARQL endpoint dataset from TriplyDB check whether a sparql service and elastic search services are running by visiting the dataset page (\"services\" tab). Importing from SPARQL endpoints Importing from manual sparql endpoints currently only supports Wikimedia search api. If your dataset does not contain wikimedia data, then you are limited to simple code cell execution. Dataset Actions Dataset deletion deletes the local dataset and the search index. Creating a Report Create a report by selecting a dataset and adding cells to it. Report Cell Types Report consists of cells that query the dataset SPARQL and search index and display the results in a persistent way. Prefixes RDF iris are verbose and can be confusing to users. Define custom prefixes to shorten and make the iris more readable. Code Cell Code cells allow you to execute SPARQL queries. The editor is powered by yasqui . The defined prefixes can be used in autocompletion. See yasgui documentation to find out how to visualize your results in a more coherent way. Code Generation using GPT Writing SPARQL queries is a tedious task and is not always straightforward as it may require some knowledge of the underlying dataset. To make it easier to write SPARQL queries, BOLD provides a utility to promt GPT to generate SPARQL queries for you. The specified prompt as well as the defined prefixes are used as context for the generation. Class Browser Widget Class browser counts the number of instances for each class and displays them in a hierarchical structure. Each type in the plot is sized proportionally to the number of instances it contains. For larger datasets, it may be useful to increase the limit of the query to get a better overview of the class hierarchy. Histogram Widget The histogram widget displays the distribution of values for a certain property given a set of filters. This cell is perfect for finding biases in the data or getting important insights. Various options are available to customize the histogram: Group values of - property whose values are used in the histogram Filters - filters to reduce number of matching entities before the histogram is generated Continuous - whether property values are continuous (e.g. interger, decimal, date) or discrete Limit number of groups - limits the number of bins in the histogram Min group size - minimum number of entities required to generate a group in the histogram Temporal grouping - Adds a third dimension to the histogram by creating a histogram for each time period given a temporal property. The results are displayed in four tabs: Plot - The generated histogram Table - The data used to generate the histogram Examples - List of examples of entities that match the filters Completeness Analysis - Displays a ratio of entities that match the filters and have a defined property value versus the entities that don't have the property value. Note: Completeness analysis counts amount of entities, meaning that multiple triples corresponding to the same subject won't be counted twice. Query Builder Query Builder provides a simple interface to create SPARQL queries. The default search input allows you to fuzzy search for relevant entities and properties. Create filters to filter resulting entities and select the properties to be used in the plot. The results are displayed in a table below. Plot Builder The plot builder widget allows you to create custom plots using the data from the dataset. Create filters to filter resulting entities and select the properties to be used in the plot. If you want to build a 3d plot (x, y and groups) select the \"Is XYZ\" option. This widget is compatible with Query Builder widget and can be converted from/to Query Builder widget. Entity Properties The properties cell displays the properties of a given entity. Subgraph View The subgraph cell displays arbitrary depth the subgraph of a given entity by using properties as egdes. Various options are available to customize the subgraph: Target Entity - The entity to start the subgraph from Use any property - Include all applicable properties in the subgraph or specify specific properties to use Limit results - Limits the number of nodes displayed Limit depth - Limits the depth of the subgraph Class Statistics The class tree cell displays the class hierarchy of the classes and properties in the dataset. It can be used to get important insights in new data user wants to explore. Various options are available to customize the class tree: With counts - Count the number of entities for each class and property' With subclasses - Include class subclasses in the tree With equivalent classes - Include equivalent classes in the tree With keys - Include keys in the tree for other reources With properties - Include class properties in the tree Use owl classes - Include owl classes in the tree Use rdf classes - Include rdf classes in the tree Use any classes - Include all classes in the tree","title":"User Manual"},{"location":"user_manual/#user-manual","text":"This document is aimed at new users who want learn or get an overview of the BOLD platform features.","title":"User Manual"},{"location":"user_manual/#definitions","text":"","title":"Definitions"},{"location":"user_manual/#dataset","text":"A BOLD dataset represents an (imported) RDF dataset. The dataset consists of a SPARQL endpoint and a search index. Both can point to either a local of a remote resource. The datasets can be shared between users and reports.","title":"Dataset"},{"location":"user_manual/#report","text":"A BOLD report represents a collection of cells that contain SPARQL queries and widgets. A report also persists all the query and widget results so that they can be reviewed later.","title":"Report"},{"location":"user_manual/#report-cell","text":"A report cell contains either SPARQL queries or a configurable widget which generates queries for the database.","title":"Report Cell"},{"location":"user_manual/#task","text":"A BOLD task represents a collection of work that can be scheduled and assigned to a worker. Tasks are meant to be used for long-running tasks and run in parallel to avoid blocking the main server.","title":"Task"},{"location":"user_manual/#basic-navigation","text":"Navigate through the app by using the sidebar. Reports - Page to manage the reports. Datasets - Page to manage the datasets. Tasks - Page to view scheduled/completed tasks. LODC - Browse the LODC datasets TriplyDB - Browse the TriplyDB datasets Filter the entities by defining custom filters and select specific columns to display.","title":"Basic Navigation"},{"location":"user_manual/#task-bar","text":"The task bar is located in the bottom right corner of the screen and shows the progress of currently running tasks. You can expand it to view the details of the running or completed tasks.","title":"Task bar"},{"location":"user_manual/#importing-datasets","text":"The datasets can be imported a multitude of sources. There are two main ways to import datasets in BOLD. RDF Dataset - Import an RDF dataset by importing it into a local dataset and creating a search index. This requires the machine to have sufficient memory and storage to store the dataset. SPARQL Endpoint - Create a dataset from SPARQL endpoint and using an external search index. This does not require machine to have enough storage to store the dataset as it is accessed directly from the endpoint.","title":"Importing Datasets"},{"location":"user_manual/#importing-from-linked-open-data-cloud","text":"Import a dataset from (linked open data cloud)[https://lod-cloud.net/datasets]. Since LODC stores links to external resources check whether the file is accessible before importing it by clicking the download link.","title":"Importing from Linked Open Data Cloud"},{"location":"user_manual/#importing-from-triply-db","text":"Import a dataset from (Triply DB)[https://triplydb.com/]. When importing a SPARQL endpoint dataset from TriplyDB check whether a sparql service and elastic search services are running by visiting the dataset page (\"services\" tab).","title":"Importing from Triply DB"},{"location":"user_manual/#importing-from-sparql-endpoints","text":"Importing from manual sparql endpoints currently only supports Wikimedia search api. If your dataset does not contain wikimedia data, then you are limited to simple code cell execution.","title":"Importing from SPARQL endpoints"},{"location":"user_manual/#dataset-actions","text":"Dataset deletion deletes the local dataset and the search index.","title":"Dataset Actions"},{"location":"user_manual/#creating-a-report","text":"Create a report by selecting a dataset and adding cells to it.","title":"Creating a Report"},{"location":"user_manual/#report-cell-types","text":"Report consists of cells that query the dataset SPARQL and search index and display the results in a persistent way.","title":"Report Cell Types"},{"location":"user_manual/#prefixes","text":"RDF iris are verbose and can be confusing to users. Define custom prefixes to shorten and make the iris more readable.","title":"Prefixes"},{"location":"user_manual/#code-cell","text":"Code cells allow you to execute SPARQL queries. The editor is powered by yasqui . The defined prefixes can be used in autocompletion. See yasgui documentation to find out how to visualize your results in a more coherent way.","title":"Code Cell"},{"location":"user_manual/#code-generation-using-gpt","text":"Writing SPARQL queries is a tedious task and is not always straightforward as it may require some knowledge of the underlying dataset. To make it easier to write SPARQL queries, BOLD provides a utility to promt GPT to generate SPARQL queries for you. The specified prompt as well as the defined prefixes are used as context for the generation.","title":"Code Generation using GPT"},{"location":"user_manual/#class-browser-widget","text":"Class browser counts the number of instances for each class and displays them in a hierarchical structure. Each type in the plot is sized proportionally to the number of instances it contains. For larger datasets, it may be useful to increase the limit of the query to get a better overview of the class hierarchy.","title":"Class Browser Widget"},{"location":"user_manual/#histogram-widget","text":"The histogram widget displays the distribution of values for a certain property given a set of filters. This cell is perfect for finding biases in the data or getting important insights. Various options are available to customize the histogram: Group values of - property whose values are used in the histogram Filters - filters to reduce number of matching entities before the histogram is generated Continuous - whether property values are continuous (e.g. interger, decimal, date) or discrete Limit number of groups - limits the number of bins in the histogram Min group size - minimum number of entities required to generate a group in the histogram Temporal grouping - Adds a third dimension to the histogram by creating a histogram for each time period given a temporal property. The results are displayed in four tabs: Plot - The generated histogram Table - The data used to generate the histogram Examples - List of examples of entities that match the filters Completeness Analysis - Displays a ratio of entities that match the filters and have a defined property value versus the entities that don't have the property value. Note: Completeness analysis counts amount of entities, meaning that multiple triples corresponding to the same subject won't be counted twice.","title":"Histogram Widget"},{"location":"user_manual/#query-builder","text":"Query Builder provides a simple interface to create SPARQL queries. The default search input allows you to fuzzy search for relevant entities and properties. Create filters to filter resulting entities and select the properties to be used in the plot. The results are displayed in a table below.","title":"Query Builder"},{"location":"user_manual/#plot-builder","text":"The plot builder widget allows you to create custom plots using the data from the dataset. Create filters to filter resulting entities and select the properties to be used in the plot. If you want to build a 3d plot (x, y and groups) select the \"Is XYZ\" option. This widget is compatible with Query Builder widget and can be converted from/to Query Builder widget.","title":"Plot Builder"},{"location":"user_manual/#entity-properties","text":"The properties cell displays the properties of a given entity.","title":"Entity Properties"},{"location":"user_manual/#subgraph-view","text":"The subgraph cell displays arbitrary depth the subgraph of a given entity by using properties as egdes. Various options are available to customize the subgraph: Target Entity - The entity to start the subgraph from Use any property - Include all applicable properties in the subgraph or specify specific properties to use Limit results - Limits the number of nodes displayed Limit depth - Limits the depth of the subgraph","title":"Subgraph View"},{"location":"user_manual/#class-statistics","text":"The class tree cell displays the class hierarchy of the classes and properties in the dataset. It can be used to get important insights in new data user wants to explore. Various options are available to customize the class tree: With counts - Count the number of entities for each class and property' With subclasses - Include class subclasses in the tree With equivalent classes - Include equivalent classes in the tree With keys - Include keys in the tree for other reources With properties - Include class properties in the tree Use owl classes - Include owl classes in the tree Use rdf classes - Include rdf classes in the tree Use any classes - Include all classes in the tree","title":"Class Statistics"},{"location":"reference/SUMMARY/","text":"datasets management commands import_kg models serializers services bold_cli query search stardog_api tasks imports maintenance pipeline processing views datasets lodc reports consumers models permissions serializers tasks views shared dict logging models paths query random shell websocket tasks apps consumers management commands celery_worker models serializers signals utils views users middleware mixins permissions serializers views","title":"SUMMARY"},{"location":"reference/datasets/","text":"","title":"datasets"},{"location":"reference/datasets/models/","text":"Dataset Bases: TaskMixin , TimeStampMixin , OwnableMixin The internal dataset model. Source code in datasets/models.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class Dataset ( TaskMixin , TimeStampMixin , OwnableMixin ): \"\"\" The internal dataset model. \"\"\" STATES = (( state . value , state . value ) for state in DatasetState ) class Mode ( models . TextChoices ): \"\"\" The Mode class is an enumeration of the possible modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) SPARQL = 'SPARQL' , _ ( 'From SPARQL endpoint' ) class SearchMode ( models . TextChoices ): \"\"\" The SearchMode class is an enumeration of the possible search modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) WIKIDATA = 'WIKIDATA' , _ ( 'From Wikidata' ) TRIPLYDB = 'TRIPLYDB' , _ ( 'From TripyDB' ) id = models . UUIDField ( default = uuid . uuid4 , primary_key = True ) \"\"\"The identifier of the dataset.\"\"\" name = models . CharField ( max_length = 255 ) \"\"\"The name of the dataset.\"\"\" description = models . TextField ( blank = True ) \"\"\"The description of the dataset.\"\"\" source = models . JSONField () \"\"\"The source of the dataset.\"\"\" mode = models . CharField ( max_length = 255 , choices = Mode . choices , default = Mode . LOCAL ) \"\"\"The mode of the dataset.\"\"\" search_mode = models . CharField ( max_length = 255 , choices = SearchMode . choices , default = SearchMode . LOCAL ) \"\"\"The search mode of the dataset.\"\"\" creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) \"\"\"The user who created the dataset.\"\"\" local_database = models . CharField ( max_length = 255 , null = True ) \"\"\"The local stardog database identifier of the dataset.\"\"\" sparql_endpoint = models . CharField ( max_length = 255 , null = True ) \"\"\"The SPARQL endpoint of the dataset.\"\"\" statistics = models . JSONField ( null = True ) \"\"\"The statistics of the dataset.\"\"\" namespaces = models . JSONField ( null = True ) \"\"\"The list of sparql namespaces/prefixes in the dataset.\"\"\" state = models . CharField ( choices = STATES , default = DatasetState . QUEUED . value , max_length = 255 ) \"\"\"The import state of the dataset.\"\"\" import_task = models . OneToOneField ( 'tasks.Task' , on_delete = models . SET_NULL , null = True ) \"\"\"The import task of the dataset.\"\"\" objects = models . Manager () @property def search_index_path ( self ): \"\"\" The path to the search index of the dataset. :return: \"\"\" return DATA_DIR / f 'search_index_ { self . local_database } ' if self . local_database else None def get_search_service ( self ) -> SearchService : \"\"\" Return appropriate search service depending on the search mode \"\"\" match self . search_mode : case self . SearchMode . LOCAL : if not self . search_index_path . exists (): raise Exception ( 'Dataset search index has not been created yet' ) return LocalSearchService ( self . search_index_path ) case self . SearchMode . WIKIDATA : return WikidataSearchService () case self . SearchMode . TRIPLYDB : if 'tdb_id' not in self . source : raise Exception ( 'Dataset is not a TriplyDB dataset' ) return TriplyDBSearchService ( self . source [ 'tdb_id' ]) case _ : raise ValueError ( f 'Unknown search mode { self . search_mode } ' ) def get_query_service ( self ) -> QueryService : \"\"\" If the mode is local, return a local query service, otherwise return a SPARQL query service \"\"\" match self . mode : case self . Mode . LOCAL : if not self . local_database : raise Exception ( 'Dataset local database has not been imported yet' ) return LocalQueryService ( str ( self . local_database )) case self . Mode . SPARQL : return SPARQLQueryService ( str ( self . sparql_endpoint )) case _ : raise ValueError ( f 'Unknown mode { self . mode } ' ) def can_view ( self , user : User ): return bool ( user ) def can_edit ( self , user : User ): return super () . can_edit ( user ) or self . creator == user STATES = ( state . value , state . value ) for state in DatasetState class-attribute creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) class-attribute The user who created the dataset. description = models . TextField ( blank = True ) class-attribute The description of the dataset. id = models . UUIDField ( default = uuid . uuid4 , primary_key = True ) class-attribute The identifier of the dataset. import_task = models . OneToOneField ( 'tasks.Task' , on_delete = models . SET_NULL , null = True ) class-attribute The import task of the dataset. local_database = models . CharField ( max_length = 255 , null = True ) class-attribute The local stardog database identifier of the dataset. mode = models . CharField ( max_length = 255 , choices = Mode . choices , default = Mode . LOCAL ) class-attribute The mode of the dataset. name = models . CharField ( max_length = 255 ) class-attribute The name of the dataset. namespaces = models . JSONField ( null = True ) class-attribute The list of sparql namespaces/prefixes in the dataset. objects = models . Manager () class-attribute search_mode = models . CharField ( max_length = 255 , choices = SearchMode . choices , default = SearchMode . LOCAL ) class-attribute The search mode of the dataset. source = models . JSONField () class-attribute The source of the dataset. sparql_endpoint = models . CharField ( max_length = 255 , null = True ) class-attribute The SPARQL endpoint of the dataset. state = models . CharField ( choices = STATES , default = DatasetState . QUEUED . value , max_length = 255 ) class-attribute The import state of the dataset. statistics = models . JSONField ( null = True ) class-attribute The statistics of the dataset. Mode Bases: models . TextChoices The Mode class is an enumeration of the possible modes of a dataset Source code in datasets/models.py 33 34 35 36 37 38 class Mode ( models . TextChoices ): \"\"\" The Mode class is an enumeration of the possible modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) SPARQL = 'SPARQL' , _ ( 'From SPARQL endpoint' ) LOCAL = ( 'LOCAL' , _ ( 'Imported locally ' )) class-attribute SPARQL = ( 'SPARQL' , _ ( 'From SPARQL endpoint' )) class-attribute SearchMode Bases: models . TextChoices The SearchMode class is an enumeration of the possible search modes of a dataset Source code in datasets/models.py 40 41 42 43 44 45 46 class SearchMode ( models . TextChoices ): \"\"\" The SearchMode class is an enumeration of the possible search modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) WIKIDATA = 'WIKIDATA' , _ ( 'From Wikidata' ) TRIPLYDB = 'TRIPLYDB' , _ ( 'From TripyDB' ) LOCAL = ( 'LOCAL' , _ ( 'Imported locally ' )) class-attribute TRIPLYDB = ( 'TRIPLYDB' , _ ( 'From TripyDB' )) class-attribute WIKIDATA = ( 'WIKIDATA' , _ ( 'From Wikidata' )) class-attribute can_edit ( user ) Source code in datasets/models.py 122 123 def can_edit ( self , user : User ): return super () . can_edit ( user ) or self . creator == user can_view ( user ) Source code in datasets/models.py 119 120 def can_view ( self , user : User ): return bool ( user ) get_query_service () If the mode is local, return a local query service, otherwise return a SPARQL query service Source code in datasets/models.py 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_query_service ( self ) -> QueryService : \"\"\" If the mode is local, return a local query service, otherwise return a SPARQL query service \"\"\" match self . mode : case self . Mode . LOCAL : if not self . local_database : raise Exception ( 'Dataset local database has not been imported yet' ) return LocalQueryService ( str ( self . local_database )) case self . Mode . SPARQL : return SPARQLQueryService ( str ( self . sparql_endpoint )) case _ : raise ValueError ( f 'Unknown mode { self . mode } ' ) get_search_service () Return appropriate search service depending on the search mode Source code in datasets/models.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def get_search_service ( self ) -> SearchService : \"\"\" Return appropriate search service depending on the search mode \"\"\" match self . search_mode : case self . SearchMode . LOCAL : if not self . search_index_path . exists (): raise Exception ( 'Dataset search index has not been created yet' ) return LocalSearchService ( self . search_index_path ) case self . SearchMode . WIKIDATA : return WikidataSearchService () case self . SearchMode . TRIPLYDB : if 'tdb_id' not in self . source : raise Exception ( 'Dataset is not a TriplyDB dataset' ) return TriplyDBSearchService ( self . source [ 'tdb_id' ]) case _ : raise ValueError ( f 'Unknown search mode { self . search_mode } ' ) search_index_path () property The path to the search index of the dataset. :return: Source code in datasets/models.py 79 80 81 82 83 84 85 @property def search_index_path ( self ): \"\"\" The path to the search index of the dataset. :return: \"\"\" return DATA_DIR / f 'search_index_ { self . local_database } ' if self . local_database else None DatasetState Bases: Enum The DatasetState class is an enumeration of the possible states of a dataset Source code in datasets/models.py 17 18 19 20 21 22 23 24 class DatasetState ( Enum ): \"\"\" The DatasetState class is an enumeration of the possible states of a dataset \"\"\" QUEUED = 'QUEUED' IMPORTING = 'IMPORTING' IMPORTED = 'IMPORTED' FAILED = 'FAILED' FAILED = 'FAILED' class-attribute IMPORTED = 'IMPORTED' class-attribute IMPORTING = 'IMPORTING' class-attribute QUEUED = 'QUEUED' class-attribute","title":"models"},{"location":"reference/datasets/models/#datasets.models.Dataset","text":"Bases: TaskMixin , TimeStampMixin , OwnableMixin The internal dataset model. Source code in datasets/models.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 class Dataset ( TaskMixin , TimeStampMixin , OwnableMixin ): \"\"\" The internal dataset model. \"\"\" STATES = (( state . value , state . value ) for state in DatasetState ) class Mode ( models . TextChoices ): \"\"\" The Mode class is an enumeration of the possible modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) SPARQL = 'SPARQL' , _ ( 'From SPARQL endpoint' ) class SearchMode ( models . TextChoices ): \"\"\" The SearchMode class is an enumeration of the possible search modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) WIKIDATA = 'WIKIDATA' , _ ( 'From Wikidata' ) TRIPLYDB = 'TRIPLYDB' , _ ( 'From TripyDB' ) id = models . UUIDField ( default = uuid . uuid4 , primary_key = True ) \"\"\"The identifier of the dataset.\"\"\" name = models . CharField ( max_length = 255 ) \"\"\"The name of the dataset.\"\"\" description = models . TextField ( blank = True ) \"\"\"The description of the dataset.\"\"\" source = models . JSONField () \"\"\"The source of the dataset.\"\"\" mode = models . CharField ( max_length = 255 , choices = Mode . choices , default = Mode . LOCAL ) \"\"\"The mode of the dataset.\"\"\" search_mode = models . CharField ( max_length = 255 , choices = SearchMode . choices , default = SearchMode . LOCAL ) \"\"\"The search mode of the dataset.\"\"\" creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) \"\"\"The user who created the dataset.\"\"\" local_database = models . CharField ( max_length = 255 , null = True ) \"\"\"The local stardog database identifier of the dataset.\"\"\" sparql_endpoint = models . CharField ( max_length = 255 , null = True ) \"\"\"The SPARQL endpoint of the dataset.\"\"\" statistics = models . JSONField ( null = True ) \"\"\"The statistics of the dataset.\"\"\" namespaces = models . JSONField ( null = True ) \"\"\"The list of sparql namespaces/prefixes in the dataset.\"\"\" state = models . CharField ( choices = STATES , default = DatasetState . QUEUED . value , max_length = 255 ) \"\"\"The import state of the dataset.\"\"\" import_task = models . OneToOneField ( 'tasks.Task' , on_delete = models . SET_NULL , null = True ) \"\"\"The import task of the dataset.\"\"\" objects = models . Manager () @property def search_index_path ( self ): \"\"\" The path to the search index of the dataset. :return: \"\"\" return DATA_DIR / f 'search_index_ { self . local_database } ' if self . local_database else None def get_search_service ( self ) -> SearchService : \"\"\" Return appropriate search service depending on the search mode \"\"\" match self . search_mode : case self . SearchMode . LOCAL : if not self . search_index_path . exists (): raise Exception ( 'Dataset search index has not been created yet' ) return LocalSearchService ( self . search_index_path ) case self . SearchMode . WIKIDATA : return WikidataSearchService () case self . SearchMode . TRIPLYDB : if 'tdb_id' not in self . source : raise Exception ( 'Dataset is not a TriplyDB dataset' ) return TriplyDBSearchService ( self . source [ 'tdb_id' ]) case _ : raise ValueError ( f 'Unknown search mode { self . search_mode } ' ) def get_query_service ( self ) -> QueryService : \"\"\" If the mode is local, return a local query service, otherwise return a SPARQL query service \"\"\" match self . mode : case self . Mode . LOCAL : if not self . local_database : raise Exception ( 'Dataset local database has not been imported yet' ) return LocalQueryService ( str ( self . local_database )) case self . Mode . SPARQL : return SPARQLQueryService ( str ( self . sparql_endpoint )) case _ : raise ValueError ( f 'Unknown mode { self . mode } ' ) def can_view ( self , user : User ): return bool ( user ) def can_edit ( self , user : User ): return super () . can_edit ( user ) or self . creator == user","title":"Dataset"},{"location":"reference/datasets/models/#datasets.models.Dataset.STATES","text":"","title":"STATES"},{"location":"reference/datasets/models/#datasets.models.Dataset.creator","text":"The user who created the dataset.","title":"creator"},{"location":"reference/datasets/models/#datasets.models.Dataset.description","text":"The description of the dataset.","title":"description"},{"location":"reference/datasets/models/#datasets.models.Dataset.id","text":"The identifier of the dataset.","title":"id"},{"location":"reference/datasets/models/#datasets.models.Dataset.import_task","text":"The import task of the dataset.","title":"import_task"},{"location":"reference/datasets/models/#datasets.models.Dataset.local_database","text":"The local stardog database identifier of the dataset.","title":"local_database"},{"location":"reference/datasets/models/#datasets.models.Dataset.mode","text":"The mode of the dataset.","title":"mode"},{"location":"reference/datasets/models/#datasets.models.Dataset.name","text":"The name of the dataset.","title":"name"},{"location":"reference/datasets/models/#datasets.models.Dataset.namespaces","text":"The list of sparql namespaces/prefixes in the dataset.","title":"namespaces"},{"location":"reference/datasets/models/#datasets.models.Dataset.objects","text":"","title":"objects"},{"location":"reference/datasets/models/#datasets.models.Dataset.search_mode","text":"The search mode of the dataset.","title":"search_mode"},{"location":"reference/datasets/models/#datasets.models.Dataset.source","text":"The source of the dataset.","title":"source"},{"location":"reference/datasets/models/#datasets.models.Dataset.sparql_endpoint","text":"The SPARQL endpoint of the dataset.","title":"sparql_endpoint"},{"location":"reference/datasets/models/#datasets.models.Dataset.state","text":"The import state of the dataset.","title":"state"},{"location":"reference/datasets/models/#datasets.models.Dataset.statistics","text":"The statistics of the dataset.","title":"statistics"},{"location":"reference/datasets/models/#datasets.models.Dataset.Mode","text":"Bases: models . TextChoices The Mode class is an enumeration of the possible modes of a dataset Source code in datasets/models.py 33 34 35 36 37 38 class Mode ( models . TextChoices ): \"\"\" The Mode class is an enumeration of the possible modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) SPARQL = 'SPARQL' , _ ( 'From SPARQL endpoint' )","title":"Mode"},{"location":"reference/datasets/models/#datasets.models.Dataset.Mode.LOCAL","text":"","title":"LOCAL"},{"location":"reference/datasets/models/#datasets.models.Dataset.Mode.SPARQL","text":"","title":"SPARQL"},{"location":"reference/datasets/models/#datasets.models.Dataset.SearchMode","text":"Bases: models . TextChoices The SearchMode class is an enumeration of the possible search modes of a dataset Source code in datasets/models.py 40 41 42 43 44 45 46 class SearchMode ( models . TextChoices ): \"\"\" The SearchMode class is an enumeration of the possible search modes of a dataset \"\"\" LOCAL = 'LOCAL' , _ ( 'Imported locally ' ) WIKIDATA = 'WIKIDATA' , _ ( 'From Wikidata' ) TRIPLYDB = 'TRIPLYDB' , _ ( 'From TripyDB' )","title":"SearchMode"},{"location":"reference/datasets/models/#datasets.models.Dataset.SearchMode.LOCAL","text":"","title":"LOCAL"},{"location":"reference/datasets/models/#datasets.models.Dataset.SearchMode.TRIPLYDB","text":"","title":"TRIPLYDB"},{"location":"reference/datasets/models/#datasets.models.Dataset.SearchMode.WIKIDATA","text":"","title":"WIKIDATA"},{"location":"reference/datasets/models/#datasets.models.Dataset.can_edit","text":"Source code in datasets/models.py 122 123 def can_edit ( self , user : User ): return super () . can_edit ( user ) or self . creator == user","title":"can_edit()"},{"location":"reference/datasets/models/#datasets.models.Dataset.can_view","text":"Source code in datasets/models.py 119 120 def can_view ( self , user : User ): return bool ( user )","title":"can_view()"},{"location":"reference/datasets/models/#datasets.models.Dataset.get_query_service","text":"If the mode is local, return a local query service, otherwise return a SPARQL query service Source code in datasets/models.py 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_query_service ( self ) -> QueryService : \"\"\" If the mode is local, return a local query service, otherwise return a SPARQL query service \"\"\" match self . mode : case self . Mode . LOCAL : if not self . local_database : raise Exception ( 'Dataset local database has not been imported yet' ) return LocalQueryService ( str ( self . local_database )) case self . Mode . SPARQL : return SPARQLQueryService ( str ( self . sparql_endpoint )) case _ : raise ValueError ( f 'Unknown mode { self . mode } ' )","title":"get_query_service()"},{"location":"reference/datasets/models/#datasets.models.Dataset.get_search_service","text":"Return appropriate search service depending on the search mode Source code in datasets/models.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def get_search_service ( self ) -> SearchService : \"\"\" Return appropriate search service depending on the search mode \"\"\" match self . search_mode : case self . SearchMode . LOCAL : if not self . search_index_path . exists (): raise Exception ( 'Dataset search index has not been created yet' ) return LocalSearchService ( self . search_index_path ) case self . SearchMode . WIKIDATA : return WikidataSearchService () case self . SearchMode . TRIPLYDB : if 'tdb_id' not in self . source : raise Exception ( 'Dataset is not a TriplyDB dataset' ) return TriplyDBSearchService ( self . source [ 'tdb_id' ]) case _ : raise ValueError ( f 'Unknown search mode { self . search_mode } ' )","title":"get_search_service()"},{"location":"reference/datasets/models/#datasets.models.Dataset.search_index_path","text":"The path to the search index of the dataset. :return: Source code in datasets/models.py 79 80 81 82 83 84 85 @property def search_index_path ( self ): \"\"\" The path to the search index of the dataset. :return: \"\"\" return DATA_DIR / f 'search_index_ { self . local_database } ' if self . local_database else None","title":"search_index_path()"},{"location":"reference/datasets/models/#datasets.models.DatasetState","text":"Bases: Enum The DatasetState class is an enumeration of the possible states of a dataset Source code in datasets/models.py 17 18 19 20 21 22 23 24 class DatasetState ( Enum ): \"\"\" The DatasetState class is an enumeration of the possible states of a dataset \"\"\" QUEUED = 'QUEUED' IMPORTING = 'IMPORTING' IMPORTED = 'IMPORTED' FAILED = 'FAILED'","title":"DatasetState"},{"location":"reference/datasets/models/#datasets.models.DatasetState.FAILED","text":"","title":"FAILED"},{"location":"reference/datasets/models/#datasets.models.DatasetState.IMPORTED","text":"","title":"IMPORTED"},{"location":"reference/datasets/models/#datasets.models.DatasetState.IMPORTING","text":"","title":"IMPORTING"},{"location":"reference/datasets/models/#datasets.models.DatasetState.QUEUED","text":"","title":"QUEUED"},{"location":"reference/datasets/serializers/","text":"DatasetSerializer Bases: serializers . ModelSerializer Source code in datasets/serializers.py 7 8 9 10 11 12 class DatasetSerializer ( serializers . ModelSerializer ): creator = ShortUserSerializer ( read_only = True ) class Meta : model = Dataset exclude = [] creator = ShortUserSerializer ( read_only = True ) class-attribute Meta Source code in datasets/serializers.py 10 11 12 class Meta : model = Dataset exclude = [] exclude = [] class-attribute model = Dataset class-attribute","title":"serializers"},{"location":"reference/datasets/serializers/#datasets.serializers.DatasetSerializer","text":"Bases: serializers . ModelSerializer Source code in datasets/serializers.py 7 8 9 10 11 12 class DatasetSerializer ( serializers . ModelSerializer ): creator = ShortUserSerializer ( read_only = True ) class Meta : model = Dataset exclude = []","title":"DatasetSerializer"},{"location":"reference/datasets/serializers/#datasets.serializers.DatasetSerializer.creator","text":"","title":"creator"},{"location":"reference/datasets/serializers/#datasets.serializers.DatasetSerializer.Meta","text":"Source code in datasets/serializers.py 10 11 12 class Meta : model = Dataset exclude = []","title":"Meta"},{"location":"reference/datasets/serializers/#datasets.serializers.DatasetSerializer.Meta.exclude","text":"","title":"exclude"},{"location":"reference/datasets/serializers/#datasets.serializers.DatasetSerializer.Meta.model","text":"","title":"model"},{"location":"reference/datasets/management/","text":"","title":"management"},{"location":"reference/datasets/management/commands/","text":"","title":"commands"},{"location":"reference/datasets/management/commands/import_kg/","text":"Command Bases: BaseCommand Source code in datasets/management/commands/import_kg.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Command ( BaseCommand ): def add_arguments ( self , parser ): parser . add_argument ( 'file' , type = str ) def handle ( self , file , * args , ** options ): datasets = LinkedOpenDataCloudApi . fetch_all_datasets () downloadable = [ d for d in datasets if len ( d . downloads ()) > 0 ] print ( f ' { len ( downloadable ) } datasets are downloadable' ) defdownloadable = [ d for d in datasets if any ( do . is_kg for do in d . downloads ())] print ( f ' { len ( defdownloadable ) } datasets are definitely kg' ) downs = defaultdict ( list ) for d in defdownloadable : for do in d . downloads (): if do . is_kg : downs [ do . guess_format ()] . append ( do ) import_kg ( Path ( file ) . absolute ()) add_arguments ( parser ) Source code in datasets/management/commands/import_kg.py 11 12 def add_arguments ( self , parser ): parser . add_argument ( 'file' , type = str ) handle ( file , * args , ** options ) Source code in datasets/management/commands/import_kg.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def handle ( self , file , * args , ** options ): datasets = LinkedOpenDataCloudApi . fetch_all_datasets () downloadable = [ d for d in datasets if len ( d . downloads ()) > 0 ] print ( f ' { len ( downloadable ) } datasets are downloadable' ) defdownloadable = [ d for d in datasets if any ( do . is_kg for do in d . downloads ())] print ( f ' { len ( defdownloadable ) } datasets are definitely kg' ) downs = defaultdict ( list ) for d in defdownloadable : for do in d . downloads (): if do . is_kg : downs [ do . guess_format ()] . append ( do ) import_kg ( Path ( file ) . absolute ())","title":"import_kg"},{"location":"reference/datasets/management/commands/import_kg/#datasets.management.commands.import_kg.Command","text":"Bases: BaseCommand Source code in datasets/management/commands/import_kg.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Command ( BaseCommand ): def add_arguments ( self , parser ): parser . add_argument ( 'file' , type = str ) def handle ( self , file , * args , ** options ): datasets = LinkedOpenDataCloudApi . fetch_all_datasets () downloadable = [ d for d in datasets if len ( d . downloads ()) > 0 ] print ( f ' { len ( downloadable ) } datasets are downloadable' ) defdownloadable = [ d for d in datasets if any ( do . is_kg for do in d . downloads ())] print ( f ' { len ( defdownloadable ) } datasets are definitely kg' ) downs = defaultdict ( list ) for d in defdownloadable : for do in d . downloads (): if do . is_kg : downs [ do . guess_format ()] . append ( do ) import_kg ( Path ( file ) . absolute ())","title":"Command"},{"location":"reference/datasets/management/commands/import_kg/#datasets.management.commands.import_kg.Command.add_arguments","text":"Source code in datasets/management/commands/import_kg.py 11 12 def add_arguments ( self , parser ): parser . add_argument ( 'file' , type = str )","title":"add_arguments()"},{"location":"reference/datasets/management/commands/import_kg/#datasets.management.commands.import_kg.Command.handle","text":"Source code in datasets/management/commands/import_kg.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def handle ( self , file , * args , ** options ): datasets = LinkedOpenDataCloudApi . fetch_all_datasets () downloadable = [ d for d in datasets if len ( d . downloads ()) > 0 ] print ( f ' { len ( downloadable ) } datasets are downloadable' ) defdownloadable = [ d for d in datasets if any ( do . is_kg for do in d . downloads ())] print ( f ' { len ( defdownloadable ) } datasets are definitely kg' ) downs = defaultdict ( list ) for d in defdownloadable : for do in d . downloads (): if do . is_kg : downs [ do . guess_format ()] . append ( do ) import_kg ( Path ( file ) . absolute ())","title":"handle()"},{"location":"reference/datasets/services/","text":"","title":"services"},{"location":"reference/datasets/services/bold_cli/","text":"BoldCli Source code in datasets/services/bold_cli.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BoldCli : @staticmethod def cmd ( command : List [ str ], ** kwargs ) -> Iterator [ str ]: yield from execute_command ( [ str ( BIN_DIR / 'bold-cli' ), * command ], cwd = STORAGE_DIR , ** kwargs ) @staticmethod def search ( index_path : Path , query : str , limit : int , offset : int , pos : int = None , url : int = None , min_count : int = None , max_count : int = None , ** kwargs ) -> dict : filter_args = [] if pos is not None : filter_args . append ( f '--pos= { pos } ' ) if url is not None : filter_args . append ( f '--url= { url } ' ) if min_count is not None : filter_args . append ( f '--min-count= { min_count } ' ) if max_count is not None : filter_args . append ( f '--max-count= { max_count } ' ) print ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ]) result_lines = list ( BoldCli . cmd ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ], ignore_errors = True , ** kwargs )) # print(''.join(result_lines)) return json . loads ( '' . join ( result_lines )) cmd ( command , ** kwargs ) staticmethod Source code in datasets/services/bold_cli.py 11 12 13 14 15 16 17 @staticmethod def cmd ( command : List [ str ], ** kwargs ) -> Iterator [ str ]: yield from execute_command ( [ str ( BIN_DIR / 'bold-cli' ), * command ], cwd = STORAGE_DIR , ** kwargs ) search ( index_path , query , limit , offset , pos = None , url = None , min_count = None , max_count = None , ** kwargs ) staticmethod Source code in datasets/services/bold_cli.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @staticmethod def search ( index_path : Path , query : str , limit : int , offset : int , pos : int = None , url : int = None , min_count : int = None , max_count : int = None , ** kwargs ) -> dict : filter_args = [] if pos is not None : filter_args . append ( f '--pos= { pos } ' ) if url is not None : filter_args . append ( f '--url= { url } ' ) if min_count is not None : filter_args . append ( f '--min-count= { min_count } ' ) if max_count is not None : filter_args . append ( f '--max-count= { max_count } ' ) print ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ]) result_lines = list ( BoldCli . cmd ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ], ignore_errors = True , ** kwargs )) # print(''.join(result_lines)) return json . loads ( '' . join ( result_lines ))","title":"bold_cli"},{"location":"reference/datasets/services/bold_cli/#datasets.services.bold_cli.BoldCli","text":"Source code in datasets/services/bold_cli.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BoldCli : @staticmethod def cmd ( command : List [ str ], ** kwargs ) -> Iterator [ str ]: yield from execute_command ( [ str ( BIN_DIR / 'bold-cli' ), * command ], cwd = STORAGE_DIR , ** kwargs ) @staticmethod def search ( index_path : Path , query : str , limit : int , offset : int , pos : int = None , url : int = None , min_count : int = None , max_count : int = None , ** kwargs ) -> dict : filter_args = [] if pos is not None : filter_args . append ( f '--pos= { pos } ' ) if url is not None : filter_args . append ( f '--url= { url } ' ) if min_count is not None : filter_args . append ( f '--min-count= { min_count } ' ) if max_count is not None : filter_args . append ( f '--max-count= { max_count } ' ) print ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ]) result_lines = list ( BoldCli . cmd ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ], ignore_errors = True , ** kwargs )) # print(''.join(result_lines)) return json . loads ( '' . join ( result_lines ))","title":"BoldCli"},{"location":"reference/datasets/services/bold_cli/#datasets.services.bold_cli.BoldCli.cmd","text":"Source code in datasets/services/bold_cli.py 11 12 13 14 15 16 17 @staticmethod def cmd ( command : List [ str ], ** kwargs ) -> Iterator [ str ]: yield from execute_command ( [ str ( BIN_DIR / 'bold-cli' ), * command ], cwd = STORAGE_DIR , ** kwargs )","title":"cmd()"},{"location":"reference/datasets/services/bold_cli/#datasets.services.bold_cli.BoldCli.search","text":"Source code in datasets/services/bold_cli.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @staticmethod def search ( index_path : Path , query : str , limit : int , offset : int , pos : int = None , url : int = None , min_count : int = None , max_count : int = None , ** kwargs ) -> dict : filter_args = [] if pos is not None : filter_args . append ( f '--pos= { pos } ' ) if url is not None : filter_args . append ( f '--url= { url } ' ) if min_count is not None : filter_args . append ( f '--min-count= { min_count } ' ) if max_count is not None : filter_args . append ( f '--max-count= { max_count } ' ) print ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ]) result_lines = list ( BoldCli . cmd ([ 'search' , '--index' , str ( index_path ), '--limit' , str ( limit ), '--offset' , str ( offset ), * filter_args , query ], ignore_errors = True , ** kwargs )) # print(''.join(result_lines)) return json . loads ( '' . join ( result_lines ))","title":"search()"},{"location":"reference/datasets/services/query/","text":"LocalQueryService Bases: QueryService Source code in datasets/services/query.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class LocalQueryService ( QueryService ): database : str def __init__ ( self , database : str ): self . database = database def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : try : with StardogApi . connection ( self . database ) as conn : if 'LIMIT' in query : limit = None if is_graph_query ( query ): return { 'application/n-triples' : conn . graph ( query , 'application/n-triples' , limit = limit , timeout = timeout ) . decode ( 'utf-8' ) } else : output = conn . select ( query , limit = limit , timeout = timeout ) return { 'application/sparql-results+json' : json . dumps ( output ) } except stardog . exceptions . StardogException as e : raise QueryExecutionException ( str ( e )) database = database instance-attribute __init__ ( database ) Source code in datasets/services/query.py 28 29 def __init__ ( self , database : str ): self . database = database query ( query , limit = 10 , timeout = None , ** options ) Source code in datasets/services/query.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : try : with StardogApi . connection ( self . database ) as conn : if 'LIMIT' in query : limit = None if is_graph_query ( query ): return { 'application/n-triples' : conn . graph ( query , 'application/n-triples' , limit = limit , timeout = timeout ) . decode ( 'utf-8' ) } else : output = conn . select ( query , limit = limit , timeout = timeout ) return { 'application/sparql-results+json' : json . dumps ( output ) } except stardog . exceptions . StardogException as e : raise QueryExecutionException ( str ( e )) QueryExecutionException Bases: Exception Source code in datasets/services/query.py 11 12 class QueryExecutionException ( Exception ): pass QueryService Bases: ABC Source code in datasets/services/query.py 15 16 17 18 19 20 21 22 class QueryService ( ABC ): @abstractmethod def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : pass def query_select ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : data = self . query ( query , limit , timeout , ** options ) return json . loads ( data [ 'application/sparql-results+json' ]) query ( query , limit = 10 , timeout = None , ** options ) abstractmethod Source code in datasets/services/query.py 16 17 18 @abstractmethod def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : pass query_select ( query , limit = 10 , timeout = None , ** options ) Source code in datasets/services/query.py 20 21 22 def query_select ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : data = self . query ( query , limit , timeout , ** options ) return json . loads ( data [ 'application/sparql-results+json' ]) SPARQLQueryService Bases: QueryService Source code in datasets/services/query.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class SPARQLQueryService ( QueryService ): endpoint : str def __init__ ( self , endpoint : str ): self . endpoint = endpoint def query ( self , query : str , limit : int = 10 , timeout : int = None , ignore_limit = False , ** options ) -> dict : if 'LIMIT' not in query . upper () and not ignore_limit : raise QueryExecutionException ( f 'SPARQL queries must specify a LIMIT' ) accept = 'application/n-triples' if is_graph_query ( query ) else 'application/sparql-results+json' with Session () as session : response = session . post ( self . endpoint , data = query , params = { 'limit' : limit , 'timeout' : timeout , }, headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/sparql-query' , 'Accept' : accept , }, timeout = timeout , allow_redirects = False ) retry_count = 0 while response . status_code // 100 == 3 and retry_count < 3 : request = response . request request . url = response . headers . get ( 'Location' ) response = session . send ( response . request ) retry_count += 1 if response . status_code != 200 : raise QueryExecutionException ( f ' { response . status_code } { response . reason } \\n { response . text } ' ) if accept == 'application/n-triples' : return { 'application/n-triples' : response . text } else : return { 'application/sparql-results+json' : response . text } endpoint = endpoint instance-attribute __init__ ( endpoint ) Source code in datasets/services/query.py 54 55 def __init__ ( self , endpoint : str ): self . endpoint = endpoint query ( query , limit = 10 , timeout = None , ignore_limit = False , ** options ) Source code in datasets/services/query.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def query ( self , query : str , limit : int = 10 , timeout : int = None , ignore_limit = False , ** options ) -> dict : if 'LIMIT' not in query . upper () and not ignore_limit : raise QueryExecutionException ( f 'SPARQL queries must specify a LIMIT' ) accept = 'application/n-triples' if is_graph_query ( query ) else 'application/sparql-results+json' with Session () as session : response = session . post ( self . endpoint , data = query , params = { 'limit' : limit , 'timeout' : timeout , }, headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/sparql-query' , 'Accept' : accept , }, timeout = timeout , allow_redirects = False ) retry_count = 0 while response . status_code // 100 == 3 and retry_count < 3 : request = response . request request . url = response . headers . get ( 'Location' ) response = session . send ( response . request ) retry_count += 1 if response . status_code != 200 : raise QueryExecutionException ( f ' { response . status_code } { response . reason } \\n { response . text } ' ) if accept == 'application/n-triples' : return { 'application/n-triples' : response . text } else : return { 'application/sparql-results+json' : response . text } is_graph_query ( query ) Source code in datasets/services/query.py 96 97 def is_graph_query ( query : str ) -> bool : return 'CONSTRUCT' in query . upper () . strip () or 'DESCRIBE' in query . upper () . strip ()","title":"query"},{"location":"reference/datasets/services/query/#datasets.services.query.LocalQueryService","text":"Bases: QueryService Source code in datasets/services/query.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class LocalQueryService ( QueryService ): database : str def __init__ ( self , database : str ): self . database = database def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : try : with StardogApi . connection ( self . database ) as conn : if 'LIMIT' in query : limit = None if is_graph_query ( query ): return { 'application/n-triples' : conn . graph ( query , 'application/n-triples' , limit = limit , timeout = timeout ) . decode ( 'utf-8' ) } else : output = conn . select ( query , limit = limit , timeout = timeout ) return { 'application/sparql-results+json' : json . dumps ( output ) } except stardog . exceptions . StardogException as e : raise QueryExecutionException ( str ( e ))","title":"LocalQueryService"},{"location":"reference/datasets/services/query/#datasets.services.query.LocalQueryService.database","text":"","title":"database"},{"location":"reference/datasets/services/query/#datasets.services.query.LocalQueryService.__init__","text":"Source code in datasets/services/query.py 28 29 def __init__ ( self , database : str ): self . database = database","title":"__init__()"},{"location":"reference/datasets/services/query/#datasets.services.query.LocalQueryService.query","text":"Source code in datasets/services/query.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : try : with StardogApi . connection ( self . database ) as conn : if 'LIMIT' in query : limit = None if is_graph_query ( query ): return { 'application/n-triples' : conn . graph ( query , 'application/n-triples' , limit = limit , timeout = timeout ) . decode ( 'utf-8' ) } else : output = conn . select ( query , limit = limit , timeout = timeout ) return { 'application/sparql-results+json' : json . dumps ( output ) } except stardog . exceptions . StardogException as e : raise QueryExecutionException ( str ( e ))","title":"query()"},{"location":"reference/datasets/services/query/#datasets.services.query.QueryExecutionException","text":"Bases: Exception Source code in datasets/services/query.py 11 12 class QueryExecutionException ( Exception ): pass","title":"QueryExecutionException"},{"location":"reference/datasets/services/query/#datasets.services.query.QueryService","text":"Bases: ABC Source code in datasets/services/query.py 15 16 17 18 19 20 21 22 class QueryService ( ABC ): @abstractmethod def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : pass def query_select ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : data = self . query ( query , limit , timeout , ** options ) return json . loads ( data [ 'application/sparql-results+json' ])","title":"QueryService"},{"location":"reference/datasets/services/query/#datasets.services.query.QueryService.query","text":"Source code in datasets/services/query.py 16 17 18 @abstractmethod def query ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : pass","title":"query()"},{"location":"reference/datasets/services/query/#datasets.services.query.QueryService.query_select","text":"Source code in datasets/services/query.py 20 21 22 def query_select ( self , query : str , limit : int = 10 , timeout : int = None , ** options ) -> dict : data = self . query ( query , limit , timeout , ** options ) return json . loads ( data [ 'application/sparql-results+json' ])","title":"query_select()"},{"location":"reference/datasets/services/query/#datasets.services.query.SPARQLQueryService","text":"Bases: QueryService Source code in datasets/services/query.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class SPARQLQueryService ( QueryService ): endpoint : str def __init__ ( self , endpoint : str ): self . endpoint = endpoint def query ( self , query : str , limit : int = 10 , timeout : int = None , ignore_limit = False , ** options ) -> dict : if 'LIMIT' not in query . upper () and not ignore_limit : raise QueryExecutionException ( f 'SPARQL queries must specify a LIMIT' ) accept = 'application/n-triples' if is_graph_query ( query ) else 'application/sparql-results+json' with Session () as session : response = session . post ( self . endpoint , data = query , params = { 'limit' : limit , 'timeout' : timeout , }, headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/sparql-query' , 'Accept' : accept , }, timeout = timeout , allow_redirects = False ) retry_count = 0 while response . status_code // 100 == 3 and retry_count < 3 : request = response . request request . url = response . headers . get ( 'Location' ) response = session . send ( response . request ) retry_count += 1 if response . status_code != 200 : raise QueryExecutionException ( f ' { response . status_code } { response . reason } \\n { response . text } ' ) if accept == 'application/n-triples' : return { 'application/n-triples' : response . text } else : return { 'application/sparql-results+json' : response . text }","title":"SPARQLQueryService"},{"location":"reference/datasets/services/query/#datasets.services.query.SPARQLQueryService.endpoint","text":"","title":"endpoint"},{"location":"reference/datasets/services/query/#datasets.services.query.SPARQLQueryService.__init__","text":"Source code in datasets/services/query.py 54 55 def __init__ ( self , endpoint : str ): self . endpoint = endpoint","title":"__init__()"},{"location":"reference/datasets/services/query/#datasets.services.query.SPARQLQueryService.query","text":"Source code in datasets/services/query.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def query ( self , query : str , limit : int = 10 , timeout : int = None , ignore_limit = False , ** options ) -> dict : if 'LIMIT' not in query . upper () and not ignore_limit : raise QueryExecutionException ( f 'SPARQL queries must specify a LIMIT' ) accept = 'application/n-triples' if is_graph_query ( query ) else 'application/sparql-results+json' with Session () as session : response = session . post ( self . endpoint , data = query , params = { 'limit' : limit , 'timeout' : timeout , }, headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/sparql-query' , 'Accept' : accept , }, timeout = timeout , allow_redirects = False ) retry_count = 0 while response . status_code // 100 == 3 and retry_count < 3 : request = response . request request . url = response . headers . get ( 'Location' ) response = session . send ( response . request ) retry_count += 1 if response . status_code != 200 : raise QueryExecutionException ( f ' { response . status_code } { response . reason } \\n { response . text } ' ) if accept == 'application/n-triples' : return { 'application/n-triples' : response . text } else : return { 'application/sparql-results+json' : response . text }","title":"query()"},{"location":"reference/datasets/services/query/#datasets.services.query.is_graph_query","text":"Source code in datasets/services/query.py 96 97 def is_graph_query ( query : str ) -> bool : return 'CONSTRUCT' in query . upper () . strip () or 'DESCRIBE' in query . upper () . strip ()","title":"is_graph_query()"},{"location":"reference/datasets/services/search/","text":"LocalSearchService Bases: SearchService Source code in datasets/services/search.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class LocalSearchService ( SearchService ): index_path : Path def __init__ ( self , index_path : Path ): self . index_path = index_path def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : url = parse_int_or_none ( options . get ( 'url' , None )) min_count = parse_int_or_none ( options . get ( 'min_count' , None )) max_count = parse_int_or_none ( options . get ( 'max_count' , None )) result_data = BoldCli . search ( self . index_path , query , limit , offset , pos . to_int (), url , min_count , max_count ) return SearchResult ( count = result_data [ 'count' ], hits = [ SearchHit ( score = hit [ 'score' ], document = self . _parse_doc ( hit [ 'doc' ]) ) for hit in result_data [ 'hits' ] ], agg = result_data [ 'agg' ] ) def _parse_doc ( self , doc : dict ) -> TermDocument : doc = { k : v [ 0 ] for k , v in doc . items ()} iri = doc [ 'iri' ] type = 'uri' if 'http' in iri else 'literal' lang = None if type == 'literal' and re . match ( r '^.*@[a-z]*$' , iri ): iri , lang = iri . rsplit ( '@' , 1 ) return TermDocument ( type = type , value = iri . removeprefix ( '<' ) . removesuffix ( '>' ), lang = lang , pos = TermPos . from_int ( doc . get ( 'pos' , 0 )), rdf_type = doc . get ( 'ty' , None ), label = doc . get ( 'label' , None ), count = doc . get ( 'count' , None ), search_text = doc . get ( 'iri_text' , None ), description = doc . get ( 'description' , None ), ) index_path = index_path instance-attribute __init__ ( index_path ) Source code in datasets/services/search.py 94 95 def __init__ ( self , index_path : Path ): self . index_path = index_path search ( query , pos , limit = 100 , offset = 0 , timeout = 5000 , ** options ) Source code in datasets/services/search.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : url = parse_int_or_none ( options . get ( 'url' , None )) min_count = parse_int_or_none ( options . get ( 'min_count' , None )) max_count = parse_int_or_none ( options . get ( 'max_count' , None )) result_data = BoldCli . search ( self . index_path , query , limit , offset , pos . to_int (), url , min_count , max_count ) return SearchResult ( count = result_data [ 'count' ], hits = [ SearchHit ( score = hit [ 'score' ], document = self . _parse_doc ( hit [ 'doc' ]) ) for hit in result_data [ 'hits' ] ], agg = result_data [ 'agg' ] ) SearchHit dataclass Bases: Serializable Source code in datasets/services/search.py 65 66 67 68 @dataclass class SearchHit ( Serializable ): score : float document : Serializable | TermDocument document : Serializable | TermDocument class-attribute score : float class-attribute SearchResult dataclass Bases: Serializable Source code in datasets/services/search.py 71 72 73 74 75 76 @dataclass class SearchResult ( Serializable ): count : int = field ( default = 0 ) hits : List [ SearchHit | dict ] = field ( default_factory = list ) agg : dict = field ( default_factory = dict ) error : Optional [ str ] = None agg : dict = field ( default_factory = dict ) class-attribute count : int = field ( default = 0 ) class-attribute error : Optional [ str ] = None class-attribute hits : List [ SearchHit | dict ] = field ( default_factory = list ) class-attribute SearchService Bases: ABC Source code in datasets/services/search.py 79 80 81 82 class SearchService ( ABC ): @abstractmethod def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : pass search ( query , pos , limit = 100 , offset = 0 , timeout = 5000 , ** options ) abstractmethod Source code in datasets/services/search.py 80 81 82 @abstractmethod def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : pass TermDocument dataclass Bases: Serializable Source code in datasets/services/search.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class TermDocument ( Serializable ): type : str search_text : str value : str pos : TermPos lang : Optional [ str ] = None rdf_type : Optional [ str ] = None label : Optional [ str ] = None description : Optional [ str ] = None count : Optional [ int ] = None range : Optional [ str ] = None def searchable_text ( self ): return ' ' . join ([ self . search_text , self . label or '' , self . description or '' , self . value or '' , ]) count : Optional [ int ] = None class-attribute description : Optional [ str ] = None class-attribute label : Optional [ str ] = None class-attribute lang : Optional [ str ] = None class-attribute pos : TermPos class-attribute range : Optional [ str ] = None class-attribute rdf_type : Optional [ str ] = None class-attribute search_text : str class-attribute type : str class-attribute value : str class-attribute searchable_text () Source code in datasets/services/search.py 56 57 58 59 60 61 62 def searchable_text ( self ): return ' ' . join ([ self . search_text , self . label or '' , self . description or '' , self . value or '' , ]) TermPos Bases: Enum Source code in datasets/services/search.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class TermPos ( Enum ): SUBJECT = 'SUBJECT' PREDICATE = 'PREDICATE' OBJECT = 'OBJECT' def to_int ( self ): match self : case TermPos . SUBJECT : return 0 case TermPos . PREDICATE : return 1 case TermPos . OBJECT : return 2 @staticmethod def from_int ( value : int ): match value : case 0 : return TermPos . SUBJECT case 1 : return TermPos . PREDICATE case 2 : return TermPos . OBJECT OBJECT = 'OBJECT' class-attribute PREDICATE = 'PREDICATE' class-attribute SUBJECT = 'SUBJECT' class-attribute from_int ( value ) staticmethod Source code in datasets/services/search.py 32 33 34 35 36 37 38 39 40 @staticmethod def from_int ( value : int ): match value : case 0 : return TermPos . SUBJECT case 1 : return TermPos . PREDICATE case 2 : return TermPos . OBJECT to_int () Source code in datasets/services/search.py 23 24 25 26 27 28 29 30 def to_int ( self ): match self : case TermPos . SUBJECT : return 0 case TermPos . PREDICATE : return 1 case TermPos . OBJECT : return 2 TriplyDBSearchService Bases: SearchService Source code in datasets/services/search.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 class TriplyDBSearchService ( SearchService ): namespace : str def __init__ ( self , namespace : str ): self . namespace = namespace def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : endpoint = self . get_endpoint () if not endpoint : raise ValidationError ( 'Search endpoint is not reachable' ) query = self . build_query ( query , pos , limit , offset ) response = requests . post ( endpoint , data = json . dumps ( query ), timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () print ( result_data ) return SearchResult ( count = result_data [ 'hits' ][ 'total' ][ 'value' ], hits = [ SearchHit ( score = hit [ '_score' ], document = self . _parse_doc ( hit [ '_source' ]) ) for hit in result_data [ 'hits' ][ 'hits' ] ], ) # noinspection PyTypeChecker def build_query ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 ): base_query = { 'size' : limit , 'from' : offset , 'query' : { \"bool\" : defaultdict ( dict )}, } if pos != TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must_not' ] = [{ \"terms\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : [ \"http://www.w3.org/2002/07/owl#DatatypeProperty\" ] } }] if query : base_query [ 'query' ][ 'bool' ][ 'must' ] = [{ \"simple_query_string\" : { \"query\" : query } }] base_query [ 'query' ][ 'bool' ][ 'should' ] = [{ \"simple_query_string\" : { \"query\" : query , \"fields\" : [ \"http://www w3 org/2000/01/rdf-schema#label\" ] } }] if pos == TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must' ] = [ * base_query [ 'query' ][ 'bool' ] . get ( 'must' , []), { \"match\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : \"http://www.w3.org/2002/07/owl#DatatypeProperty\" } } ] return base_query def get_endpoint ( self ): response = requests . get ( f 'https://api.triplydb.com/datasets/ { self . namespace } /services/' , timeout = 5 , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return None result_data = response . json () for service in result_data : if service . get ( 'type' , None ) == 'elasticSearch' : return service [ 'endpoint' ] return None def _parse_doc ( self , doc : dict ) -> TermDocument : iri = doc [ '@id' ] rdf_type = doc . get ( 'http://www w3 org/1999/02/22-rdf-syntax-ns#type' , None ) pos = TermPos . PREDICATE if rdf_type == 'http://www.w3.org/2002/07/owl#DatatypeProperty' \\ else ( TermPos . SUBJECT if 'http' in iri else TermPos . OBJECT ) label = doc . get ( 'http://www w3 org/2000/01/rdf-schema#label' , None ) search_text = label if label else re . sub ( r '[-_#]' , ' ' , iri . split ( '/' )[ - 1 ] . split ( '#' )[ - 1 ]) return TermDocument ( type = 'uri' , value = doc [ '@id' ], pos = pos , rdf_type = first_or_self ( rdf_type ), label = first_or_self ( label ), description = first_or_self ( doc . get ( 'http://www w3 org/2000/01/rdf-schema#comment' , None )), count = None , search_text = first_or_self ( search_text ), range = doc . get ( 'http://www w3 org/2000/01/rdf-schema#range' , None ), ) namespace = namespace instance-attribute __init__ ( namespace ) Source code in datasets/services/search.py 198 199 def __init__ ( self , namespace : str ): self . namespace = namespace build_query ( query , pos , limit = 100 , offset = 0 ) Source code in datasets/services/search.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def build_query ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 ): base_query = { 'size' : limit , 'from' : offset , 'query' : { \"bool\" : defaultdict ( dict )}, } if pos != TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must_not' ] = [{ \"terms\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : [ \"http://www.w3.org/2002/07/owl#DatatypeProperty\" ] } }] if query : base_query [ 'query' ][ 'bool' ][ 'must' ] = [{ \"simple_query_string\" : { \"query\" : query } }] base_query [ 'query' ][ 'bool' ][ 'should' ] = [{ \"simple_query_string\" : { \"query\" : query , \"fields\" : [ \"http://www w3 org/2000/01/rdf-schema#label\" ] } }] if pos == TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must' ] = [ * base_query [ 'query' ][ 'bool' ] . get ( 'must' , []), { \"match\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : \"http://www.w3.org/2002/07/owl#DatatypeProperty\" } } ] return base_query get_endpoint () Source code in datasets/services/search.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def get_endpoint ( self ): response = requests . get ( f 'https://api.triplydb.com/datasets/ { self . namespace } /services/' , timeout = 5 , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return None result_data = response . json () for service in result_data : if service . get ( 'type' , None ) == 'elasticSearch' : return service [ 'endpoint' ] return None search ( query , pos , limit = 100 , offset = 0 , timeout = 5000 , ** options ) Source code in datasets/services/search.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : endpoint = self . get_endpoint () if not endpoint : raise ValidationError ( 'Search endpoint is not reachable' ) query = self . build_query ( query , pos , limit , offset ) response = requests . post ( endpoint , data = json . dumps ( query ), timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () print ( result_data ) return SearchResult ( count = result_data [ 'hits' ][ 'total' ][ 'value' ], hits = [ SearchHit ( score = hit [ '_score' ], document = self . _parse_doc ( hit [ '_source' ]) ) for hit in result_data [ 'hits' ][ 'hits' ] ], ) WikidataSearchService Bases: SearchService Source code in datasets/services/search.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class WikidataSearchService ( SearchService ): def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : type = 'item' if pos == TermPos . PREDICATE : type = 'property' response = requests . get ( 'https://www.wikidata.org/w/api.php' , params = { 'action' : 'wbsearchentities' , 'search' : query , 'language' : 'en' , 'uselang' : 'en' , 'type' : type , 'format' : 'json' , 'formatversion' : 2 , 'errorformat' : 'plaintext' , 'limit' : limit , 'continue' : offset , }, timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , } ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () return SearchResult ( count = 999999 if 'search-continue' in result_data else offset + len ( result_data . get ( 'search' , [])), hits = [ SearchHit ( score = 1.0 , document = self . _parse_doc ( hit , pos ), ) for hit in result_data . get ( 'search' , []) ], ) def _parse_doc ( self , doc : dict , pos : TermPos ) -> TermDocument : iri = doc [ 'concepturi' ] if pos == TermPos . PREDICATE : iri = f 'http://www.wikidata.org/prop/direct/ { doc [ \"id\" ] } ' return TermDocument ( type = 'uri' , value = iri , pos = pos , rdf_type = None , label = doc . get ( 'label' , None ), description = doc . get ( 'description' , None ), count = doc . get ( 'count' , None ), search_text = doc . get ( 'label' , None ), ) search ( query , pos , limit = 100 , offset = 0 , timeout = 5000 , ** options ) Source code in datasets/services/search.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : type = 'item' if pos == TermPos . PREDICATE : type = 'property' response = requests . get ( 'https://www.wikidata.org/w/api.php' , params = { 'action' : 'wbsearchentities' , 'search' : query , 'language' : 'en' , 'uselang' : 'en' , 'type' : type , 'format' : 'json' , 'formatversion' : 2 , 'errorformat' : 'plaintext' , 'limit' : limit , 'continue' : offset , }, timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , } ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () return SearchResult ( count = 999999 if 'search-continue' in result_data else offset + len ( result_data . get ( 'search' , [])), hits = [ SearchHit ( score = 1.0 , document = self . _parse_doc ( hit , pos ), ) for hit in result_data . get ( 'search' , []) ], ) first_or_self ( item ) Source code in datasets/services/search.py 321 322 323 324 325 def first_or_self ( item : Union [ List [ Any ], Any ]): if isinstance ( item , list ): return item [ 0 ] return item merge_results ( a , b , query ) Source code in datasets/services/search.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 def merge_results ( a : SearchResult , b : SearchResult , query : str , ) -> SearchResult : score_fn = lambda x : fuzz . partial_ratio ( x . document . searchable_text (), query ) a_hits = deque (( score_fn ( hit ), hit ) for hit in a . hits ) b_hits = deque (( score_fn ( hit ), hit ) for hit in b . hits ) seen = set () hits = [] while a_hits and b_hits : if a_hits [ 0 ][ 0 ] > b_hits [ 0 ][ 0 ]: item = a_hits . popleft ()[ 1 ] else : item = b_hits . popleft ()[ 1 ] if item . document . value not in seen : hits . append ( item ) seen . add ( item . document . value ) for ( _ , item ) in a_hits + b_hits : if item . document . value not in seen : hits . append ( item ) seen . add ( item . document . value ) return SearchResult ( count = len ( hits ), hits = hits , ) parse_int_or_none ( value ) Source code in datasets/services/search.py 85 86 87 88 def parse_int_or_none ( value : str ) -> int : if value is None : return None return int ( value )","title":"search"},{"location":"reference/datasets/services/search/#datasets.services.search.LocalSearchService","text":"Bases: SearchService Source code in datasets/services/search.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class LocalSearchService ( SearchService ): index_path : Path def __init__ ( self , index_path : Path ): self . index_path = index_path def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : url = parse_int_or_none ( options . get ( 'url' , None )) min_count = parse_int_or_none ( options . get ( 'min_count' , None )) max_count = parse_int_or_none ( options . get ( 'max_count' , None )) result_data = BoldCli . search ( self . index_path , query , limit , offset , pos . to_int (), url , min_count , max_count ) return SearchResult ( count = result_data [ 'count' ], hits = [ SearchHit ( score = hit [ 'score' ], document = self . _parse_doc ( hit [ 'doc' ]) ) for hit in result_data [ 'hits' ] ], agg = result_data [ 'agg' ] ) def _parse_doc ( self , doc : dict ) -> TermDocument : doc = { k : v [ 0 ] for k , v in doc . items ()} iri = doc [ 'iri' ] type = 'uri' if 'http' in iri else 'literal' lang = None if type == 'literal' and re . match ( r '^.*@[a-z]*$' , iri ): iri , lang = iri . rsplit ( '@' , 1 ) return TermDocument ( type = type , value = iri . removeprefix ( '<' ) . removesuffix ( '>' ), lang = lang , pos = TermPos . from_int ( doc . get ( 'pos' , 0 )), rdf_type = doc . get ( 'ty' , None ), label = doc . get ( 'label' , None ), count = doc . get ( 'count' , None ), search_text = doc . get ( 'iri_text' , None ), description = doc . get ( 'description' , None ), )","title":"LocalSearchService"},{"location":"reference/datasets/services/search/#datasets.services.search.LocalSearchService.index_path","text":"","title":"index_path"},{"location":"reference/datasets/services/search/#datasets.services.search.LocalSearchService.__init__","text":"Source code in datasets/services/search.py 94 95 def __init__ ( self , index_path : Path ): self . index_path = index_path","title":"__init__()"},{"location":"reference/datasets/services/search/#datasets.services.search.LocalSearchService.search","text":"Source code in datasets/services/search.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : url = parse_int_or_none ( options . get ( 'url' , None )) min_count = parse_int_or_none ( options . get ( 'min_count' , None )) max_count = parse_int_or_none ( options . get ( 'max_count' , None )) result_data = BoldCli . search ( self . index_path , query , limit , offset , pos . to_int (), url , min_count , max_count ) return SearchResult ( count = result_data [ 'count' ], hits = [ SearchHit ( score = hit [ 'score' ], document = self . _parse_doc ( hit [ 'doc' ]) ) for hit in result_data [ 'hits' ] ], agg = result_data [ 'agg' ] )","title":"search()"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchHit","text":"Bases: Serializable Source code in datasets/services/search.py 65 66 67 68 @dataclass class SearchHit ( Serializable ): score : float document : Serializable | TermDocument","title":"SearchHit"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchHit.document","text":"","title":"document"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchHit.score","text":"","title":"score"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchResult","text":"Bases: Serializable Source code in datasets/services/search.py 71 72 73 74 75 76 @dataclass class SearchResult ( Serializable ): count : int = field ( default = 0 ) hits : List [ SearchHit | dict ] = field ( default_factory = list ) agg : dict = field ( default_factory = dict ) error : Optional [ str ] = None","title":"SearchResult"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchResult.agg","text":"","title":"agg"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchResult.count","text":"","title":"count"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchResult.error","text":"","title":"error"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchResult.hits","text":"","title":"hits"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchService","text":"Bases: ABC Source code in datasets/services/search.py 79 80 81 82 class SearchService ( ABC ): @abstractmethod def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : pass","title":"SearchService"},{"location":"reference/datasets/services/search/#datasets.services.search.SearchService.search","text":"Source code in datasets/services/search.py 80 81 82 @abstractmethod def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : pass","title":"search()"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument","text":"Bases: Serializable Source code in datasets/services/search.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @dataclass class TermDocument ( Serializable ): type : str search_text : str value : str pos : TermPos lang : Optional [ str ] = None rdf_type : Optional [ str ] = None label : Optional [ str ] = None description : Optional [ str ] = None count : Optional [ int ] = None range : Optional [ str ] = None def searchable_text ( self ): return ' ' . join ([ self . search_text , self . label or '' , self . description or '' , self . value or '' , ])","title":"TermDocument"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.count","text":"","title":"count"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.description","text":"","title":"description"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.label","text":"","title":"label"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.lang","text":"","title":"lang"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.pos","text":"","title":"pos"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.range","text":"","title":"range"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.rdf_type","text":"","title":"rdf_type"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.search_text","text":"","title":"search_text"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.type","text":"","title":"type"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.value","text":"","title":"value"},{"location":"reference/datasets/services/search/#datasets.services.search.TermDocument.searchable_text","text":"Source code in datasets/services/search.py 56 57 58 59 60 61 62 def searchable_text ( self ): return ' ' . join ([ self . search_text , self . label or '' , self . description or '' , self . value or '' , ])","title":"searchable_text()"},{"location":"reference/datasets/services/search/#datasets.services.search.TermPos","text":"Bases: Enum Source code in datasets/services/search.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class TermPos ( Enum ): SUBJECT = 'SUBJECT' PREDICATE = 'PREDICATE' OBJECT = 'OBJECT' def to_int ( self ): match self : case TermPos . SUBJECT : return 0 case TermPos . PREDICATE : return 1 case TermPos . OBJECT : return 2 @staticmethod def from_int ( value : int ): match value : case 0 : return TermPos . SUBJECT case 1 : return TermPos . PREDICATE case 2 : return TermPos . OBJECT","title":"TermPos"},{"location":"reference/datasets/services/search/#datasets.services.search.TermPos.OBJECT","text":"","title":"OBJECT"},{"location":"reference/datasets/services/search/#datasets.services.search.TermPos.PREDICATE","text":"","title":"PREDICATE"},{"location":"reference/datasets/services/search/#datasets.services.search.TermPos.SUBJECT","text":"","title":"SUBJECT"},{"location":"reference/datasets/services/search/#datasets.services.search.TermPos.from_int","text":"Source code in datasets/services/search.py 32 33 34 35 36 37 38 39 40 @staticmethod def from_int ( value : int ): match value : case 0 : return TermPos . SUBJECT case 1 : return TermPos . PREDICATE case 2 : return TermPos . OBJECT","title":"from_int()"},{"location":"reference/datasets/services/search/#datasets.services.search.TermPos.to_int","text":"Source code in datasets/services/search.py 23 24 25 26 27 28 29 30 def to_int ( self ): match self : case TermPos . SUBJECT : return 0 case TermPos . PREDICATE : return 1 case TermPos . OBJECT : return 2","title":"to_int()"},{"location":"reference/datasets/services/search/#datasets.services.search.TriplyDBSearchService","text":"Bases: SearchService Source code in datasets/services/search.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 class TriplyDBSearchService ( SearchService ): namespace : str def __init__ ( self , namespace : str ): self . namespace = namespace def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : endpoint = self . get_endpoint () if not endpoint : raise ValidationError ( 'Search endpoint is not reachable' ) query = self . build_query ( query , pos , limit , offset ) response = requests . post ( endpoint , data = json . dumps ( query ), timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () print ( result_data ) return SearchResult ( count = result_data [ 'hits' ][ 'total' ][ 'value' ], hits = [ SearchHit ( score = hit [ '_score' ], document = self . _parse_doc ( hit [ '_source' ]) ) for hit in result_data [ 'hits' ][ 'hits' ] ], ) # noinspection PyTypeChecker def build_query ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 ): base_query = { 'size' : limit , 'from' : offset , 'query' : { \"bool\" : defaultdict ( dict )}, } if pos != TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must_not' ] = [{ \"terms\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : [ \"http://www.w3.org/2002/07/owl#DatatypeProperty\" ] } }] if query : base_query [ 'query' ][ 'bool' ][ 'must' ] = [{ \"simple_query_string\" : { \"query\" : query } }] base_query [ 'query' ][ 'bool' ][ 'should' ] = [{ \"simple_query_string\" : { \"query\" : query , \"fields\" : [ \"http://www w3 org/2000/01/rdf-schema#label\" ] } }] if pos == TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must' ] = [ * base_query [ 'query' ][ 'bool' ] . get ( 'must' , []), { \"match\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : \"http://www.w3.org/2002/07/owl#DatatypeProperty\" } } ] return base_query def get_endpoint ( self ): response = requests . get ( f 'https://api.triplydb.com/datasets/ { self . namespace } /services/' , timeout = 5 , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return None result_data = response . json () for service in result_data : if service . get ( 'type' , None ) == 'elasticSearch' : return service [ 'endpoint' ] return None def _parse_doc ( self , doc : dict ) -> TermDocument : iri = doc [ '@id' ] rdf_type = doc . get ( 'http://www w3 org/1999/02/22-rdf-syntax-ns#type' , None ) pos = TermPos . PREDICATE if rdf_type == 'http://www.w3.org/2002/07/owl#DatatypeProperty' \\ else ( TermPos . SUBJECT if 'http' in iri else TermPos . OBJECT ) label = doc . get ( 'http://www w3 org/2000/01/rdf-schema#label' , None ) search_text = label if label else re . sub ( r '[-_#]' , ' ' , iri . split ( '/' )[ - 1 ] . split ( '#' )[ - 1 ]) return TermDocument ( type = 'uri' , value = doc [ '@id' ], pos = pos , rdf_type = first_or_self ( rdf_type ), label = first_or_self ( label ), description = first_or_self ( doc . get ( 'http://www w3 org/2000/01/rdf-schema#comment' , None )), count = None , search_text = first_or_self ( search_text ), range = doc . get ( 'http://www w3 org/2000/01/rdf-schema#range' , None ), )","title":"TriplyDBSearchService"},{"location":"reference/datasets/services/search/#datasets.services.search.TriplyDBSearchService.namespace","text":"","title":"namespace"},{"location":"reference/datasets/services/search/#datasets.services.search.TriplyDBSearchService.__init__","text":"Source code in datasets/services/search.py 198 199 def __init__ ( self , namespace : str ): self . namespace = namespace","title":"__init__()"},{"location":"reference/datasets/services/search/#datasets.services.search.TriplyDBSearchService.build_query","text":"Source code in datasets/services/search.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def build_query ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 ): base_query = { 'size' : limit , 'from' : offset , 'query' : { \"bool\" : defaultdict ( dict )}, } if pos != TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must_not' ] = [{ \"terms\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : [ \"http://www.w3.org/2002/07/owl#DatatypeProperty\" ] } }] if query : base_query [ 'query' ][ 'bool' ][ 'must' ] = [{ \"simple_query_string\" : { \"query\" : query } }] base_query [ 'query' ][ 'bool' ][ 'should' ] = [{ \"simple_query_string\" : { \"query\" : query , \"fields\" : [ \"http://www w3 org/2000/01/rdf-schema#label\" ] } }] if pos == TermPos . PREDICATE : base_query [ 'query' ][ 'bool' ][ 'must' ] = [ * base_query [ 'query' ][ 'bool' ] . get ( 'must' , []), { \"match\" : { \"http://www w3 org/1999/02/22-rdf-syntax-ns#type\" : \"http://www.w3.org/2002/07/owl#DatatypeProperty\" } } ] return base_query","title":"build_query()"},{"location":"reference/datasets/services/search/#datasets.services.search.TriplyDBSearchService.get_endpoint","text":"Source code in datasets/services/search.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 def get_endpoint ( self ): response = requests . get ( f 'https://api.triplydb.com/datasets/ { self . namespace } /services/' , timeout = 5 , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return None result_data = response . json () for service in result_data : if service . get ( 'type' , None ) == 'elasticSearch' : return service [ 'endpoint' ] return None","title":"get_endpoint()"},{"location":"reference/datasets/services/search/#datasets.services.search.TriplyDBSearchService.search","text":"Source code in datasets/services/search.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : endpoint = self . get_endpoint () if not endpoint : raise ValidationError ( 'Search endpoint is not reachable' ) query = self . build_query ( query , pos , limit , offset ) response = requests . post ( endpoint , data = json . dumps ( query ), timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () print ( result_data ) return SearchResult ( count = result_data [ 'hits' ][ 'total' ][ 'value' ], hits = [ SearchHit ( score = hit [ '_score' ], document = self . _parse_doc ( hit [ '_source' ]) ) for hit in result_data [ 'hits' ][ 'hits' ] ], )","title":"search()"},{"location":"reference/datasets/services/search/#datasets.services.search.WikidataSearchService","text":"Bases: SearchService Source code in datasets/services/search.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class WikidataSearchService ( SearchService ): def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : type = 'item' if pos == TermPos . PREDICATE : type = 'property' response = requests . get ( 'https://www.wikidata.org/w/api.php' , params = { 'action' : 'wbsearchentities' , 'search' : query , 'language' : 'en' , 'uselang' : 'en' , 'type' : type , 'format' : 'json' , 'formatversion' : 2 , 'errorformat' : 'plaintext' , 'limit' : limit , 'continue' : offset , }, timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , } ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () return SearchResult ( count = 999999 if 'search-continue' in result_data else offset + len ( result_data . get ( 'search' , [])), hits = [ SearchHit ( score = 1.0 , document = self . _parse_doc ( hit , pos ), ) for hit in result_data . get ( 'search' , []) ], ) def _parse_doc ( self , doc : dict , pos : TermPos ) -> TermDocument : iri = doc [ 'concepturi' ] if pos == TermPos . PREDICATE : iri = f 'http://www.wikidata.org/prop/direct/ { doc [ \"id\" ] } ' return TermDocument ( type = 'uri' , value = iri , pos = pos , rdf_type = None , label = doc . get ( 'label' , None ), description = doc . get ( 'description' , None ), count = doc . get ( 'count' , None ), search_text = doc . get ( 'label' , None ), )","title":"WikidataSearchService"},{"location":"reference/datasets/services/search/#datasets.services.search.WikidataSearchService.search","text":"Source code in datasets/services/search.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def search ( self , query , pos : TermPos , limit : int = 100 , offset : int = 0 , timeout = 5000 , ** options ) -> SearchResult : type = 'item' if pos == TermPos . PREDICATE : type = 'property' response = requests . get ( 'https://www.wikidata.org/w/api.php' , params = { 'action' : 'wbsearchentities' , 'search' : query , 'language' : 'en' , 'uselang' : 'en' , 'type' : type , 'format' : 'json' , 'formatversion' : 2 , 'errorformat' : 'plaintext' , 'limit' : limit , 'continue' : offset , }, timeout = timeout , headers = { 'User-Agent' : 'https://github.com/EgorDm/BOLD' , } ) if response . status_code != 200 : return SearchResult ( error = response . text ) result_data = response . json () return SearchResult ( count = 999999 if 'search-continue' in result_data else offset + len ( result_data . get ( 'search' , [])), hits = [ SearchHit ( score = 1.0 , document = self . _parse_doc ( hit , pos ), ) for hit in result_data . get ( 'search' , []) ], )","title":"search()"},{"location":"reference/datasets/services/search/#datasets.services.search.first_or_self","text":"Source code in datasets/services/search.py 321 322 323 324 325 def first_or_self ( item : Union [ List [ Any ], Any ]): if isinstance ( item , list ): return item [ 0 ] return item","title":"first_or_self()"},{"location":"reference/datasets/services/search/#datasets.services.search.merge_results","text":"Source code in datasets/services/search.py 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 def merge_results ( a : SearchResult , b : SearchResult , query : str , ) -> SearchResult : score_fn = lambda x : fuzz . partial_ratio ( x . document . searchable_text (), query ) a_hits = deque (( score_fn ( hit ), hit ) for hit in a . hits ) b_hits = deque (( score_fn ( hit ), hit ) for hit in b . hits ) seen = set () hits = [] while a_hits and b_hits : if a_hits [ 0 ][ 0 ] > b_hits [ 0 ][ 0 ]: item = a_hits . popleft ()[ 1 ] else : item = b_hits . popleft ()[ 1 ] if item . document . value not in seen : hits . append ( item ) seen . add ( item . document . value ) for ( _ , item ) in a_hits + b_hits : if item . document . value not in seen : hits . append ( item ) seen . add ( item . document . value ) return SearchResult ( count = len ( hits ), hits = hits , )","title":"merge_results()"},{"location":"reference/datasets/services/search/#datasets.services.search.parse_int_or_none","text":"Source code in datasets/services/search.py 85 86 87 88 def parse_int_or_none ( value : str ) -> int : if value is None : return None return int ( value )","title":"parse_int_or_none()"},{"location":"reference/datasets/services/stardog_api/","text":"StardogApi Source code in datasets/services/stardog_api.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class StardogApi : endpoint : str credentials : str token : Optional [ str ] = None @staticmethod def admin () -> 'stardog.Admin' : return stardog . Admin ( ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS }) @staticmethod def connection ( name : str ) -> 'stardog.Connection' : return stardog . Connection ( name , ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS }) credentials : str class-attribute endpoint : str class-attribute token : Optional [ str ] = None class-attribute admin () staticmethod Source code in datasets/services/stardog_api.py 13 14 15 16 17 18 19 @staticmethod def admin () -> 'stardog.Admin' : return stardog . Admin ( ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS }) connection ( name ) staticmethod Source code in datasets/services/stardog_api.py 21 22 23 24 25 26 27 @staticmethod def connection ( name : str ) -> 'stardog.Connection' : return stardog . Connection ( name , ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS })","title":"stardog_api"},{"location":"reference/datasets/services/stardog_api/#datasets.services.stardog_api.StardogApi","text":"Source code in datasets/services/stardog_api.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class StardogApi : endpoint : str credentials : str token : Optional [ str ] = None @staticmethod def admin () -> 'stardog.Admin' : return stardog . Admin ( ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS }) @staticmethod def connection ( name : str ) -> 'stardog.Connection' : return stardog . Connection ( name , ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS })","title":"StardogApi"},{"location":"reference/datasets/services/stardog_api/#datasets.services.stardog_api.StardogApi.credentials","text":"","title":"credentials"},{"location":"reference/datasets/services/stardog_api/#datasets.services.stardog_api.StardogApi.endpoint","text":"","title":"endpoint"},{"location":"reference/datasets/services/stardog_api/#datasets.services.stardog_api.StardogApi.token","text":"","title":"token"},{"location":"reference/datasets/services/stardog_api/#datasets.services.stardog_api.StardogApi.admin","text":"Source code in datasets/services/stardog_api.py 13 14 15 16 17 18 19 @staticmethod def admin () -> 'stardog.Admin' : return stardog . Admin ( ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS })","title":"admin()"},{"location":"reference/datasets/services/stardog_api/#datasets.services.stardog_api.StardogApi.connection","text":"Source code in datasets/services/stardog_api.py 21 22 23 24 25 26 27 @staticmethod def connection ( name : str ) -> 'stardog.Connection' : return stardog . Connection ( name , ** { 'endpoint' : settings . STARDOG_ENDPOINT , 'username' : settings . STARDOG_USER , 'password' : settings . STARDOG_PASS })","title":"connection()"},{"location":"reference/datasets/tasks/","text":"","title":"tasks"},{"location":"reference/datasets/tasks/imports/","text":"logger = get_logger () module-attribute download_url ( url , path = None ) Source code in datasets/tasks/imports.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @shared_task () def download_url ( url : str , path : str = None ) -> Optional [ Path ]: logger . info ( f \"Downloading knowledge graph from { url } \" ) download_folder = ( Path ( path ) if path else DOWNLOAD_DIR ) / random_string ( 10 ) download_folder . mkdir ( parents = True ) try : if 'github.com' in url and 'raw' not in url : logger . info ( f \"Downloading from github raw\" ) url += ( '&raw=true' if urlparse ( url ) . query else '?raw=true' ) except Exception as e : logger . error ( f \"Error preprocessing { url } : { e } \" ) raise Exception ( e ) try : filename = None logger . info ( 'Trying to infer file name' ) remotefile = urlopen ( url ) headerblob = remotefile . info () . get ( 'Content-Disposition' , None ) if headerblob : value , params = cgi . parse_header ( headerblob ) filename = params . get ( 'filename' , None ) if filename is None : filename = os . path . basename ( urlparse ( url ) . path ) except Exception as e : logger . error ( f \"Failed to parse URL { url } . Error: { e } \" ) shutil . rmtree ( download_folder ) raise Exception ( e ) download_path = download_folder / filename try : logger . info ( f \"Downloading { url } to { download_path } \" ) # TODO: add a hook to check if file is not too big urlretrieve ( url , download_path ) logger . info ( f \"Downloaded { url } to { download_path } \" ) except Exception as e : logger . error ( f \"Failed to download { url } . Error: { e } \" ) shutil . rmtree ( download_folder ) raise Exception ( e ) # TODO: We may need to rename the file extension. return download_path import_files ( files , database = None ) Source code in datasets/tasks/imports.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 @shared_task () def import_files ( files : List [ Path ], database : Optional [ str ] = None ) -> str : if database is None : database = 'a' + random_string ( 10 ) logger . info ( f \"Starting KG import { files } into { database } \" ) if isinstance ( files , str ): files = Path ( files ) if isinstance ( files , ( Path , str )): if files . is_dir (): files = list ( files . glob ( '**/*' )) else : files = [ files ] logger . info ( f \"Loading KG from { files } \" ) with StardogApi . admin () as admin : admin . new_database ( database , { 'strict.parsing' : False }, * [ stardog . content . File ( str ( file . absolute ())) for file in files ] ) return database","title":"imports"},{"location":"reference/datasets/tasks/imports/#datasets.tasks.imports.logger","text":"","title":"logger"},{"location":"reference/datasets/tasks/imports/#datasets.tasks.imports.download_url","text":"Source code in datasets/tasks/imports.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @shared_task () def download_url ( url : str , path : str = None ) -> Optional [ Path ]: logger . info ( f \"Downloading knowledge graph from { url } \" ) download_folder = ( Path ( path ) if path else DOWNLOAD_DIR ) / random_string ( 10 ) download_folder . mkdir ( parents = True ) try : if 'github.com' in url and 'raw' not in url : logger . info ( f \"Downloading from github raw\" ) url += ( '&raw=true' if urlparse ( url ) . query else '?raw=true' ) except Exception as e : logger . error ( f \"Error preprocessing { url } : { e } \" ) raise Exception ( e ) try : filename = None logger . info ( 'Trying to infer file name' ) remotefile = urlopen ( url ) headerblob = remotefile . info () . get ( 'Content-Disposition' , None ) if headerblob : value , params = cgi . parse_header ( headerblob ) filename = params . get ( 'filename' , None ) if filename is None : filename = os . path . basename ( urlparse ( url ) . path ) except Exception as e : logger . error ( f \"Failed to parse URL { url } . Error: { e } \" ) shutil . rmtree ( download_folder ) raise Exception ( e ) download_path = download_folder / filename try : logger . info ( f \"Downloading { url } to { download_path } \" ) # TODO: add a hook to check if file is not too big urlretrieve ( url , download_path ) logger . info ( f \"Downloaded { url } to { download_path } \" ) except Exception as e : logger . error ( f \"Failed to download { url } . Error: { e } \" ) shutil . rmtree ( download_folder ) raise Exception ( e ) # TODO: We may need to rename the file extension. return download_path","title":"download_url()"},{"location":"reference/datasets/tasks/imports/#datasets.tasks.imports.import_files","text":"Source code in datasets/tasks/imports.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 @shared_task () def import_files ( files : List [ Path ], database : Optional [ str ] = None ) -> str : if database is None : database = 'a' + random_string ( 10 ) logger . info ( f \"Starting KG import { files } into { database } \" ) if isinstance ( files , str ): files = Path ( files ) if isinstance ( files , ( Path , str )): if files . is_dir (): files = list ( files . glob ( '**/*' )) else : files = [ files ] logger . info ( f \"Loading KG from { files } \" ) with StardogApi . admin () as admin : admin . new_database ( database , { 'strict.parsing' : False }, * [ stardog . content . File ( str ( file . absolute ())) for file in files ] ) return database","title":"import_files()"},{"location":"reference/datasets/tasks/maintenance/","text":"logger = get_logger () module-attribute update_dataset_info ( dataset_id ) Source code in datasets/tasks/maintenance.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @shared_task () def update_dataset_info ( dataset_id : UUID ): dataset = Dataset . objects . get ( id = dataset_id ) match dataset . mode : case Dataset . Mode . LOCAL . value : if dataset . local_database is None : raise Exception ( \"Dataset has no database\" ) database = dataset . local_database with StardogApi . admin () as admin : database_api = admin . database ( database ) logger . info ( 'Retrieving namespace info' ) namespaces = database_api . namespaces () with StardogApi . connection ( database ) as conn : logger . info ( 'Getting number of triples' ) triple_count = conn . size ( exact = False ) case Dataset . Mode . SPARQL . value : if dataset . sparql_endpoint is None : raise Exception ( \"Dataset has no database\" ) namespaces = [] triple_count = int ( dataset . get_query_service () . query_select ( ''' SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o } ''' , limit = 1 , ignore_limit = True ) . get ( 'results' ) . get ( 'bindings' )[ 0 ] . get ( 'count' ) . get ( 'value' )) case _ : raise Exception ( f \"Unsupported mode { dataset . mode } \" ) # path = ['results', 'bindings', 0, 'count', 'value'] # ATOM_QUERY = 'SELECT (COUNT(DISTINCT {}) as ?count) {{ ?s ?p ?o }}' # subject_count = deepget(client.query(database, ATOM_QUERY.format(\"?s\")), path, default=-1) # predicate_count = deepget(client.query(database, ATOM_QUERY.format(\"?p\")), path, default=-1) # object_count = deepget(client.query(database, ATOM_QUERY.format(\"?o\")), path, default=-1) logger . info ( 'Saving dataset info' ) dataset = Dataset . objects . get ( id = dataset_id ) dataset . namespaces = namespaces dataset . statistics = { ** ( dataset . statistics or {}), 'triple_count' : triple_count , } dataset . save () logger . info ( 'Successfully updated dataset info' )","title":"maintenance"},{"location":"reference/datasets/tasks/maintenance/#datasets.tasks.maintenance.logger","text":"","title":"logger"},{"location":"reference/datasets/tasks/maintenance/#datasets.tasks.maintenance.update_dataset_info","text":"Source code in datasets/tasks/maintenance.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @shared_task () def update_dataset_info ( dataset_id : UUID ): dataset = Dataset . objects . get ( id = dataset_id ) match dataset . mode : case Dataset . Mode . LOCAL . value : if dataset . local_database is None : raise Exception ( \"Dataset has no database\" ) database = dataset . local_database with StardogApi . admin () as admin : database_api = admin . database ( database ) logger . info ( 'Retrieving namespace info' ) namespaces = database_api . namespaces () with StardogApi . connection ( database ) as conn : logger . info ( 'Getting number of triples' ) triple_count = conn . size ( exact = False ) case Dataset . Mode . SPARQL . value : if dataset . sparql_endpoint is None : raise Exception ( \"Dataset has no database\" ) namespaces = [] triple_count = int ( dataset . get_query_service () . query_select ( ''' SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o } ''' , limit = 1 , ignore_limit = True ) . get ( 'results' ) . get ( 'bindings' )[ 0 ] . get ( 'count' ) . get ( 'value' )) case _ : raise Exception ( f \"Unsupported mode { dataset . mode } \" ) # path = ['results', 'bindings', 0, 'count', 'value'] # ATOM_QUERY = 'SELECT (COUNT(DISTINCT {}) as ?count) {{ ?s ?p ?o }}' # subject_count = deepget(client.query(database, ATOM_QUERY.format(\"?s\")), path, default=-1) # predicate_count = deepget(client.query(database, ATOM_QUERY.format(\"?p\")), path, default=-1) # object_count = deepget(client.query(database, ATOM_QUERY.format(\"?o\")), path, default=-1) logger . info ( 'Saving dataset info' ) dataset = Dataset . objects . get ( id = dataset_id ) dataset . namespaces = namespaces dataset . statistics = { ** ( dataset . statistics or {}), 'triple_count' : triple_count , } dataset . save () logger . info ( 'Successfully updated dataset info' )","title":"update_dataset_info()"},{"location":"reference/datasets/tasks/pipeline/","text":"logger = get_logger () module-attribute delete_dataset ( dataset_id ) Source code in datasets/tasks/pipeline.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @shared_task () def delete_dataset ( dataset_id : UUID ) -> str : dataset = Dataset . objects . get ( id = dataset_id ) logger . info ( f \"Deleting dataset { dataset . name } \" ) if dataset . search_mode == Dataset . SearchMode . LOCAL . value : if dataset . search_index_path and dataset . search_index_path . exists (): logger . info ( f \"Deleting search index { dataset . search_index_path } \" ) shutil . rmtree ( dataset . search_index_path ) if dataset . mode == Dataset . Mode . LOCAL . value and dataset . local_database : logger . info ( f \"Deleting database { dataset . local_database } \" ) with StardogApi . admin () as admin : admin . database ( dataset . local_database ) . drop () dataset . delete () import_dataset ( dataset_id , files = None ) Source code in datasets/tasks/pipeline.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 @shared_task () def import_dataset ( dataset_id : UUID , files : List [ str ] = None ) -> str : dataset = Dataset . objects . get ( id = dataset_id ) source = dataset . source logger . info ( f \"Importing dataset { dataset . name } \" ) Dataset . objects . filter ( id = dataset_id ) . update ( state = DatasetState . IMPORTING . value , import_task_id = import_dataset . request . id , ) tmp_dir = DOWNLOAD_DIR / random_string ( 10 ) tmp_dir . mkdir ( parents = True ) files = files or [] files = list ( map ( Path , files )) try : source_type = source . get ( 'source_type' , None ) match ( dataset . mode , source_type ): case ( Dataset . Mode . LOCAL . value , 'urls' ): urls = source . get ( 'urls' , []) if len ( urls ) == 0 : raise Exception ( \"No URLs specified\" ) logger . info ( f \"Downloading { len ( urls ) } files\" ) files = [] for url in set ( urls ): file = download_url ( url , str ( tmp_dir )) files . append ( file ) logger . info ( f \"Importing { len ( files ) } files\" ) dataset . local_database = import_files ( files ) logger . info ( f 'Created database { dataset . local_database } ' ) case ( Dataset . Mode . LOCAL . value , 'existing' ): dataset . local_database = source . get ( 'database' , None ) logger . info ( f 'Using existing database { dataset . local_database } ' ) case ( Dataset . Mode . SPARQL . value , 'sparql' ): dataset . sparql_endpoint = source . get ( 'sparql' , None ) logger . info ( f 'Using sparql endpoint { dataset . sparql_endpoint } ' ) case ( Dataset . Mode . LOCAL . value , 'upload' ): if len ( files ) == 0 : raise Exception ( \"No files specified\" ) dataset . local_database = import_files ( files ) logger . info ( f 'Using existing database { dataset . local_database } ' ) case _ : raise Exception ( f \"Unsupported source type { source_type } \" ) dataset . save () logger . info ( f \"Updating dataset info\" ) update_dataset_info ( dataset_id ) create_default_search_index ( path = str ( tmp_dir ), force = False ) if dataset . search_mode == Dataset . SearchMode . LOCAL . value : logger . info ( f \"Creating search index\" ) create_search_index ( dataset_id , path = str ( tmp_dir )) logger . info ( f \"Import finished\" ) Dataset . objects . filter ( id = dataset_id ) . update ( state = DatasetState . IMPORTED . value ) except Exception as e : logger . error ( f \"Error importing dataset { dataset . name } : { e } \" ) Dataset . objects . filter ( id = dataset_id ) . update ( state = DatasetState . FAILED . value ) raise e finally : logger . info ( f \"Cleaning up { tmp_dir } \" ) shutil . rmtree ( tmp_dir , ignore_errors = True ) for file in files : logger . info ( f \"Cleaning up { file } \" ) try : file . unlink () except Exception as e : logger . error ( f \"Error deleting { file } : { e } \" )","title":"pipeline"},{"location":"reference/datasets/tasks/pipeline/#datasets.tasks.pipeline.logger","text":"","title":"logger"},{"location":"reference/datasets/tasks/pipeline/#datasets.tasks.pipeline.delete_dataset","text":"Source code in datasets/tasks/pipeline.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @shared_task () def delete_dataset ( dataset_id : UUID ) -> str : dataset = Dataset . objects . get ( id = dataset_id ) logger . info ( f \"Deleting dataset { dataset . name } \" ) if dataset . search_mode == Dataset . SearchMode . LOCAL . value : if dataset . search_index_path and dataset . search_index_path . exists (): logger . info ( f \"Deleting search index { dataset . search_index_path } \" ) shutil . rmtree ( dataset . search_index_path ) if dataset . mode == Dataset . Mode . LOCAL . value and dataset . local_database : logger . info ( f \"Deleting database { dataset . local_database } \" ) with StardogApi . admin () as admin : admin . database ( dataset . local_database ) . drop () dataset . delete ()","title":"delete_dataset()"},{"location":"reference/datasets/tasks/pipeline/#datasets.tasks.pipeline.import_dataset","text":"Source code in datasets/tasks/pipeline.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 @shared_task () def import_dataset ( dataset_id : UUID , files : List [ str ] = None ) -> str : dataset = Dataset . objects . get ( id = dataset_id ) source = dataset . source logger . info ( f \"Importing dataset { dataset . name } \" ) Dataset . objects . filter ( id = dataset_id ) . update ( state = DatasetState . IMPORTING . value , import_task_id = import_dataset . request . id , ) tmp_dir = DOWNLOAD_DIR / random_string ( 10 ) tmp_dir . mkdir ( parents = True ) files = files or [] files = list ( map ( Path , files )) try : source_type = source . get ( 'source_type' , None ) match ( dataset . mode , source_type ): case ( Dataset . Mode . LOCAL . value , 'urls' ): urls = source . get ( 'urls' , []) if len ( urls ) == 0 : raise Exception ( \"No URLs specified\" ) logger . info ( f \"Downloading { len ( urls ) } files\" ) files = [] for url in set ( urls ): file = download_url ( url , str ( tmp_dir )) files . append ( file ) logger . info ( f \"Importing { len ( files ) } files\" ) dataset . local_database = import_files ( files ) logger . info ( f 'Created database { dataset . local_database } ' ) case ( Dataset . Mode . LOCAL . value , 'existing' ): dataset . local_database = source . get ( 'database' , None ) logger . info ( f 'Using existing database { dataset . local_database } ' ) case ( Dataset . Mode . SPARQL . value , 'sparql' ): dataset . sparql_endpoint = source . get ( 'sparql' , None ) logger . info ( f 'Using sparql endpoint { dataset . sparql_endpoint } ' ) case ( Dataset . Mode . LOCAL . value , 'upload' ): if len ( files ) == 0 : raise Exception ( \"No files specified\" ) dataset . local_database = import_files ( files ) logger . info ( f 'Using existing database { dataset . local_database } ' ) case _ : raise Exception ( f \"Unsupported source type { source_type } \" ) dataset . save () logger . info ( f \"Updating dataset info\" ) update_dataset_info ( dataset_id ) create_default_search_index ( path = str ( tmp_dir ), force = False ) if dataset . search_mode == Dataset . SearchMode . LOCAL . value : logger . info ( f \"Creating search index\" ) create_search_index ( dataset_id , path = str ( tmp_dir )) logger . info ( f \"Import finished\" ) Dataset . objects . filter ( id = dataset_id ) . update ( state = DatasetState . IMPORTED . value ) except Exception as e : logger . error ( f \"Error importing dataset { dataset . name } : { e } \" ) Dataset . objects . filter ( id = dataset_id ) . update ( state = DatasetState . FAILED . value ) raise e finally : logger . info ( f \"Cleaning up { tmp_dir } \" ) shutil . rmtree ( tmp_dir , ignore_errors = True ) for file in files : logger . info ( f \"Cleaning up { file } \" ) try : file . unlink () except Exception as e : logger . error ( f \"Error deleting { file } : { e } \" )","title":"import_dataset()"},{"location":"reference/datasets/tasks/processing/","text":"QUERY_EXPORT_SEARCH = \" \\n PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \\n SELECT \\n ?iri \\n (STR(?labelRaw) AS ?label) \\n ?count \\n ?pos \\n ?type \\n (STR(?descriptionRaw) AS ?description) \\n { \\n { \\n SELECT (?t as ?iri) (COUNT(?t) as ?count) ( {pos} as ?pos) \\n {triple} \\n GROUP BY ?t HAVING (?count > {min_count} ) \\n } \\n\\n OPTIONAL { \\n ?iri rdfs:label ?labelRaw. \\n FILTER (STRSTARTS(lang(?labelRaw), 'en') || lang(?labelRaw)='') \\n } \\n OPTIONAL { \\n ?iri rdfs:comment ?descriptionRaw. \\n FILTER (STRSTARTS(lang(?descriptionRaw), 'en') || lang(?descriptionRaw)='') \\n } \\n OPTIONAL { ?iri rdfs:type ?type } \\n } \\n \" module-attribute logger = get_logger () module-attribute create_default_search_index ( path = None , force = True ) Source code in datasets/tasks/processing.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @shared_task () def create_default_search_index ( path : str = None , force : bool = True , ): logger . info ( f \"Creating default search index\" ) search_index_dir = DEFAULT_SEARCH_INDEX if search_index_dir . exists (): if force : logger . info ( f \"Removing existing search index at { search_index_dir } \" ) shutil . rmtree ( search_index_dir ) else : logger . info ( f \"Default search index already exists\" ) return search_index_dir . mkdir ( parents = True , exist_ok = True ) tmp_dir = ( Path ( path ) if path else DOWNLOAD_DIR ) / random_string ( 10 ) tmp_dir . mkdir ( parents = True , exist_ok = True ) try : terms_files = [ settings . BASE_DIR . joinpath ( 'data' , 'rdf.tsv' ), settings . BASE_DIR . joinpath ( 'data' , 'rdfs.tsv' ), settings . BASE_DIR . joinpath ( 'data' , 'owl.tsv' ), settings . BASE_DIR . joinpath ( 'data' , 'foaf.tsv' ), ] logger . info ( 'Creating search index from documents' ) consume_print ( BoldCli . cmd ( [ 'build-index' , '--force' , * map ( str , terms_files ), '--index' , str ( search_index_dir )] )) logger . info ( 'Search index created' ) finally : logger . info ( f \"Cleaning up { tmp_dir } \" ) shutil . rmtree ( tmp_dir , ignore_errors = True ) create_search_index ( dataset_id , min_term_count = 3 , path = None , force = True ) Source code in datasets/tasks/processing.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 @shared_task () def create_search_index ( dataset_id : UUID , min_term_count : int = 3 , path : str = None , force : bool = True , ): dataset = Dataset . objects . get ( id = dataset_id ) logger . info ( f \"Creating search index for { dataset . name } \" ) database = dataset . local_database if database is None : raise Exception ( \"Dataset has no database\" ) search_index_dir = DATA_DIR / f 'search_index_ { database } ' if search_index_dir . exists (): if force : logger . info ( f \"Removing existing search index at { search_index_dir } \" ) shutil . rmtree ( search_index_dir ) else : logger . info ( f \"Search index already exists for { dataset . name } \" ) return search_index_dir . mkdir ( parents = True , exist_ok = True ) tmp_dir = ( Path ( path ) if path else DOWNLOAD_DIR ) / random_string ( 10 ) tmp_dir . mkdir ( parents = True , exist_ok = True ) try : terms_files = [] terms_s_file = tmp_dir / 'terms_s.tsv' query = QUERY_EXPORT_SEARCH \\ . replace ( ' {triple} ' , '{ ?t ?p ?v }' ) \\ . replace ( ' {min_count} ' , str ( min_term_count )) \\ . replace ( ' {pos} ' , '0' ) logger . info ( f 'Exporting subject search terms { terms_s_file } ' ) query_to_file ( database , query , terms_s_file , timeout = 60 * 60 * 1000 ) terms_files . append ( terms_s_file ) terms_p_file = tmp_dir / 'terms_p.tsv' query = QUERY_EXPORT_SEARCH \\ . replace ( ' {triple} ' , '{ ?s ?t ?v }' ) \\ . replace ( ' {min_count} ' , str ( min_term_count )) \\ . replace ( ' {pos} ' , '1' ) logger . info ( f 'Exporting predicate search terms { terms_p_file } ' ) query_to_file ( database , query , terms_p_file , timeout = 60 * 60 * 1000 ) terms_files . append ( terms_p_file ) terms_o_file = tmp_dir / 'terms_o.tsv' query = QUERY_EXPORT_SEARCH \\ . replace ( ' {triple} ' , '{ ?s ?p ?t FILTER(?p != rdfs:label) }' ) \\ . replace ( ' {min_count} ' , str ( min_term_count )) \\ . replace ( ' {pos} ' , '2' ) logger . info ( f 'Exporting object search terms { terms_o_file } ' ) query_to_file ( database , query , terms_o_file , timeout = 60 * 60 * 1000 ) terms_files . append ( terms_o_file ) logger . info ( 'Creating search index from documents' ) consume_print ( BoldCli . cmd ( [ 'build-index' , '--force' , * map ( str , terms_files ), '--index' , str ( search_index_dir )] )) logger . info ( 'Search index created' ) finally : logger . info ( f \"Cleaning up { tmp_dir } \" ) shutil . rmtree ( tmp_dir , ignore_errors = True ) query_to_file ( database , query , file , timeout = 5000 , ** kwargs ) Source code in datasets/tasks/processing.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def query_to_file ( database : str , query : str , file : Path , timeout = 5000 , ** kwargs ): endpoint = settings . STARDOG_ENDPOINT . rstrip ( '/' ) credentials = base64 . b64encode ( f ' { settings . STARDOG_USER } : { settings . STARDOG_PASS } ' . encode ( 'utf-8' )) . decode ( 'utf-8' ) headers = { 'Content-Type' : 'application/sparql-query' , 'Accept' : 'text/tsv' , 'Authorization' : f 'Basic { credentials } ' , } response = requests . post ( f ' { endpoint } / { database } /query' , headers = headers , data = query , params = { ** kwargs , 'timeout' : timeout , }, stream = True ) with response as r : r . raw . decode_content = True with file . open ( 'wb' ) as f : # https://stackoverflow.com/a/49684845 shutil . copyfileobj ( r . raw , f )","title":"processing"},{"location":"reference/datasets/tasks/processing/#datasets.tasks.processing.QUERY_EXPORT_SEARCH","text":"","title":"QUERY_EXPORT_SEARCH"},{"location":"reference/datasets/tasks/processing/#datasets.tasks.processing.logger","text":"","title":"logger"},{"location":"reference/datasets/tasks/processing/#datasets.tasks.processing.create_default_search_index","text":"Source code in datasets/tasks/processing.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @shared_task () def create_default_search_index ( path : str = None , force : bool = True , ): logger . info ( f \"Creating default search index\" ) search_index_dir = DEFAULT_SEARCH_INDEX if search_index_dir . exists (): if force : logger . info ( f \"Removing existing search index at { search_index_dir } \" ) shutil . rmtree ( search_index_dir ) else : logger . info ( f \"Default search index already exists\" ) return search_index_dir . mkdir ( parents = True , exist_ok = True ) tmp_dir = ( Path ( path ) if path else DOWNLOAD_DIR ) / random_string ( 10 ) tmp_dir . mkdir ( parents = True , exist_ok = True ) try : terms_files = [ settings . BASE_DIR . joinpath ( 'data' , 'rdf.tsv' ), settings . BASE_DIR . joinpath ( 'data' , 'rdfs.tsv' ), settings . BASE_DIR . joinpath ( 'data' , 'owl.tsv' ), settings . BASE_DIR . joinpath ( 'data' , 'foaf.tsv' ), ] logger . info ( 'Creating search index from documents' ) consume_print ( BoldCli . cmd ( [ 'build-index' , '--force' , * map ( str , terms_files ), '--index' , str ( search_index_dir )] )) logger . info ( 'Search index created' ) finally : logger . info ( f \"Cleaning up { tmp_dir } \" ) shutil . rmtree ( tmp_dir , ignore_errors = True )","title":"create_default_search_index()"},{"location":"reference/datasets/tasks/processing/#datasets.tasks.processing.create_search_index","text":"Source code in datasets/tasks/processing.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 @shared_task () def create_search_index ( dataset_id : UUID , min_term_count : int = 3 , path : str = None , force : bool = True , ): dataset = Dataset . objects . get ( id = dataset_id ) logger . info ( f \"Creating search index for { dataset . name } \" ) database = dataset . local_database if database is None : raise Exception ( \"Dataset has no database\" ) search_index_dir = DATA_DIR / f 'search_index_ { database } ' if search_index_dir . exists (): if force : logger . info ( f \"Removing existing search index at { search_index_dir } \" ) shutil . rmtree ( search_index_dir ) else : logger . info ( f \"Search index already exists for { dataset . name } \" ) return search_index_dir . mkdir ( parents = True , exist_ok = True ) tmp_dir = ( Path ( path ) if path else DOWNLOAD_DIR ) / random_string ( 10 ) tmp_dir . mkdir ( parents = True , exist_ok = True ) try : terms_files = [] terms_s_file = tmp_dir / 'terms_s.tsv' query = QUERY_EXPORT_SEARCH \\ . replace ( ' {triple} ' , '{ ?t ?p ?v }' ) \\ . replace ( ' {min_count} ' , str ( min_term_count )) \\ . replace ( ' {pos} ' , '0' ) logger . info ( f 'Exporting subject search terms { terms_s_file } ' ) query_to_file ( database , query , terms_s_file , timeout = 60 * 60 * 1000 ) terms_files . append ( terms_s_file ) terms_p_file = tmp_dir / 'terms_p.tsv' query = QUERY_EXPORT_SEARCH \\ . replace ( ' {triple} ' , '{ ?s ?t ?v }' ) \\ . replace ( ' {min_count} ' , str ( min_term_count )) \\ . replace ( ' {pos} ' , '1' ) logger . info ( f 'Exporting predicate search terms { terms_p_file } ' ) query_to_file ( database , query , terms_p_file , timeout = 60 * 60 * 1000 ) terms_files . append ( terms_p_file ) terms_o_file = tmp_dir / 'terms_o.tsv' query = QUERY_EXPORT_SEARCH \\ . replace ( ' {triple} ' , '{ ?s ?p ?t FILTER(?p != rdfs:label) }' ) \\ . replace ( ' {min_count} ' , str ( min_term_count )) \\ . replace ( ' {pos} ' , '2' ) logger . info ( f 'Exporting object search terms { terms_o_file } ' ) query_to_file ( database , query , terms_o_file , timeout = 60 * 60 * 1000 ) terms_files . append ( terms_o_file ) logger . info ( 'Creating search index from documents' ) consume_print ( BoldCli . cmd ( [ 'build-index' , '--force' , * map ( str , terms_files ), '--index' , str ( search_index_dir )] )) logger . info ( 'Search index created' ) finally : logger . info ( f \"Cleaning up { tmp_dir } \" ) shutil . rmtree ( tmp_dir , ignore_errors = True )","title":"create_search_index()"},{"location":"reference/datasets/tasks/processing/#datasets.tasks.processing.query_to_file","text":"Source code in datasets/tasks/processing.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def query_to_file ( database : str , query : str , file : Path , timeout = 5000 , ** kwargs ): endpoint = settings . STARDOG_ENDPOINT . rstrip ( '/' ) credentials = base64 . b64encode ( f ' { settings . STARDOG_USER } : { settings . STARDOG_PASS } ' . encode ( 'utf-8' )) . decode ( 'utf-8' ) headers = { 'Content-Type' : 'application/sparql-query' , 'Accept' : 'text/tsv' , 'Authorization' : f 'Basic { credentials } ' , } response = requests . post ( f ' { endpoint } / { database } /query' , headers = headers , data = query , params = { ** kwargs , 'timeout' : timeout , }, stream = True ) with response as r : r . raw . decode_content = True with file . open ( 'wb' ) as f : # https://stackoverflow.com/a/49684845 shutil . copyfileobj ( r . raw , f )","title":"query_to_file()"},{"location":"reference/datasets/views/","text":"","title":"views"},{"location":"reference/datasets/views/datasets/","text":"DatasetViewSet Bases: viewsets . ModelViewSet API endpoint that allows users to be viewed or edited. Source code in datasets/views/datasets.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class DatasetViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = Dataset . objects . all () serializer_class = DatasetSerializer pagination_class = LimitOffsetPagination filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] filterset_fields = [ 'mode' , 'search_mode' , 'state' , 'id' , 'creator' ] search_fields = [ 'name' , 'source' , 'description' ] def perform_create ( self , serializer ): if serializer . validated_data . get ( 'mode' ) == Dataset . Mode . SPARQL . value and \\ serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value : raise ValidationError ( 'Local search index for sparql datasets is not yet supported' ) if serializer . validated_data . get ( 'search_mode' , None ) == Dataset . SearchMode . TRIPLYDB . value and \\ 'tdb_id' not in serializer . validated_data . get ( 'source' , {}): raise ValidationError ( 'TriplyDB dataset must be a TriplyDB dataset' ) if not settings . STARDOG_ENABLE and ( serializer . validated_data . get ( 'mode' ) != Dataset . Mode . SPARQL . value or serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value ): raise ValidationError ( 'Local datasets are not enabled on this server' ) super () . perform_create ( serializer ) instance : Dataset = serializer . instance instance . creator = self . request . user instance . save () files = None # If a files are uploaded, store them in a temporary folder if instance . source . get ( 'source_type' ) == 'upload' : tmp_dir = DOWNLOAD_DIR / random_string ( 10 ) tmp_dir . mkdir ( parents = True ) files = [] for file in self . request . FILES . getlist ( 'files' ): file_path = tmp_dir / file . name with file_path . open ( 'wb+' ) as destination : for chunk in file . chunks (): destination . write ( chunk ) files . append ( str ( file_path . absolute ())) instance . apply_async ( import_dataset , ( instance . id , files ), creator = self . request . user , name = f 'Import dataset { instance . name } ' ) def perform_destroy ( self , instance ): instance . apply_async ( delete_dataset , ( instance . id ,), creator = self . request . user , name = f 'Deleting dataset { instance . name } ' ) def perform_update ( self , serializer ): super () . perform_update ( serializer ) def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) return permissions filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] class-attribute filterset_fields = [ 'mode' , 'search_mode' , 'state' , 'id' , 'creator' ] class-attribute pagination_class = LimitOffsetPagination class-attribute queryset = Dataset . objects . all () class-attribute search_fields = [ 'name' , 'source' , 'description' ] class-attribute serializer_class = DatasetSerializer class-attribute get_permissions () Source code in datasets/views/datasets.py 90 91 92 93 94 95 96 def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) return permissions perform_create ( serializer ) Source code in datasets/views/datasets.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def perform_create ( self , serializer ): if serializer . validated_data . get ( 'mode' ) == Dataset . Mode . SPARQL . value and \\ serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value : raise ValidationError ( 'Local search index for sparql datasets is not yet supported' ) if serializer . validated_data . get ( 'search_mode' , None ) == Dataset . SearchMode . TRIPLYDB . value and \\ 'tdb_id' not in serializer . validated_data . get ( 'source' , {}): raise ValidationError ( 'TriplyDB dataset must be a TriplyDB dataset' ) if not settings . STARDOG_ENABLE and ( serializer . validated_data . get ( 'mode' ) != Dataset . Mode . SPARQL . value or serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value ): raise ValidationError ( 'Local datasets are not enabled on this server' ) super () . perform_create ( serializer ) instance : Dataset = serializer . instance instance . creator = self . request . user instance . save () files = None # If a files are uploaded, store them in a temporary folder if instance . source . get ( 'source_type' ) == 'upload' : tmp_dir = DOWNLOAD_DIR / random_string ( 10 ) tmp_dir . mkdir ( parents = True ) files = [] for file in self . request . FILES . getlist ( 'files' ): file_path = tmp_dir / file . name with file_path . open ( 'wb+' ) as destination : for chunk in file . chunks (): destination . write ( chunk ) files . append ( str ( file_path . absolute ())) instance . apply_async ( import_dataset , ( instance . id , files ), creator = self . request . user , name = f 'Import dataset { instance . name } ' ) perform_destroy ( instance ) Source code in datasets/views/datasets.py 79 80 81 82 83 84 85 def perform_destroy ( self , instance ): instance . apply_async ( delete_dataset , ( instance . id ,), creator = self . request . user , name = f 'Deleting dataset { instance . name } ' ) perform_update ( serializer ) Source code in datasets/views/datasets.py 87 88 def perform_update ( self , serializer ): super () . perform_update ( serializer ) dataset_query ( request , id ) Source code in datasets/views/datasets.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @swagger_auto_schema ( methods = [ 'post' ], manual_parameters = [ openapi . Parameter ( 'limit' , openapi . IN_QUERY , \"Limit\" , type = openapi . TYPE_INTEGER ), openapi . Parameter ( 'timeout' , openapi . IN_QUERY , \"Timeout\" , type = openapi . TYPE_INTEGER ), ], request_body = openapi . Schema ( type = openapi . TYPE_STRING , description = 'Query to be executed' )) @api_view ([ 'POST' ]) def dataset_query ( request : Request , id : UUID ): dataset = Dataset . objects . get ( id = id ) limit = int ( request . GET . get ( 'limit' , 10 )) timeout = int ( request . GET . get ( 'timeout' , 5000 )) query = request . body . decode ( 'utf-8' ) result = dataset . get_query_service () . query ( query , limit , timeout ) return JsonResponse ( result ) parse_int_or_none ( value ) Source code in datasets/views/datasets.py 99 100 101 102 def parse_int_or_none ( value : str ) -> int : if value is None : return None return int ( value ) term_search ( request , id ) Source code in datasets/views/datasets.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 @swagger_auto_schema ( methods = [ 'get' ], manual_parameters = [ openapi . Parameter ( 'query' , openapi . IN_QUERY , \"Search query\" , type = openapi . TYPE_STRING ), openapi . Parameter ( 'pos' , openapi . IN_QUERY , \"Term Position\" , type = openapi . TYPE_STRING ), openapi . Parameter ( 'limit' , openapi . IN_QUERY , \"Limit\" , type = openapi . TYPE_INTEGER ), openapi . Parameter ( 'offset' , openapi . IN_QUERY , \"Offset\" , type = openapi . TYPE_INTEGER ), openapi . Parameter ( 'timeout' , openapi . IN_QUERY , \"Timeout\" , type = openapi . TYPE_INTEGER ), ]) @api_view ([ 'GET' ]) def term_search ( request : Request , id : UUID ): dataset = Dataset . objects . get ( id = id ) q = request . GET . get ( 'query' , '' ) pos = TermPos ( request . GET . get ( 'pos' , 'OBJECT' )) limit = int ( request . GET . get ( 'limit' , 10 )) offset = int ( request . GET . get ( 'offset' , 0 )) timeout = int ( request . GET . get ( 'timeout' , 5000 )) cache_key = f 'search: { dataset . id } : { pos } : { q } : { limit } : { offset } : { timeout } ' result_dict = cache . get ( cache_key ) if result_dict is None : result_default = LocalSearchService ( DEFAULT_SEARCH_INDEX ) . search ( q , pos , limit , offset , timeout ) \\ if q and DEFAULT_SEARCH_INDEX . exists () else None result_dataset = dataset . get_search_service () . search ( q , pos , limit , offset , timeout ) result = merge_results ( result_default , result_dataset , q ) if result_default else result_dataset result_dict = result . to_dict () cache . set ( cache_key , result_dict , 60 * 60 * 24 * 7 ) return JsonResponse ( result_dict )","title":"datasets"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet","text":"Bases: viewsets . ModelViewSet API endpoint that allows users to be viewed or edited. Source code in datasets/views/datasets.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class DatasetViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = Dataset . objects . all () serializer_class = DatasetSerializer pagination_class = LimitOffsetPagination filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] filterset_fields = [ 'mode' , 'search_mode' , 'state' , 'id' , 'creator' ] search_fields = [ 'name' , 'source' , 'description' ] def perform_create ( self , serializer ): if serializer . validated_data . get ( 'mode' ) == Dataset . Mode . SPARQL . value and \\ serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value : raise ValidationError ( 'Local search index for sparql datasets is not yet supported' ) if serializer . validated_data . get ( 'search_mode' , None ) == Dataset . SearchMode . TRIPLYDB . value and \\ 'tdb_id' not in serializer . validated_data . get ( 'source' , {}): raise ValidationError ( 'TriplyDB dataset must be a TriplyDB dataset' ) if not settings . STARDOG_ENABLE and ( serializer . validated_data . get ( 'mode' ) != Dataset . Mode . SPARQL . value or serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value ): raise ValidationError ( 'Local datasets are not enabled on this server' ) super () . perform_create ( serializer ) instance : Dataset = serializer . instance instance . creator = self . request . user instance . save () files = None # If a files are uploaded, store them in a temporary folder if instance . source . get ( 'source_type' ) == 'upload' : tmp_dir = DOWNLOAD_DIR / random_string ( 10 ) tmp_dir . mkdir ( parents = True ) files = [] for file in self . request . FILES . getlist ( 'files' ): file_path = tmp_dir / file . name with file_path . open ( 'wb+' ) as destination : for chunk in file . chunks (): destination . write ( chunk ) files . append ( str ( file_path . absolute ())) instance . apply_async ( import_dataset , ( instance . id , files ), creator = self . request . user , name = f 'Import dataset { instance . name } ' ) def perform_destroy ( self , instance ): instance . apply_async ( delete_dataset , ( instance . id ,), creator = self . request . user , name = f 'Deleting dataset { instance . name } ' ) def perform_update ( self , serializer ): super () . perform_update ( serializer ) def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) return permissions","title":"DatasetViewSet"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.filter_backends","text":"","title":"filter_backends"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.filterset_fields","text":"","title":"filterset_fields"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.pagination_class","text":"","title":"pagination_class"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.queryset","text":"","title":"queryset"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.search_fields","text":"","title":"search_fields"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.serializer_class","text":"","title":"serializer_class"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.get_permissions","text":"Source code in datasets/views/datasets.py 90 91 92 93 94 95 96 def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) return permissions","title":"get_permissions()"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.perform_create","text":"Source code in datasets/views/datasets.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def perform_create ( self , serializer ): if serializer . validated_data . get ( 'mode' ) == Dataset . Mode . SPARQL . value and \\ serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value : raise ValidationError ( 'Local search index for sparql datasets is not yet supported' ) if serializer . validated_data . get ( 'search_mode' , None ) == Dataset . SearchMode . TRIPLYDB . value and \\ 'tdb_id' not in serializer . validated_data . get ( 'source' , {}): raise ValidationError ( 'TriplyDB dataset must be a TriplyDB dataset' ) if not settings . STARDOG_ENABLE and ( serializer . validated_data . get ( 'mode' ) != Dataset . Mode . SPARQL . value or serializer . validated_data . get ( 'search_mode' ) == Dataset . SearchMode . LOCAL . value ): raise ValidationError ( 'Local datasets are not enabled on this server' ) super () . perform_create ( serializer ) instance : Dataset = serializer . instance instance . creator = self . request . user instance . save () files = None # If a files are uploaded, store them in a temporary folder if instance . source . get ( 'source_type' ) == 'upload' : tmp_dir = DOWNLOAD_DIR / random_string ( 10 ) tmp_dir . mkdir ( parents = True ) files = [] for file in self . request . FILES . getlist ( 'files' ): file_path = tmp_dir / file . name with file_path . open ( 'wb+' ) as destination : for chunk in file . chunks (): destination . write ( chunk ) files . append ( str ( file_path . absolute ())) instance . apply_async ( import_dataset , ( instance . id , files ), creator = self . request . user , name = f 'Import dataset { instance . name } ' )","title":"perform_create()"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.perform_destroy","text":"Source code in datasets/views/datasets.py 79 80 81 82 83 84 85 def perform_destroy ( self , instance ): instance . apply_async ( delete_dataset , ( instance . id ,), creator = self . request . user , name = f 'Deleting dataset { instance . name } ' )","title":"perform_destroy()"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.DatasetViewSet.perform_update","text":"Source code in datasets/views/datasets.py 87 88 def perform_update ( self , serializer ): super () . perform_update ( serializer )","title":"perform_update()"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.dataset_query","text":"Source code in datasets/views/datasets.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 @swagger_auto_schema ( methods = [ 'post' ], manual_parameters = [ openapi . Parameter ( 'limit' , openapi . IN_QUERY , \"Limit\" , type = openapi . TYPE_INTEGER ), openapi . Parameter ( 'timeout' , openapi . IN_QUERY , \"Timeout\" , type = openapi . TYPE_INTEGER ), ], request_body = openapi . Schema ( type = openapi . TYPE_STRING , description = 'Query to be executed' )) @api_view ([ 'POST' ]) def dataset_query ( request : Request , id : UUID ): dataset = Dataset . objects . get ( id = id ) limit = int ( request . GET . get ( 'limit' , 10 )) timeout = int ( request . GET . get ( 'timeout' , 5000 )) query = request . body . decode ( 'utf-8' ) result = dataset . get_query_service () . query ( query , limit , timeout ) return JsonResponse ( result )","title":"dataset_query()"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.parse_int_or_none","text":"Source code in datasets/views/datasets.py 99 100 101 102 def parse_int_or_none ( value : str ) -> int : if value is None : return None return int ( value )","title":"parse_int_or_none()"},{"location":"reference/datasets/views/datasets/#datasets.views.datasets.term_search","text":"Source code in datasets/views/datasets.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 @swagger_auto_schema ( methods = [ 'get' ], manual_parameters = [ openapi . Parameter ( 'query' , openapi . IN_QUERY , \"Search query\" , type = openapi . TYPE_STRING ), openapi . Parameter ( 'pos' , openapi . IN_QUERY , \"Term Position\" , type = openapi . TYPE_STRING ), openapi . Parameter ( 'limit' , openapi . IN_QUERY , \"Limit\" , type = openapi . TYPE_INTEGER ), openapi . Parameter ( 'offset' , openapi . IN_QUERY , \"Offset\" , type = openapi . TYPE_INTEGER ), openapi . Parameter ( 'timeout' , openapi . IN_QUERY , \"Timeout\" , type = openapi . TYPE_INTEGER ), ]) @api_view ([ 'GET' ]) def term_search ( request : Request , id : UUID ): dataset = Dataset . objects . get ( id = id ) q = request . GET . get ( 'query' , '' ) pos = TermPos ( request . GET . get ( 'pos' , 'OBJECT' )) limit = int ( request . GET . get ( 'limit' , 10 )) offset = int ( request . GET . get ( 'offset' , 0 )) timeout = int ( request . GET . get ( 'timeout' , 5000 )) cache_key = f 'search: { dataset . id } : { pos } : { q } : { limit } : { offset } : { timeout } ' result_dict = cache . get ( cache_key ) if result_dict is None : result_default = LocalSearchService ( DEFAULT_SEARCH_INDEX ) . search ( q , pos , limit , offset , timeout ) \\ if q and DEFAULT_SEARCH_INDEX . exists () else None result_dataset = dataset . get_search_service () . search ( q , pos , limit , offset , timeout ) result = merge_results ( result_default , result_dataset , q ) if result_default else result_dataset result_dict = result . to_dict () cache . set ( cache_key , result_dict , 60 * 60 * 24 * 7 ) return JsonResponse ( result_dict )","title":"term_search()"},{"location":"reference/datasets/views/lodc/","text":"KG_FORMAT_TERMS = { KGFormat . RDF : [ 'rdf' ], KGFormat . TURTLE : [ 'turtle' , 'ttl' ], KGFormat . NTRIPLES : [ '.nt' , 'ntriples' , 'n-triples' ], KGFormat . NQUADS : [ 'nq' , 'nquads' , 'n-quads' ], KGFormat . JSONLD : [ 'jsonld' , 'json-ld' ], KGFormat . OWL : [ 'owl' ]} module-attribute KG_TERMS = [ 'rdf' , 'turtle' , 'ntriples' , 'n-triples' , 'owl' , 'nquads' , 'n-quads' , 'jsonld' , 'json-ld' , '.nt' , '.ttl' , '.rdf' , '.nq' , '.jsonld' , '.json' , '.owl' ] module-attribute KGFormat Bases: Enum Source code in datasets/views/lodc.py 11 12 13 14 15 16 17 class KGFormat ( Enum ): RDF = \"rdf\" TURTLE = \"turtle\" NTRIPLES = \"nt\" NQUADS = \"nq\" JSONLD = \"jsonld\" OWL = \"owl\" JSONLD = 'jsonld' class-attribute NQUADS = 'nq' class-attribute NTRIPLES = 'nt' class-attribute OWL = 'owl' class-attribute RDF = 'rdf' class-attribute TURTLE = 'turtle' class-attribute preprocess_dataset ( dataset ) Source code in datasets/views/lodc.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def preprocess_dataset ( dataset : dict ): full_downloads = dataset . get ( 'full_download' , []) other_downloads = dataset . get ( 'other_download' , []) sparql = dataset . get ( 'sparql' , []) full_downloads = [ preprocess_download ( d ) for d in full_downloads ] other_downloads = [ preprocess_download ( d ) for d in other_downloads ] sparql = [ preprocess_download ( d ) for d in sparql ] n_downloads_available , n_downloads_kg , n_downloads_maybekg , n_kg_available = 0 , 0 , 0 , 0 for d in it . chain ( iter ( full_downloads ), iter ( other_downloads ), iter ( sparql )): n_downloads_available += d [ 'available' ] if d [ 'detect_kg' ] == 'YES' : n_downloads_kg += 1 if d [ 'detect_kg' ] != 'NO' : n_downloads_maybekg += 1 if d [ 'available' ]: n_kg_available += 1 dataset [ 'n_downloads_available' ] = n_downloads_available return { ** dataset , 'full_download' : full_downloads , 'other_download' : other_downloads , 'sparql' : sparql , 'n_downloads_available' : n_downloads_available , 'n_downloads_kg' : n_downloads_kg , 'n_downloads_maybekg' : n_downloads_maybekg , 'n_kg_available' : n_kg_available , } preprocess_download ( download ) Source code in datasets/views/lodc.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def preprocess_download ( download : dict ): title = download . get ( 'title' , '' ) status = download . get ( 'status' , None ) url = download . get ( 'download_url' , None ) or download . get ( 'access_url' , None ) available = url and ( status is None or status == \"OK\" ) media_type = download . get ( 'media_type' , '' ) corpus = ' ' . join ([ media_type , title , url or '' ]) . lower () maybe_kg = ( 'html' not in media_type . lower () and 'sitemap' not in media_type . lower () ) is_kg = url and ( 'void' not in corpus or any ( term in corpus for term in KG_TERMS ) ) return { ** download , 'url' : url , 'available' : available , 'detect_kg' : 'YES' if is_kg else ( 'MAYBE' if maybe_kg else 'NO' ), } proxy_lodc_api ( request ) Source code in datasets/views/lodc.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @api_view ([ 'GET' ]) def proxy_lodc_api ( request : Request ): datasets = cache . get ( 'lodc_datasets' ) if datasets is None : response = requests . get ( 'https://lod-cloud.net/lod-data.json' , stream = True , verify = False ) datasets = response . json () for k , v in datasets . items (): try : datasets [ k ] = preprocess_dataset ( v ) except Exception : pass cache . set ( 'lodc_datasets' , datasets , timeout = 60 * 60 * 24 ) return JsonResponse ( datasets )","title":"lodc"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KG_FORMAT_TERMS","text":"","title":"KG_FORMAT_TERMS"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KG_TERMS","text":"","title":"KG_TERMS"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat","text":"Bases: Enum Source code in datasets/views/lodc.py 11 12 13 14 15 16 17 class KGFormat ( Enum ): RDF = \"rdf\" TURTLE = \"turtle\" NTRIPLES = \"nt\" NQUADS = \"nq\" JSONLD = \"jsonld\" OWL = \"owl\"","title":"KGFormat"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat.JSONLD","text":"","title":"JSONLD"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat.NQUADS","text":"","title":"NQUADS"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat.NTRIPLES","text":"","title":"NTRIPLES"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat.OWL","text":"","title":"OWL"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat.RDF","text":"","title":"RDF"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.KGFormat.TURTLE","text":"","title":"TURTLE"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.preprocess_dataset","text":"Source code in datasets/views/lodc.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def preprocess_dataset ( dataset : dict ): full_downloads = dataset . get ( 'full_download' , []) other_downloads = dataset . get ( 'other_download' , []) sparql = dataset . get ( 'sparql' , []) full_downloads = [ preprocess_download ( d ) for d in full_downloads ] other_downloads = [ preprocess_download ( d ) for d in other_downloads ] sparql = [ preprocess_download ( d ) for d in sparql ] n_downloads_available , n_downloads_kg , n_downloads_maybekg , n_kg_available = 0 , 0 , 0 , 0 for d in it . chain ( iter ( full_downloads ), iter ( other_downloads ), iter ( sparql )): n_downloads_available += d [ 'available' ] if d [ 'detect_kg' ] == 'YES' : n_downloads_kg += 1 if d [ 'detect_kg' ] != 'NO' : n_downloads_maybekg += 1 if d [ 'available' ]: n_kg_available += 1 dataset [ 'n_downloads_available' ] = n_downloads_available return { ** dataset , 'full_download' : full_downloads , 'other_download' : other_downloads , 'sparql' : sparql , 'n_downloads_available' : n_downloads_available , 'n_downloads_kg' : n_downloads_kg , 'n_downloads_maybekg' : n_downloads_maybekg , 'n_kg_available' : n_kg_available , }","title":"preprocess_dataset()"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.preprocess_download","text":"Source code in datasets/views/lodc.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def preprocess_download ( download : dict ): title = download . get ( 'title' , '' ) status = download . get ( 'status' , None ) url = download . get ( 'download_url' , None ) or download . get ( 'access_url' , None ) available = url and ( status is None or status == \"OK\" ) media_type = download . get ( 'media_type' , '' ) corpus = ' ' . join ([ media_type , title , url or '' ]) . lower () maybe_kg = ( 'html' not in media_type . lower () and 'sitemap' not in media_type . lower () ) is_kg = url and ( 'void' not in corpus or any ( term in corpus for term in KG_TERMS ) ) return { ** download , 'url' : url , 'available' : available , 'detect_kg' : 'YES' if is_kg else ( 'MAYBE' if maybe_kg else 'NO' ), }","title":"preprocess_download()"},{"location":"reference/datasets/views/lodc/#datasets.views.lodc.proxy_lodc_api","text":"Source code in datasets/views/lodc.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @api_view ([ 'GET' ]) def proxy_lodc_api ( request : Request ): datasets = cache . get ( 'lodc_datasets' ) if datasets is None : response = requests . get ( 'https://lod-cloud.net/lod-data.json' , stream = True , verify = False ) datasets = response . json () for k , v in datasets . items (): try : datasets [ k ] = preprocess_dataset ( v ) except Exception : pass cache . set ( 'lodc_datasets' , datasets , timeout = 60 * 60 * 24 ) return JsonResponse ( datasets )","title":"proxy_lodc_api()"},{"location":"reference/reports/","text":"","title":"reports"},{"location":"reference/reports/consumers/","text":"logger = get_logger () module-attribute NotebookConsumer Bases: WebsocketConsumer Source code in reports/consumers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class NotebookConsumer ( WebsocketConsumer ): report_id : UUID user : User def connect ( self ): self . report_id = self . scope [ 'url_route' ][ 'kwargs' ][ 'notebook_id' ] self . user = self . scope [ \"user\" ] if not self . report . can_view ( self . user ): self . close () return async_to_sync ( self . channel_layer . group_add )( str ( self . report_id ), self . channel_name ) self . accept () def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( self . report_id ), self . channel_name ) def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) match PacketType ( packet . type ): case PacketType . CELL_RUN : if not self . report . can_edit ( self . user ): return report = self . report cell_id = UUID ( packet . data ) cell_index = report . notebook . get ( 'content' , {}) . get ( 'cell_order' , []) . index ( str ( cell_id )) Report . update_cell_outputs ( self . report_id , cell_id , []) Report . update_cell_state ( self . report_id , cell_id , CellState . QUEUED ) self . report . apply_async ( run_cell , ( self . report_id , cell_id ), creator = self . user , name = 'Run cell # {} ' . format ( cell_index ) ) case PacketType . CELL_RESULT : self . send_packet ( PacketType . CELL_RESULT , packet . data ) case PacketType . CELL_STATE : self . send_packet ( PacketType . CELL_STATE , packet . data ) def task_message ( self , event ): self . receive ( event [ 'message' ]) def send_packet ( self , type : PacketType , data : Any ): self . send ( text_data = Packet ( type . value , data ) . dumps ()) @property def report ( self ) -> Report : report , created = Report . objects . get_or_create ( id = self . report_id , defaults = { 'notebook' : { 'id' : self . report_id , } } ) return report def check_permissions ( self ): pass report_id : UUID class-attribute user : User class-attribute check_permissions () Source code in reports/consumers.py 82 83 def check_permissions ( self ): pass connect () Source code in reports/consumers.py 20 21 22 23 24 25 26 27 28 29 30 31 def connect ( self ): self . report_id = self . scope [ 'url_route' ][ 'kwargs' ][ 'notebook_id' ] self . user = self . scope [ \"user\" ] if not self . report . can_view ( self . user ): self . close () return async_to_sync ( self . channel_layer . group_add )( str ( self . report_id ), self . channel_name ) self . accept () disconnect ( close_code ) Source code in reports/consumers.py 33 34 35 36 def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( self . report_id ), self . channel_name ) receive ( text_data ) Source code in reports/consumers.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) match PacketType ( packet . type ): case PacketType . CELL_RUN : if not self . report . can_edit ( self . user ): return report = self . report cell_id = UUID ( packet . data ) cell_index = report . notebook . get ( 'content' , {}) . get ( 'cell_order' , []) . index ( str ( cell_id )) Report . update_cell_outputs ( self . report_id , cell_id , []) Report . update_cell_state ( self . report_id , cell_id , CellState . QUEUED ) self . report . apply_async ( run_cell , ( self . report_id , cell_id ), creator = self . user , name = 'Run cell # {} ' . format ( cell_index ) ) case PacketType . CELL_RESULT : self . send_packet ( PacketType . CELL_RESULT , packet . data ) case PacketType . CELL_STATE : self . send_packet ( PacketType . CELL_STATE , packet . data ) report () property Source code in reports/consumers.py 69 70 71 72 73 74 75 76 77 78 79 80 @property def report ( self ) -> Report : report , created = Report . objects . get_or_create ( id = self . report_id , defaults = { 'notebook' : { 'id' : self . report_id , } } ) return report send_packet ( type , data ) Source code in reports/consumers.py 66 67 def send_packet ( self , type : PacketType , data : Any ): self . send ( text_data = Packet ( type . value , data ) . dumps ()) task_message ( event ) Source code in reports/consumers.py 63 64 def task_message ( self , event ): self . receive ( event [ 'message' ])","title":"consumers"},{"location":"reference/reports/consumers/#reports.consumers.logger","text":"","title":"logger"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer","text":"Bases: WebsocketConsumer Source code in reports/consumers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class NotebookConsumer ( WebsocketConsumer ): report_id : UUID user : User def connect ( self ): self . report_id = self . scope [ 'url_route' ][ 'kwargs' ][ 'notebook_id' ] self . user = self . scope [ \"user\" ] if not self . report . can_view ( self . user ): self . close () return async_to_sync ( self . channel_layer . group_add )( str ( self . report_id ), self . channel_name ) self . accept () def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( self . report_id ), self . channel_name ) def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) match PacketType ( packet . type ): case PacketType . CELL_RUN : if not self . report . can_edit ( self . user ): return report = self . report cell_id = UUID ( packet . data ) cell_index = report . notebook . get ( 'content' , {}) . get ( 'cell_order' , []) . index ( str ( cell_id )) Report . update_cell_outputs ( self . report_id , cell_id , []) Report . update_cell_state ( self . report_id , cell_id , CellState . QUEUED ) self . report . apply_async ( run_cell , ( self . report_id , cell_id ), creator = self . user , name = 'Run cell # {} ' . format ( cell_index ) ) case PacketType . CELL_RESULT : self . send_packet ( PacketType . CELL_RESULT , packet . data ) case PacketType . CELL_STATE : self . send_packet ( PacketType . CELL_STATE , packet . data ) def task_message ( self , event ): self . receive ( event [ 'message' ]) def send_packet ( self , type : PacketType , data : Any ): self . send ( text_data = Packet ( type . value , data ) . dumps ()) @property def report ( self ) -> Report : report , created = Report . objects . get_or_create ( id = self . report_id , defaults = { 'notebook' : { 'id' : self . report_id , } } ) return report def check_permissions ( self ): pass","title":"NotebookConsumer"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.report_id","text":"","title":"report_id"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.user","text":"","title":"user"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.check_permissions","text":"Source code in reports/consumers.py 82 83 def check_permissions ( self ): pass","title":"check_permissions()"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.connect","text":"Source code in reports/consumers.py 20 21 22 23 24 25 26 27 28 29 30 31 def connect ( self ): self . report_id = self . scope [ 'url_route' ][ 'kwargs' ][ 'notebook_id' ] self . user = self . scope [ \"user\" ] if not self . report . can_view ( self . user ): self . close () return async_to_sync ( self . channel_layer . group_add )( str ( self . report_id ), self . channel_name ) self . accept ()","title":"connect()"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.disconnect","text":"Source code in reports/consumers.py 33 34 35 36 def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( self . report_id ), self . channel_name )","title":"disconnect()"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.receive","text":"Source code in reports/consumers.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) match PacketType ( packet . type ): case PacketType . CELL_RUN : if not self . report . can_edit ( self . user ): return report = self . report cell_id = UUID ( packet . data ) cell_index = report . notebook . get ( 'content' , {}) . get ( 'cell_order' , []) . index ( str ( cell_id )) Report . update_cell_outputs ( self . report_id , cell_id , []) Report . update_cell_state ( self . report_id , cell_id , CellState . QUEUED ) self . report . apply_async ( run_cell , ( self . report_id , cell_id ), creator = self . user , name = 'Run cell # {} ' . format ( cell_index ) ) case PacketType . CELL_RESULT : self . send_packet ( PacketType . CELL_RESULT , packet . data ) case PacketType . CELL_STATE : self . send_packet ( PacketType . CELL_STATE , packet . data )","title":"receive()"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.report","text":"Source code in reports/consumers.py 69 70 71 72 73 74 75 76 77 78 79 80 @property def report ( self ) -> Report : report , created = Report . objects . get_or_create ( id = self . report_id , defaults = { 'notebook' : { 'id' : self . report_id , } } ) return report","title":"report()"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.send_packet","text":"Source code in reports/consumers.py 66 67 def send_packet ( self , type : PacketType , data : Any ): self . send ( text_data = Packet ( type . value , data ) . dumps ())","title":"send_packet()"},{"location":"reference/reports/consumers/#reports.consumers.NotebookConsumer.task_message","text":"Source code in reports/consumers.py 63 64 def task_message ( self , event ): self . receive ( event [ 'message' ])","title":"task_message()"},{"location":"reference/reports/models/","text":"CellState Bases: Enum Source code in reports/models.py 18 19 20 21 22 23 24 class CellState ( Enum ): # 'FINISHED' | 'ERROR' | 'RUNNING' | 'QUEUED' | 'INITIAL' FINISHED = 'FINISHED' ERROR = 'ERROR' RUNNING = 'RUNNING' QUEUED = 'QUEUED' INITIAL = 'INITIAL' ERROR = 'ERROR' class-attribute FINISHED = 'FINISHED' class-attribute INITIAL = 'INITIAL' class-attribute QUEUED = 'QUEUED' class-attribute RUNNING = 'RUNNING' class-attribute PacketType Bases: Enum Source code in reports/models.py 27 28 29 30 class PacketType ( Enum ): CELL_RUN = 'CELL_RUN' CELL_RESULT = 'CELL_RESULT' CELL_STATE = 'CELL_STATE' CELL_RESULT = 'CELL_RESULT' class-attribute CELL_RUN = 'CELL_RUN' class-attribute CELL_STATE = 'CELL_STATE' class-attribute Report Bases: TaskMixin , TimeStampMixin Source code in reports/models.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class Report ( TaskMixin , TimeStampMixin ): class ShareModes ( models . TextChoices ): PRIVATE = 'PRIVATE' , _ ( 'Private' ) PUBLIC_READONLY = 'PUBLIC_READONLY' , _ ( 'Public (read-only)' ) PUBLIC_READWRITE = 'PUBLIC_READWRITE' , _ ( 'Public (read-write)' ) id = models . UUIDField ( default = uuid . uuid4 , primary_key = True ) creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) dataset = models . ForeignKey ( Dataset , on_delete = models . CASCADE ) notebook = models . JSONField () share_mode = models . CharField ( max_length = 255 , choices = ShareModes . choices , default = ShareModes . PRIVATE ) discoverable = models . BooleanField ( default = False ) objects = models . Manager () def get_cell ( self , cell_id : uuid . UUID ): cell = deepget ( self . notebook , [ 'content' , 'cells' , str ( cell_id )]) if cell is None : raise Exception ( f 'Cell { cell_id } not found in notebook { self . id } ' ) return cell def get_cell_state ( self , cell_id : uuid . UUID ): state = deepget ( self . notebook , [ 'results' , 'states' , str ( cell_id )]) return state @staticmethod def update_cell_state ( report_id : uuid . UUID , cell_id : uuid . UUID , state : CellState ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'states' , str ( cell_id ), 'status' ], state . value ) ) report = Report . objects . get ( id = report_id ) state = deepget ( report . notebook , [ 'results' , 'states' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_STATE . value , { 'cell_id' : str ( cell_id ), 'state' : state , }) . dumps () }) @staticmethod def update_cell_outputs ( report_id : uuid . UUID , cell_id : uuid . UUID , outputs : list ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'outputs' , str ( cell_id )], outputs ) ) report = Report . objects . get ( id = report_id ) result = deepget ( report . notebook , [ 'results' , 'outputs' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_RESULT . value , { 'cell_id' : str ( cell_id ), 'outputs' : result , }) . dumps () }) def can_edit ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode == self . ShareModes . PUBLIC_READWRITE ) def can_view ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode != self . ShareModes . PRIVATE ) creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) class-attribute dataset = models . ForeignKey ( Dataset , on_delete = models . CASCADE ) class-attribute discoverable = models . BooleanField ( default = False ) class-attribute id = models . UUIDField ( default = uuid . uuid4 , primary_key = True ) class-attribute notebook = models . JSONField () class-attribute objects = models . Manager () class-attribute share_mode = models . CharField ( max_length = 255 , choices = ShareModes . choices , default = ShareModes . PRIVATE ) class-attribute ShareModes Bases: models . TextChoices Source code in reports/models.py 34 35 36 37 class ShareModes ( models . TextChoices ): PRIVATE = 'PRIVATE' , _ ( 'Private' ) PUBLIC_READONLY = 'PUBLIC_READONLY' , _ ( 'Public (read-only)' ) PUBLIC_READWRITE = 'PUBLIC_READWRITE' , _ ( 'Public (read-write)' ) PRIVATE = ( 'PRIVATE' , _ ( 'Private' )) class-attribute PUBLIC_READONLY = ( 'PUBLIC_READONLY' , _ ( 'Public (read-only)' )) class-attribute PUBLIC_READWRITE = ( 'PUBLIC_READWRITE' , _ ( 'Public (read-write)' )) class-attribute can_edit ( user ) Source code in reports/models.py 92 93 94 95 96 97 def can_edit ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode == self . ShareModes . PUBLIC_READWRITE ) can_view ( user ) Source code in reports/models.py 99 100 101 102 103 104 def can_view ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode != self . ShareModes . PRIVATE ) get_cell ( cell_id ) Source code in reports/models.py 49 50 51 52 53 54 def get_cell ( self , cell_id : uuid . UUID ): cell = deepget ( self . notebook , [ 'content' , 'cells' , str ( cell_id )]) if cell is None : raise Exception ( f 'Cell { cell_id } not found in notebook { self . id } ' ) return cell get_cell_state ( cell_id ) Source code in reports/models.py 56 57 58 def get_cell_state ( self , cell_id : uuid . UUID ): state = deepget ( self . notebook , [ 'results' , 'states' , str ( cell_id )]) return state update_cell_outputs ( report_id , cell_id , outputs ) staticmethod Source code in reports/models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @staticmethod def update_cell_outputs ( report_id : uuid . UUID , cell_id : uuid . UUID , outputs : list ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'outputs' , str ( cell_id )], outputs ) ) report = Report . objects . get ( id = report_id ) result = deepget ( report . notebook , [ 'results' , 'outputs' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_RESULT . value , { 'cell_id' : str ( cell_id ), 'outputs' : result , }) . dumps () }) update_cell_state ( report_id , cell_id , state ) staticmethod Source code in reports/models.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @staticmethod def update_cell_state ( report_id : uuid . UUID , cell_id : uuid . UUID , state : CellState ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'states' , str ( cell_id ), 'status' ], state . value ) ) report = Report . objects . get ( id = report_id ) state = deepget ( report . notebook , [ 'results' , 'states' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_STATE . value , { 'cell_id' : str ( cell_id ), 'state' : state , }) . dumps () })","title":"models"},{"location":"reference/reports/models/#reports.models.CellState","text":"Bases: Enum Source code in reports/models.py 18 19 20 21 22 23 24 class CellState ( Enum ): # 'FINISHED' | 'ERROR' | 'RUNNING' | 'QUEUED' | 'INITIAL' FINISHED = 'FINISHED' ERROR = 'ERROR' RUNNING = 'RUNNING' QUEUED = 'QUEUED' INITIAL = 'INITIAL'","title":"CellState"},{"location":"reference/reports/models/#reports.models.CellState.ERROR","text":"","title":"ERROR"},{"location":"reference/reports/models/#reports.models.CellState.FINISHED","text":"","title":"FINISHED"},{"location":"reference/reports/models/#reports.models.CellState.INITIAL","text":"","title":"INITIAL"},{"location":"reference/reports/models/#reports.models.CellState.QUEUED","text":"","title":"QUEUED"},{"location":"reference/reports/models/#reports.models.CellState.RUNNING","text":"","title":"RUNNING"},{"location":"reference/reports/models/#reports.models.PacketType","text":"Bases: Enum Source code in reports/models.py 27 28 29 30 class PacketType ( Enum ): CELL_RUN = 'CELL_RUN' CELL_RESULT = 'CELL_RESULT' CELL_STATE = 'CELL_STATE'","title":"PacketType"},{"location":"reference/reports/models/#reports.models.PacketType.CELL_RESULT","text":"","title":"CELL_RESULT"},{"location":"reference/reports/models/#reports.models.PacketType.CELL_RUN","text":"","title":"CELL_RUN"},{"location":"reference/reports/models/#reports.models.PacketType.CELL_STATE","text":"","title":"CELL_STATE"},{"location":"reference/reports/models/#reports.models.Report","text":"Bases: TaskMixin , TimeStampMixin Source code in reports/models.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class Report ( TaskMixin , TimeStampMixin ): class ShareModes ( models . TextChoices ): PRIVATE = 'PRIVATE' , _ ( 'Private' ) PUBLIC_READONLY = 'PUBLIC_READONLY' , _ ( 'Public (read-only)' ) PUBLIC_READWRITE = 'PUBLIC_READWRITE' , _ ( 'Public (read-write)' ) id = models . UUIDField ( default = uuid . uuid4 , primary_key = True ) creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) dataset = models . ForeignKey ( Dataset , on_delete = models . CASCADE ) notebook = models . JSONField () share_mode = models . CharField ( max_length = 255 , choices = ShareModes . choices , default = ShareModes . PRIVATE ) discoverable = models . BooleanField ( default = False ) objects = models . Manager () def get_cell ( self , cell_id : uuid . UUID ): cell = deepget ( self . notebook , [ 'content' , 'cells' , str ( cell_id )]) if cell is None : raise Exception ( f 'Cell { cell_id } not found in notebook { self . id } ' ) return cell def get_cell_state ( self , cell_id : uuid . UUID ): state = deepget ( self . notebook , [ 'results' , 'states' , str ( cell_id )]) return state @staticmethod def update_cell_state ( report_id : uuid . UUID , cell_id : uuid . UUID , state : CellState ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'states' , str ( cell_id ), 'status' ], state . value ) ) report = Report . objects . get ( id = report_id ) state = deepget ( report . notebook , [ 'results' , 'states' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_STATE . value , { 'cell_id' : str ( cell_id ), 'state' : state , }) . dumps () }) @staticmethod def update_cell_outputs ( report_id : uuid . UUID , cell_id : uuid . UUID , outputs : list ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'outputs' , str ( cell_id )], outputs ) ) report = Report . objects . get ( id = report_id ) result = deepget ( report . notebook , [ 'results' , 'outputs' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_RESULT . value , { 'cell_id' : str ( cell_id ), 'outputs' : result , }) . dumps () }) def can_edit ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode == self . ShareModes . PUBLIC_READWRITE ) def can_view ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode != self . ShareModes . PRIVATE )","title":"Report"},{"location":"reference/reports/models/#reports.models.Report.creator","text":"","title":"creator"},{"location":"reference/reports/models/#reports.models.Report.dataset","text":"","title":"dataset"},{"location":"reference/reports/models/#reports.models.Report.discoverable","text":"","title":"discoverable"},{"location":"reference/reports/models/#reports.models.Report.id","text":"","title":"id"},{"location":"reference/reports/models/#reports.models.Report.notebook","text":"","title":"notebook"},{"location":"reference/reports/models/#reports.models.Report.objects","text":"","title":"objects"},{"location":"reference/reports/models/#reports.models.Report.share_mode","text":"","title":"share_mode"},{"location":"reference/reports/models/#reports.models.Report.ShareModes","text":"Bases: models . TextChoices Source code in reports/models.py 34 35 36 37 class ShareModes ( models . TextChoices ): PRIVATE = 'PRIVATE' , _ ( 'Private' ) PUBLIC_READONLY = 'PUBLIC_READONLY' , _ ( 'Public (read-only)' ) PUBLIC_READWRITE = 'PUBLIC_READWRITE' , _ ( 'Public (read-write)' )","title":"ShareModes"},{"location":"reference/reports/models/#reports.models.Report.ShareModes.PRIVATE","text":"","title":"PRIVATE"},{"location":"reference/reports/models/#reports.models.Report.ShareModes.PUBLIC_READONLY","text":"","title":"PUBLIC_READONLY"},{"location":"reference/reports/models/#reports.models.Report.ShareModes.PUBLIC_READWRITE","text":"","title":"PUBLIC_READWRITE"},{"location":"reference/reports/models/#reports.models.Report.can_edit","text":"Source code in reports/models.py 92 93 94 95 96 97 def can_edit ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode == self . ShareModes . PUBLIC_READWRITE )","title":"can_edit()"},{"location":"reference/reports/models/#reports.models.Report.can_view","text":"Source code in reports/models.py 99 100 101 102 103 104 def can_view ( self , user : User ): return user and ( user . is_superuser or self . creator == user or self . share_mode != self . ShareModes . PRIVATE )","title":"can_view()"},{"location":"reference/reports/models/#reports.models.Report.get_cell","text":"Source code in reports/models.py 49 50 51 52 53 54 def get_cell ( self , cell_id : uuid . UUID ): cell = deepget ( self . notebook , [ 'content' , 'cells' , str ( cell_id )]) if cell is None : raise Exception ( f 'Cell { cell_id } not found in notebook { self . id } ' ) return cell","title":"get_cell()"},{"location":"reference/reports/models/#reports.models.Report.get_cell_state","text":"Source code in reports/models.py 56 57 58 def get_cell_state ( self , cell_id : uuid . UUID ): state = deepget ( self . notebook , [ 'results' , 'states' , str ( cell_id )]) return state","title":"get_cell_state()"},{"location":"reference/reports/models/#reports.models.Report.update_cell_outputs","text":"Source code in reports/models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 @staticmethod def update_cell_outputs ( report_id : uuid . UUID , cell_id : uuid . UUID , outputs : list ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'outputs' , str ( cell_id )], outputs ) ) report = Report . objects . get ( id = report_id ) result = deepget ( report . notebook , [ 'results' , 'outputs' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_RESULT . value , { 'cell_id' : str ( cell_id ), 'outputs' : result , }) . dumps () })","title":"update_cell_outputs()"},{"location":"reference/reports/models/#reports.models.Report.update_cell_state","text":"Source code in reports/models.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @staticmethod def update_cell_state ( report_id : uuid . UUID , cell_id : uuid . UUID , state : CellState ): Report . objects . filter ( id = report_id ) . update ( notebook = q_json_update ( 'notebook' , [ 'results' , 'states' , str ( cell_id ), 'status' ], state . value ) ) report = Report . objects . get ( id = report_id ) state = deepget ( report . notebook , [ 'results' , 'states' , str ( cell_id )]) send_to_group_sync ( str ( report_id ), { 'type' : 'task_message' , 'message' : Packet ( PacketType . CELL_STATE . value , { 'cell_id' : str ( cell_id ), 'state' : state , }) . dumps () })","title":"update_cell_state()"},{"location":"reference/reports/permissions/","text":"CanEditReport Bases: permissions . BasePermission Custom permission to only allow owners of an object to edit it. Source code in reports/permissions.py 18 19 20 21 22 23 24 25 26 27 class CanEditReport ( permissions . BasePermission ): \"\"\" Custom permission to only allow owners of an object to edit it. \"\"\" def has_permission ( self , request , view ): return request . user and request . user . is_authenticated def has_object_permission ( self , request , view , obj : Report ): return obj . can_edit ( request . user ) has_object_permission ( request , view , obj ) Source code in reports/permissions.py 26 27 def has_object_permission ( self , request , view , obj : Report ): return obj . can_edit ( request . user ) has_permission ( request , view ) Source code in reports/permissions.py 23 24 def has_permission ( self , request , view ): return request . user and request . user . is_authenticated CanViewReport Bases: permissions . BasePermission Custom permission to only allow owners of an object to edit it. Source code in reports/permissions.py 6 7 8 9 10 11 12 13 14 15 class CanViewReport ( permissions . BasePermission ): \"\"\" Custom permission to only allow owners of an object to edit it. \"\"\" def has_permission ( self , request , view ): return request . user and request . user . is_authenticated def has_object_permission ( self , request , view , obj : Report ): return obj . can_view ( request . user ) has_object_permission ( request , view , obj ) Source code in reports/permissions.py 14 15 def has_object_permission ( self , request , view , obj : Report ): return obj . can_view ( request . user ) has_permission ( request , view ) Source code in reports/permissions.py 11 12 def has_permission ( self , request , view ): return request . user and request . user . is_authenticated","title":"permissions"},{"location":"reference/reports/permissions/#reports.permissions.CanEditReport","text":"Bases: permissions . BasePermission Custom permission to only allow owners of an object to edit it. Source code in reports/permissions.py 18 19 20 21 22 23 24 25 26 27 class CanEditReport ( permissions . BasePermission ): \"\"\" Custom permission to only allow owners of an object to edit it. \"\"\" def has_permission ( self , request , view ): return request . user and request . user . is_authenticated def has_object_permission ( self , request , view , obj : Report ): return obj . can_edit ( request . user )","title":"CanEditReport"},{"location":"reference/reports/permissions/#reports.permissions.CanEditReport.has_object_permission","text":"Source code in reports/permissions.py 26 27 def has_object_permission ( self , request , view , obj : Report ): return obj . can_edit ( request . user )","title":"has_object_permission()"},{"location":"reference/reports/permissions/#reports.permissions.CanEditReport.has_permission","text":"Source code in reports/permissions.py 23 24 def has_permission ( self , request , view ): return request . user and request . user . is_authenticated","title":"has_permission()"},{"location":"reference/reports/permissions/#reports.permissions.CanViewReport","text":"Bases: permissions . BasePermission Custom permission to only allow owners of an object to edit it. Source code in reports/permissions.py 6 7 8 9 10 11 12 13 14 15 class CanViewReport ( permissions . BasePermission ): \"\"\" Custom permission to only allow owners of an object to edit it. \"\"\" def has_permission ( self , request , view ): return request . user and request . user . is_authenticated def has_object_permission ( self , request , view , obj : Report ): return obj . can_view ( request . user )","title":"CanViewReport"},{"location":"reference/reports/permissions/#reports.permissions.CanViewReport.has_object_permission","text":"Source code in reports/permissions.py 14 15 def has_object_permission ( self , request , view , obj : Report ): return obj . can_view ( request . user )","title":"has_object_permission()"},{"location":"reference/reports/permissions/#reports.permissions.CanViewReport.has_permission","text":"Source code in reports/permissions.py 11 12 def has_permission ( self , request , view ): return request . user and request . user . is_authenticated","title":"has_permission()"},{"location":"reference/reports/serializers/","text":"ReportSerializer Bases: serializers . ModelSerializer Source code in reports/serializers.py 8 9 10 11 12 13 14 class ReportSerializer ( serializers . ModelSerializer ): dataset = DatasetSerializer ( read_only = True ) creator = ShortUserSerializer ( read_only = True ) class Meta : model = Report exclude = [] creator = ShortUserSerializer ( read_only = True ) class-attribute dataset = DatasetSerializer ( read_only = True ) class-attribute Meta Source code in reports/serializers.py 12 13 14 class Meta : model = Report exclude = [] exclude = [] class-attribute model = Report class-attribute","title":"serializers"},{"location":"reference/reports/serializers/#reports.serializers.ReportSerializer","text":"Bases: serializers . ModelSerializer Source code in reports/serializers.py 8 9 10 11 12 13 14 class ReportSerializer ( serializers . ModelSerializer ): dataset = DatasetSerializer ( read_only = True ) creator = ShortUserSerializer ( read_only = True ) class Meta : model = Report exclude = []","title":"ReportSerializer"},{"location":"reference/reports/serializers/#reports.serializers.ReportSerializer.creator","text":"","title":"creator"},{"location":"reference/reports/serializers/#reports.serializers.ReportSerializer.dataset","text":"","title":"dataset"},{"location":"reference/reports/serializers/#reports.serializers.ReportSerializer.Meta","text":"Source code in reports/serializers.py 12 13 14 class Meta : model = Report exclude = []","title":"Meta"},{"location":"reference/reports/serializers/#reports.serializers.ReportSerializer.Meta.exclude","text":"","title":"exclude"},{"location":"reference/reports/serializers/#reports.serializers.ReportSerializer.Meta.model","text":"","title":"model"},{"location":"reference/reports/tasks/","text":"DEFAULT_LIMIT = 100 module-attribute DEFAULT_TIMEOUT = 5000 module-attribute logger = get_logger () module-attribute result_error ( e , duration ) Source code in reports/tasks.py 97 98 99 100 101 102 103 104 def result_error ( e : QueryExecutionException , duration : float ): return { 'output_type' : 'error' , 'ename' : type ( e ) . __name__ , 'evalue' : str ( e ), 'traceback' : [], 'execution_time' : duration } run_cell ( report_id , cell_id ) Source code in reports/tasks.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @shared_task () def run_cell ( report_id : UUID , cell_id : UUID ) -> str : report : Report = Report . objects . get ( id = report_id ) state = report . get_cell_state ( cell_id ) if state and state [ 'status' ] == CellState . RUNNING . value : raise Exception ( f 'Cell { cell_id } is already running' ) logger . info ( f 'Running cell { cell_id } in notebook { report_id } ' ) Report . update_cell_state ( report_id , cell_id , CellState . RUNNING ) report : Report = Report . objects . get ( id = report_id ) dataset : Dataset = report . dataset error = False try : cell = report . get_cell ( cell_id ) if cell is None : raise Exception ( f 'Cell { cell_id } not found in notebook { report_id } ' ) if dataset . state != DatasetState . IMPORTED . value : raise Exception ( f 'Dataset { dataset . id } is not imported yet' ) timeout = deepget ( cell , [ 'metadata' , 'timeout' ], None ) limit = deepget ( cell , [ 'metadata' , 'limit' ], None ) outputs : list = [] cell_type = cell . get ( 'cell_type' , '' ) match cell_type : case 'code' : outputs , error = run_sparql ( dataset , cell . get ( 'source' , '' ), timeout , limit ) case _ if cell_type . startswith ( 'widget_' ): snapshot = cell . get ( 'data' , {}) for source in cell . get ( 'source' , []): outputs_s , error = run_sparql ( dataset , source , timeout , limit ) for output in outputs_s : if output . get ( 'output_type' ) == 'execute_result' : output [ 'snapshot' ] = snapshot outputs . extend ( outputs_s ) if error : break # outputs. case _ : raise Exception ( f 'Cell { cell_id } in notebook { report_id } has unknown cell type { cell_type } ' ) Report . update_cell_outputs ( report_id , cell_id , outputs ) except Exception as e : logger . error ( f 'Error running cell { cell_id } in notebook { report_id } ' ) Report . update_cell_state ( report_id , cell_id , CellState . ERROR ) raise e Report . update_cell_state ( report_id , cell_id , CellState . ERROR if error else CellState . FINISHED ) run_sparql ( dataset , source , timeout = None , limit = None ) Source code in reports/tasks.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def run_sparql ( dataset : Dataset , source : str , timeout : int = None , limit : int = None ): start_time = timer () limit = ( limit or DEFAULT_LIMIT ) if 'LIMIT ' not in source . upper () else None timeout = timeout or DEFAULT_TIMEOUT outputs , error = [], False try : output = dataset . get_query_service () . query ( source , limit , timeout ) outputs . append ({ 'output_type' : 'execute_result' , 'execute_count' : 1 , 'data' : output , 'execution_time' : float ( timer () - start_time ) }) except QueryExecutionException as e : error = True outputs . append ( result_error ( e , float ( timer () - start_time ))) return outputs , error","title":"tasks"},{"location":"reference/reports/tasks/#reports.tasks.DEFAULT_LIMIT","text":"","title":"DEFAULT_LIMIT"},{"location":"reference/reports/tasks/#reports.tasks.DEFAULT_TIMEOUT","text":"","title":"DEFAULT_TIMEOUT"},{"location":"reference/reports/tasks/#reports.tasks.logger","text":"","title":"logger"},{"location":"reference/reports/tasks/#reports.tasks.result_error","text":"Source code in reports/tasks.py 97 98 99 100 101 102 103 104 def result_error ( e : QueryExecutionException , duration : float ): return { 'output_type' : 'error' , 'ename' : type ( e ) . __name__ , 'evalue' : str ( e ), 'traceback' : [], 'execution_time' : duration }","title":"result_error()"},{"location":"reference/reports/tasks/#reports.tasks.run_cell","text":"Source code in reports/tasks.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @shared_task () def run_cell ( report_id : UUID , cell_id : UUID ) -> str : report : Report = Report . objects . get ( id = report_id ) state = report . get_cell_state ( cell_id ) if state and state [ 'status' ] == CellState . RUNNING . value : raise Exception ( f 'Cell { cell_id } is already running' ) logger . info ( f 'Running cell { cell_id } in notebook { report_id } ' ) Report . update_cell_state ( report_id , cell_id , CellState . RUNNING ) report : Report = Report . objects . get ( id = report_id ) dataset : Dataset = report . dataset error = False try : cell = report . get_cell ( cell_id ) if cell is None : raise Exception ( f 'Cell { cell_id } not found in notebook { report_id } ' ) if dataset . state != DatasetState . IMPORTED . value : raise Exception ( f 'Dataset { dataset . id } is not imported yet' ) timeout = deepget ( cell , [ 'metadata' , 'timeout' ], None ) limit = deepget ( cell , [ 'metadata' , 'limit' ], None ) outputs : list = [] cell_type = cell . get ( 'cell_type' , '' ) match cell_type : case 'code' : outputs , error = run_sparql ( dataset , cell . get ( 'source' , '' ), timeout , limit ) case _ if cell_type . startswith ( 'widget_' ): snapshot = cell . get ( 'data' , {}) for source in cell . get ( 'source' , []): outputs_s , error = run_sparql ( dataset , source , timeout , limit ) for output in outputs_s : if output . get ( 'output_type' ) == 'execute_result' : output [ 'snapshot' ] = snapshot outputs . extend ( outputs_s ) if error : break # outputs. case _ : raise Exception ( f 'Cell { cell_id } in notebook { report_id } has unknown cell type { cell_type } ' ) Report . update_cell_outputs ( report_id , cell_id , outputs ) except Exception as e : logger . error ( f 'Error running cell { cell_id } in notebook { report_id } ' ) Report . update_cell_state ( report_id , cell_id , CellState . ERROR ) raise e Report . update_cell_state ( report_id , cell_id , CellState . ERROR if error else CellState . FINISHED )","title":"run_cell()"},{"location":"reference/reports/tasks/#reports.tasks.run_sparql","text":"Source code in reports/tasks.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def run_sparql ( dataset : Dataset , source : str , timeout : int = None , limit : int = None ): start_time = timer () limit = ( limit or DEFAULT_LIMIT ) if 'LIMIT ' not in source . upper () else None timeout = timeout or DEFAULT_TIMEOUT outputs , error = [], False try : output = dataset . get_query_service () . query ( source , limit , timeout ) outputs . append ({ 'output_type' : 'execute_result' , 'execute_count' : 1 , 'data' : output , 'execution_time' : float ( timer () - start_time ) }) except QueryExecutionException as e : error = True outputs . append ( result_error ( e , float ( timer () - start_time ))) return outputs , error","title":"run_sparql()"},{"location":"reference/reports/views/","text":"ReportViewSet Bases: viewsets . ModelViewSet API endpoint that allows users to be viewed or edited. Source code in reports/views.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class ReportViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = Report . objects . all () serializer_class = ReportSerializer pagination_class = LimitOffsetPagination filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] search_fields = [ 'notebook' ] def perform_create ( self , serializer ): serializer . save ( dataset_id = self . request . data [ 'dataset' ]) instance : Report = serializer . instance instance . creator = self . request . user instance . save () def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) | ( Q ( discoverable = True ) & ~ Q ( share_mode = Report . ShareModes . PRIVATE )) ) def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'update' , 'partial_update' ]: permissions . append ( CanEditReport ()) if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) if self . action in [ 'retrieve' ]: permissions . append ( CanViewReport ()) return permissions filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] class-attribute pagination_class = LimitOffsetPagination class-attribute queryset = Report . objects . all () class-attribute search_fields = [ 'notebook' ] class-attribute serializer_class = ReportSerializer class-attribute get_permissions () Source code in reports/views.py 46 47 48 49 50 51 52 53 54 55 56 57 58 def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'update' , 'partial_update' ]: permissions . append ( CanEditReport ()) if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) if self . action in [ 'retrieve' ]: permissions . append ( CanViewReport ()) return permissions get_queryset () Source code in reports/views.py 38 39 40 41 42 43 44 def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) | ( Q ( discoverable = True ) & ~ Q ( share_mode = Report . ShareModes . PRIVATE )) ) perform_create ( serializer ) Source code in reports/views.py 31 32 33 34 35 36 def perform_create ( self , serializer ): serializer . save ( dataset_id = self . request . data [ 'dataset' ]) instance : Report = serializer . instance instance . creator = self . request . user instance . save () gpt_prompt ( request ) Source code in reports/views.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @swagger_auto_schema ( methods = [ 'post' ], request_body = openapi . Schema ( type = openapi . TYPE_OBJECT , properties = { 'prompt' : openapi . Schema ( type = openapi . TYPE_STRING , description = \"GPT prompt\" ), } )) @api_view ([ 'POST' ]) def gpt_prompt ( request : Request ): if not request . data . get ( 'prompt' , None ): raise Exception ( 'No prompt provided' ) if not OPENAPI_KEY : raise Exception ( 'OpenAI API key not set' ) openai . api_key = OPENAPI_KEY response = openai . Completion . create ( model = \"code-davinci-002\" , prompt = request . data [ 'prompt' ], temperature = 0 , max_tokens = 150 , top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 , stop = [ \"#\" , \";\" ] ) return JsonResponse ( response . to_dict ())","title":"views"},{"location":"reference/reports/views/#reports.views.ReportViewSet","text":"Bases: viewsets . ModelViewSet API endpoint that allows users to be viewed or edited. Source code in reports/views.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class ReportViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = Report . objects . all () serializer_class = ReportSerializer pagination_class = LimitOffsetPagination filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] search_fields = [ 'notebook' ] def perform_create ( self , serializer ): serializer . save ( dataset_id = self . request . data [ 'dataset' ]) instance : Report = serializer . instance instance . creator = self . request . user instance . save () def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) | ( Q ( discoverable = True ) & ~ Q ( share_mode = Report . ShareModes . PRIVATE )) ) def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'update' , 'partial_update' ]: permissions . append ( CanEditReport ()) if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) if self . action in [ 'retrieve' ]: permissions . append ( CanViewReport ()) return permissions","title":"ReportViewSet"},{"location":"reference/reports/views/#reports.views.ReportViewSet.filter_backends","text":"","title":"filter_backends"},{"location":"reference/reports/views/#reports.views.ReportViewSet.pagination_class","text":"","title":"pagination_class"},{"location":"reference/reports/views/#reports.views.ReportViewSet.queryset","text":"","title":"queryset"},{"location":"reference/reports/views/#reports.views.ReportViewSet.search_fields","text":"","title":"search_fields"},{"location":"reference/reports/views/#reports.views.ReportViewSet.serializer_class","text":"","title":"serializer_class"},{"location":"reference/reports/views/#reports.views.ReportViewSet.get_permissions","text":"Source code in reports/views.py 46 47 48 49 50 51 52 53 54 55 56 57 58 def get_permissions ( self ): permissions = super () . get_permissions () if self . action in [ 'update' , 'partial_update' ]: permissions . append ( CanEditReport ()) if self . action in [ 'destroy' ]: permissions . append ( IsOwner ()) if self . action in [ 'retrieve' ]: permissions . append ( CanViewReport ()) return permissions","title":"get_permissions()"},{"location":"reference/reports/views/#reports.views.ReportViewSet.get_queryset","text":"Source code in reports/views.py 38 39 40 41 42 43 44 def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) | ( Q ( discoverable = True ) & ~ Q ( share_mode = Report . ShareModes . PRIVATE )) )","title":"get_queryset()"},{"location":"reference/reports/views/#reports.views.ReportViewSet.perform_create","text":"Source code in reports/views.py 31 32 33 34 35 36 def perform_create ( self , serializer ): serializer . save ( dataset_id = self . request . data [ 'dataset' ]) instance : Report = serializer . instance instance . creator = self . request . user instance . save ()","title":"perform_create()"},{"location":"reference/reports/views/#reports.views.gpt_prompt","text":"Source code in reports/views.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @swagger_auto_schema ( methods = [ 'post' ], request_body = openapi . Schema ( type = openapi . TYPE_OBJECT , properties = { 'prompt' : openapi . Schema ( type = openapi . TYPE_STRING , description = \"GPT prompt\" ), } )) @api_view ([ 'POST' ]) def gpt_prompt ( request : Request ): if not request . data . get ( 'prompt' , None ): raise Exception ( 'No prompt provided' ) if not OPENAPI_KEY : raise Exception ( 'OpenAI API key not set' ) openai . api_key = OPENAPI_KEY response = openai . Completion . create ( model = \"code-davinci-002\" , prompt = request . data [ 'prompt' ], temperature = 0 , max_tokens = 150 , top_p = 1 , frequency_penalty = 0 , presence_penalty = 0 , stop = [ \"#\" , \";\" ] ) return JsonResponse ( response . to_dict ())","title":"gpt_prompt()"},{"location":"reference/shared/","text":"","title":"shared"},{"location":"reference/shared/dict/","text":"deepget ( data , key , default = None ) It takes a dictionary, a list of keys, and a default value, and returns the value of the key in the dictionary, or the default value if the key is not found :param data: The data to search through :param key: The key to search for :param default: The default value to return if the key is not found :return: The value of the key in the data. Source code in shared/dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def deepget ( data , key , default = None ): \"\"\" It takes a dictionary, a list of keys, and a default value, and returns the value of the key in the dictionary, or the default value if the key is not found :param data: The data to search through :param key: The key to search for :param default: The default value to return if the key is not found :return: The value of the key in the data. \"\"\" try : return reduce ( getitem , key , data ) except ( KeyError , IndexError ): return default","title":"dict"},{"location":"reference/shared/dict/#shared.dict.deepget","text":"It takes a dictionary, a list of keys, and a default value, and returns the value of the key in the dictionary, or the default value if the key is not found :param data: The data to search through :param key: The key to search for :param default: The default value to return if the key is not found :return: The value of the key in the data. Source code in shared/dict.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def deepget ( data , key , default = None ): \"\"\" It takes a dictionary, a list of keys, and a default value, and returns the value of the key in the dictionary, or the default value if the key is not found :param data: The data to search through :param key: The key to search for :param default: The default value to return if the key is not found :return: The value of the key in the data. \"\"\" try : return reduce ( getitem , key , data ) except ( KeyError , IndexError ): return default","title":"deepget()"},{"location":"reference/shared/logging/","text":"get_logger ( name = None ) It returns a global logger object :param name: The name of the logger. If you don't specify a name, the root logger will be returned :return: A logger object. Source code in shared/logging.py 4 5 6 7 8 9 10 11 def get_logger ( name = None ): \"\"\" It returns a global logger object :param name: The name of the logger. If you don't specify a name, the root logger will be returned :return: A logger object. \"\"\" return logging . getLogger ( name or 'root' )","title":"logging"},{"location":"reference/shared/logging/#shared.logging.get_logger","text":"It returns a global logger object :param name: The name of the logger. If you don't specify a name, the root logger will be returned :return: A logger object. Source code in shared/logging.py 4 5 6 7 8 9 10 11 def get_logger ( name = None ): \"\"\" It returns a global logger object :param name: The name of the logger. If you don't specify a name, the root logger will be returned :return: A logger object. \"\"\" return logging . getLogger ( name or 'root' )","title":"get_logger()"},{"location":"reference/shared/models/","text":"TimeStampMixin Bases: models . Model This class is a mixin that adds created_at and updated_at fields to any model that inherits from it. Source code in shared/models.py 4 5 6 7 8 9 10 11 12 13 14 class TimeStampMixin ( models . Model ): \"\"\" This class is a mixin that adds created_at and updated_at fields to any model that inherits from it. \"\"\" created_at = models . DateTimeField ( auto_now_add = True , editable = False ) \"\"\"The date and time when the object was created.\"\"\" updated_at = models . DateTimeField ( auto_now = True ) \"\"\"The date and time when the object was last updated.\"\"\" class Meta : abstract = True created_at = models . DateTimeField ( auto_now_add = True , editable = False ) class-attribute The date and time when the object was created. updated_at = models . DateTimeField ( auto_now = True ) class-attribute The date and time when the object was last updated. Meta Source code in shared/models.py 13 14 class Meta : abstract = True abstract = True class-attribute","title":"models"},{"location":"reference/shared/models/#shared.models.TimeStampMixin","text":"Bases: models . Model This class is a mixin that adds created_at and updated_at fields to any model that inherits from it. Source code in shared/models.py 4 5 6 7 8 9 10 11 12 13 14 class TimeStampMixin ( models . Model ): \"\"\" This class is a mixin that adds created_at and updated_at fields to any model that inherits from it. \"\"\" created_at = models . DateTimeField ( auto_now_add = True , editable = False ) \"\"\"The date and time when the object was created.\"\"\" updated_at = models . DateTimeField ( auto_now = True ) \"\"\"The date and time when the object was last updated.\"\"\" class Meta : abstract = True","title":"TimeStampMixin"},{"location":"reference/shared/models/#shared.models.TimeStampMixin.created_at","text":"The date and time when the object was created.","title":"created_at"},{"location":"reference/shared/models/#shared.models.TimeStampMixin.updated_at","text":"The date and time when the object was last updated.","title":"updated_at"},{"location":"reference/shared/models/#shared.models.TimeStampMixin.Meta","text":"Source code in shared/models.py 13 14 class Meta : abstract = True","title":"Meta"},{"location":"reference/shared/models/#shared.models.TimeStampMixin.Meta.abstract","text":"","title":"abstract"},{"location":"reference/shared/paths/","text":"BIN_DIR = settings . BASE_DIR / 'backend' / 'bin' module-attribute The path to the backend/bin directory. DATA_DIR = settings . STORAGE_DIR / 'data' module-attribute The path to the data directory. DEFAULT_SEARCH_INDEX = DATA_DIR / f 'search_index_default' module-attribute The path to the data directory. DOWNLOAD_DIR = settings . STORAGE_DIR / 'downloads' module-attribute The path to the downloads directory. EXPORT_DIR = settings . STORAGE_DIR / 'export' module-attribute The path to the export directory. IMPORT_DIR = settings . STORAGE_DIR / 'import' module-attribute The path to the import directory.","title":"paths"},{"location":"reference/shared/paths/#shared.paths.BIN_DIR","text":"The path to the backend/bin directory.","title":"BIN_DIR"},{"location":"reference/shared/paths/#shared.paths.DATA_DIR","text":"The path to the data directory.","title":"DATA_DIR"},{"location":"reference/shared/paths/#shared.paths.DEFAULT_SEARCH_INDEX","text":"The path to the data directory.","title":"DEFAULT_SEARCH_INDEX"},{"location":"reference/shared/paths/#shared.paths.DOWNLOAD_DIR","text":"The path to the downloads directory.","title":"DOWNLOAD_DIR"},{"location":"reference/shared/paths/#shared.paths.EXPORT_DIR","text":"The path to the export directory.","title":"EXPORT_DIR"},{"location":"reference/shared/paths/#shared.paths.IMPORT_DIR","text":"The path to the import directory.","title":"IMPORT_DIR"},{"location":"reference/shared/query/","text":"q_json_update ( field , key , value ) It takes a JSON field, a list of keys, and a value, and returns a RawSQL object that will update the JSON field with the value at the given keys :param field: The field to update :type field: str :param key: The key to update :type key: List[str] :param value: The value to be inserted into the JSON field :type value: Any :return: A RawSQL object. Source code in shared/query.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def q_json_update ( field : str , key : List [ str ], value : Any ) -> RawSQL : \"\"\" It takes a JSON field, a list of keys, and a value, and returns a RawSQL object that will update the JSON field with the value at the given keys :param field: The field to update :type field: str :param key: The key to update :type key: List[str] :param value: The value to be inserted into the JSON field :type value: Any :return: A RawSQL object. \"\"\" key = ',' . join ( key ) return RawSQL ( f ''' jsonb_set( { field } ::jsonb, %s::text[], %s::jsonb, true) ''' , [ f ' {{ { key } }} ' , json . dumps ( value ), ])","title":"query"},{"location":"reference/shared/query/#shared.query.q_json_update","text":"It takes a JSON field, a list of keys, and a value, and returns a RawSQL object that will update the JSON field with the value at the given keys :param field: The field to update :type field: str :param key: The key to update :type key: List[str] :param value: The value to be inserted into the JSON field :type value: Any :return: A RawSQL object. Source code in shared/query.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def q_json_update ( field : str , key : List [ str ], value : Any ) -> RawSQL : \"\"\" It takes a JSON field, a list of keys, and a value, and returns a RawSQL object that will update the JSON field with the value at the given keys :param field: The field to update :type field: str :param key: The key to update :type key: List[str] :param value: The value to be inserted into the JSON field :type value: Any :return: A RawSQL object. \"\"\" key = ',' . join ( key ) return RawSQL ( f ''' jsonb_set( { field } ::jsonb, %s::text[], %s::jsonb, true) ''' , [ f ' {{ { key } }} ' , json . dumps ( value ), ])","title":"q_json_update()"},{"location":"reference/shared/random/","text":"random_string ( length = 10 ) Generates a random string of letters and digits of the specified length. Source code in shared/random.py 5 6 7 8 9 def random_string ( length = 10 ) -> str : \"\"\" Generates a random string of letters and digits of the specified length. \"\"\" return '' . join ( random . choice ( string . ascii_letters + string . digits ) for _ in range ( length ))","title":"random"},{"location":"reference/shared/random/#shared.random.random_string","text":"Generates a random string of letters and digits of the specified length. Source code in shared/random.py 5 6 7 8 9 def random_string ( length = 10 ) -> str : \"\"\" Generates a random string of letters and digits of the specified length. \"\"\" return '' . join ( random . choice ( string . ascii_letters + string . digits ) for _ in range ( length ))","title":"random_string()"},{"location":"reference/shared/shell/","text":"logger = get_logger () module-attribute CommandFailed dataclass Bases: Exception Source code in shared/shell.py 32 33 34 @dataclass class CommandFailed ( Exception ): code : int code : int class-attribute consume_print ( it ) It takes an iterator of strings and prints each line to the log :param it: Iterator[str] :type it: Iterator[str] Source code in shared/shell.py 77 78 79 80 81 82 83 84 85 def consume_print ( it : Iterator [ str ]): \"\"\" It takes an iterator of strings and prints each line to the log :param it: Iterator[str] :type it: Iterator[str] \"\"\" for line in it : logger . info ( line ) execute_command ( cmd , timeout = None , ignore_errors = False , ** kwargs ) It runs a command and yields the output line by line :param cmd: The command to execute :type cmd: Union[List[str], str] :param timeout: The maximum time to wait for the command to finish :type timeout: int :param ignore_errors: If True, don't raise an exception if the command fails, defaults to False :type ignore_errors: bool (optional) Source code in shared/shell.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def execute_command ( cmd : Union [ List [ str ], str ], timeout : int = None , ignore_errors : bool = False , ** kwargs , ) -> Iterator [ str ]: \"\"\" It runs a command and yields the output line by line :param cmd: The command to execute :type cmd: Union[List[str], str] :param timeout: The maximum time to wait for the command to finish :type timeout: int :param ignore_errors: If True, don't raise an exception if the command fails, defaults to False :type ignore_errors: bool (optional) \"\"\" p = subprocess . Popen ( cmd , shell = False , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , ** kwargs ) def run (): for line in iter ( p . stdout . readline , b '' ): line = line . rstrip () . decode ( 'utf-8' ) yield line p . stdout . close () code = p . wait () if code != 0 and not ignore_errors : raise CommandFailed ( code ) if timeout is not None : with with_timeout ( p , timeout ): yield from run () else : yield from run () with_timeout ( p , timeout ) It runs a function in a separate thread, and if the function doesn't return before the timeout, it kills the process :param p: subprocess.Popen :type p: subprocess.Popen :param timeout: The maximum time to wait for the process to finish :type timeout: int Source code in shared/shell.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @contextmanager def with_timeout ( p : subprocess . Popen , timeout : int ): \"\"\" It runs a function in a separate thread, and if the function doesn't return before the timeout, it kills the process :param p: subprocess.Popen :type p: subprocess.Popen :param timeout: The maximum time to wait for the process to finish :type timeout: int \"\"\" def timerout ( p : subprocess . Popen ): p . kill () raise TimeoutError timer = Timer ( timeout , timerout , [ p ]) timer . start () yield timer . cancel ()","title":"shell"},{"location":"reference/shared/shell/#shared.shell.logger","text":"","title":"logger"},{"location":"reference/shared/shell/#shared.shell.CommandFailed","text":"Bases: Exception Source code in shared/shell.py 32 33 34 @dataclass class CommandFailed ( Exception ): code : int","title":"CommandFailed"},{"location":"reference/shared/shell/#shared.shell.CommandFailed.code","text":"","title":"code"},{"location":"reference/shared/shell/#shared.shell.consume_print","text":"It takes an iterator of strings and prints each line to the log :param it: Iterator[str] :type it: Iterator[str] Source code in shared/shell.py 77 78 79 80 81 82 83 84 85 def consume_print ( it : Iterator [ str ]): \"\"\" It takes an iterator of strings and prints each line to the log :param it: Iterator[str] :type it: Iterator[str] \"\"\" for line in it : logger . info ( line )","title":"consume_print()"},{"location":"reference/shared/shell/#shared.shell.execute_command","text":"It runs a command and yields the output line by line :param cmd: The command to execute :type cmd: Union[List[str], str] :param timeout: The maximum time to wait for the command to finish :type timeout: int :param ignore_errors: If True, don't raise an exception if the command fails, defaults to False :type ignore_errors: bool (optional) Source code in shared/shell.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def execute_command ( cmd : Union [ List [ str ], str ], timeout : int = None , ignore_errors : bool = False , ** kwargs , ) -> Iterator [ str ]: \"\"\" It runs a command and yields the output line by line :param cmd: The command to execute :type cmd: Union[List[str], str] :param timeout: The maximum time to wait for the command to finish :type timeout: int :param ignore_errors: If True, don't raise an exception if the command fails, defaults to False :type ignore_errors: bool (optional) \"\"\" p = subprocess . Popen ( cmd , shell = False , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , ** kwargs ) def run (): for line in iter ( p . stdout . readline , b '' ): line = line . rstrip () . decode ( 'utf-8' ) yield line p . stdout . close () code = p . wait () if code != 0 and not ignore_errors : raise CommandFailed ( code ) if timeout is not None : with with_timeout ( p , timeout ): yield from run () else : yield from run ()","title":"execute_command()"},{"location":"reference/shared/shell/#shared.shell.with_timeout","text":"It runs a function in a separate thread, and if the function doesn't return before the timeout, it kills the process :param p: subprocess.Popen :type p: subprocess.Popen :param timeout: The maximum time to wait for the process to finish :type timeout: int Source code in shared/shell.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @contextmanager def with_timeout ( p : subprocess . Popen , timeout : int ): \"\"\" It runs a function in a separate thread, and if the function doesn't return before the timeout, it kills the process :param p: subprocess.Popen :type p: subprocess.Popen :param timeout: The maximum time to wait for the process to finish :type timeout: int \"\"\" def timerout ( p : subprocess . Popen ): p . kill () raise TimeoutError timer = Timer ( timeout , timerout , [ p ]) timer . start () yield timer . cancel ()","title":"with_timeout()"},{"location":"reference/shared/websocket/","text":"T = TypeVar ( 'T' ) module-attribute Packet dataclass Bases: Serializable Packet class represents a message that is sent through the websocket. Source code in shared/websocket.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @dataclass class Packet ( Serializable ): \"\"\" Packet class represents a message that is sent through the websocket. \"\"\" type : str \"\"\"The type of the packet.\"\"\" data : Generic [ T ] = None \"\"\"The data field is the actual data that is sent through the websocket.\"\"\" def to_dict ( self , dict_factory : Type [ Dict ] = dict , recurse : bool = True ) -> Dict : return { 'type' : str ( self . type ), 'data' : self . data . to_dict () if isinstance ( self . data , Serializable ) else self . data , } data : Generic [ T ] = None class-attribute The data field is the actual data that is sent through the websocket. type : str class-attribute The type of the packet. to_dict ( dict_factory = dict , recurse = True ) Source code in shared/websocket.py 19 20 21 22 23 def to_dict ( self , dict_factory : Type [ Dict ] = dict , recurse : bool = True ) -> Dict : return { 'type' : str ( self . type ), 'data' : self . data . to_dict () if isinstance ( self . data , Serializable ) else self . data , }","title":"websocket"},{"location":"reference/shared/websocket/#shared.websocket.T","text":"","title":"T"},{"location":"reference/shared/websocket/#shared.websocket.Packet","text":"Bases: Serializable Packet class represents a message that is sent through the websocket. Source code in shared/websocket.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @dataclass class Packet ( Serializable ): \"\"\" Packet class represents a message that is sent through the websocket. \"\"\" type : str \"\"\"The type of the packet.\"\"\" data : Generic [ T ] = None \"\"\"The data field is the actual data that is sent through the websocket.\"\"\" def to_dict ( self , dict_factory : Type [ Dict ] = dict , recurse : bool = True ) -> Dict : return { 'type' : str ( self . type ), 'data' : self . data . to_dict () if isinstance ( self . data , Serializable ) else self . data , }","title":"Packet"},{"location":"reference/shared/websocket/#shared.websocket.Packet.data","text":"The data field is the actual data that is sent through the websocket.","title":"data"},{"location":"reference/shared/websocket/#shared.websocket.Packet.type","text":"The type of the packet.","title":"type"},{"location":"reference/shared/websocket/#shared.websocket.Packet.to_dict","text":"Source code in shared/websocket.py 19 20 21 22 23 def to_dict ( self , dict_factory : Type [ Dict ] = dict , recurse : bool = True ) -> Dict : return { 'type' : str ( self . type ), 'data' : self . data . to_dict () if isinstance ( self . data , Serializable ) else self . data , }","title":"to_dict()"},{"location":"reference/tasks/","text":"default_app_config = 'tasks.apps.TasksAppConfig' module-attribute","title":"tasks"},{"location":"reference/tasks/#tasks.default_app_config","text":"","title":"default_app_config"},{"location":"reference/tasks/apps/","text":"old_default = JSONEncoder . default module-attribute TasksAppConfig Bases: AppConfig Source code in tasks/apps.py 18 19 20 21 22 23 class TasksAppConfig ( AppConfig ): name = 'tasks' def ready ( self ): # noinspection PyUnresolvedReferences import tasks.signals name = 'tasks' class-attribute ready () Source code in tasks/apps.py 21 22 23 def ready ( self ): # noinspection PyUnresolvedReferences import tasks.signals new_default ( self , obj ) Source code in tasks/apps.py 9 10 11 12 def new_default ( self , obj ): if isinstance ( obj , UUID ): return str ( obj ) return old_default ( self , obj )","title":"apps"},{"location":"reference/tasks/apps/#tasks.apps.old_default","text":"","title":"old_default"},{"location":"reference/tasks/apps/#tasks.apps.TasksAppConfig","text":"Bases: AppConfig Source code in tasks/apps.py 18 19 20 21 22 23 class TasksAppConfig ( AppConfig ): name = 'tasks' def ready ( self ): # noinspection PyUnresolvedReferences import tasks.signals","title":"TasksAppConfig"},{"location":"reference/tasks/apps/#tasks.apps.TasksAppConfig.name","text":"","title":"name"},{"location":"reference/tasks/apps/#tasks.apps.TasksAppConfig.ready","text":"Source code in tasks/apps.py 21 22 23 def ready ( self ): # noinspection PyUnresolvedReferences import tasks.signals","title":"ready()"},{"location":"reference/tasks/apps/#tasks.apps.new_default","text":"Source code in tasks/apps.py 9 10 11 12 def new_default ( self , obj ): if isinstance ( obj , UUID ): return str ( obj ) return old_default ( self , obj )","title":"new_default()"},{"location":"reference/tasks/consumers/","text":"GLOBAL_TASKS_GROUP = 'TASKS_GLOBAL' module-attribute logger = get_logger () module-attribute TasksConsumer Bases: WebsocketConsumer Source code in tasks/consumers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class TasksConsumer ( WebsocketConsumer ): def connect ( self ): self . user = self . scope [ \"user\" ] async_to_sync ( self . channel_layer . group_add )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) self . accept () def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) def send_packet ( self , type : str , data : Any ): self . send ( text_data = Packet ( type , data ) . dumps ()) def task_updated ( self , event ): task : Task = Task . objects . get ( task_id = event [ 'message' ]) if task . can_view ( self . user ): self . send_packet ( 'TASK_UPDATED' , TaskSerializer ( task ) . data ) connect () Source code in tasks/consumers.py 17 18 19 20 21 22 23 def connect ( self ): self . user = self . scope [ \"user\" ] async_to_sync ( self . channel_layer . group_add )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) self . accept () disconnect ( close_code ) Source code in tasks/consumers.py 25 26 27 28 def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) receive ( text_data ) Source code in tasks/consumers.py 30 31 32 def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) send_packet ( type , data ) Source code in tasks/consumers.py 34 35 def send_packet ( self , type : str , data : Any ): self . send ( text_data = Packet ( type , data ) . dumps ()) task_updated ( event ) Source code in tasks/consumers.py 37 38 39 40 def task_updated ( self , event ): task : Task = Task . objects . get ( task_id = event [ 'message' ]) if task . can_view ( self . user ): self . send_packet ( 'TASK_UPDATED' , TaskSerializer ( task ) . data )","title":"consumers"},{"location":"reference/tasks/consumers/#tasks.consumers.GLOBAL_TASKS_GROUP","text":"","title":"GLOBAL_TASKS_GROUP"},{"location":"reference/tasks/consumers/#tasks.consumers.logger","text":"","title":"logger"},{"location":"reference/tasks/consumers/#tasks.consumers.TasksConsumer","text":"Bases: WebsocketConsumer Source code in tasks/consumers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class TasksConsumer ( WebsocketConsumer ): def connect ( self ): self . user = self . scope [ \"user\" ] async_to_sync ( self . channel_layer . group_add )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) self . accept () def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' ) def send_packet ( self , type : str , data : Any ): self . send ( text_data = Packet ( type , data ) . dumps ()) def task_updated ( self , event ): task : Task = Task . objects . get ( task_id = event [ 'message' ]) if task . can_view ( self . user ): self . send_packet ( 'TASK_UPDATED' , TaskSerializer ( task ) . data )","title":"TasksConsumer"},{"location":"reference/tasks/consumers/#tasks.consumers.TasksConsumer.connect","text":"Source code in tasks/consumers.py 17 18 19 20 21 22 23 def connect ( self ): self . user = self . scope [ \"user\" ] async_to_sync ( self . channel_layer . group_add )( str ( GLOBAL_TASKS_GROUP ), self . channel_name ) self . accept ()","title":"connect()"},{"location":"reference/tasks/consumers/#tasks.consumers.TasksConsumer.disconnect","text":"Source code in tasks/consumers.py 25 26 27 28 def disconnect ( self , close_code ): async_to_sync ( self . channel_layer . group_discard )( str ( GLOBAL_TASKS_GROUP ), self . channel_name )","title":"disconnect()"},{"location":"reference/tasks/consumers/#tasks.consumers.TasksConsumer.receive","text":"Source code in tasks/consumers.py 30 31 32 def receive ( self , text_data ): packet = Packet . loads_json ( text_data ) logger . info ( f 'Received { packet . type } packet' )","title":"receive()"},{"location":"reference/tasks/consumers/#tasks.consumers.TasksConsumer.send_packet","text":"Source code in tasks/consumers.py 34 35 def send_packet ( self , type : str , data : Any ): self . send ( text_data = Packet ( type , data ) . dumps ())","title":"send_packet()"},{"location":"reference/tasks/consumers/#tasks.consumers.TasksConsumer.task_updated","text":"Source code in tasks/consumers.py 37 38 39 40 def task_updated ( self , event ): task : Task = Task . objects . get ( task_id = event [ 'message' ]) if task . can_view ( self . user ): self . send_packet ( 'TASK_UPDATED' , TaskSerializer ( task ) . data )","title":"task_updated()"},{"location":"reference/tasks/models/","text":"CustomGenericRelation Bases: GenericRelation Source code in tasks/models.py 181 182 183 class CustomGenericRelation ( GenericRelation ): def bulk_related_objects ( self , objs , using = \"default\" ): return [] bulk_related_objects ( objs , using = 'default' ) Source code in tasks/models.py 182 183 def bulk_related_objects ( self , objs , using = \"default\" ): return [] ModelAsyncResult Bases: AsyncResult Source code in tasks/models.py 106 107 108 109 class ModelAsyncResult ( AsyncResult ): def forget ( self ): Task . objects . filter ( task_id = self . id ) . delete () return super ( ModelAsyncResult , self ) . forget () forget () Source code in tasks/models.py 107 108 109 def forget ( self ): Task . objects . filter ( task_id = self . id ) . delete () return super ( ModelAsyncResult , self ) . forget () Task Bases: TimeStampMixin Source code in tasks/models.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class Task ( TimeStampMixin ): STATES = ( ( TaskState . PENDING , 'PENDING' ), ( TaskState . STARTED , 'STARTED' ), ( TaskState . RETRY , 'RETRY' ), ( TaskState . FAILURE , 'FAILURE' ), ( TaskState . SUCCESS , 'SUCCESS' ), ) task_id = models . UUIDField ( primary_key = True ) state = models . CharField ( choices = STATES , default = TaskState . PENDING , max_length = 255 ) object_id = models . UUIDField ( null = True ) content_object = GenericForeignKey () content_type = models . ForeignKey ( ContentType , null = True , on_delete = models . SET_NULL ) name = models . CharField ( max_length = 255 ) creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) \"\"\"The user who started the task.\"\"\" objects = TaskManager () def __str__ ( self ): return ' %s : %s ' % ( self . task_id , dict ( self . STATES )[ self . state ]) @property def result ( self ): return ModelAsyncResult ( self . task_id ) @property def short_id ( self ): return str ( self . task_id )[: 6 ] def delete ( self , using = None , keep_parents = False ): return super () . delete ( using , keep_parents ) def can_view ( self , user : User ): return user . is_superuser or self . creator == user or ( self . content_object and self . content_object . can_view ( user ) ) STATES = (( TaskState . PENDING , 'PENDING' ), ( TaskState . STARTED , 'STARTED' ), ( TaskState . RETRY , 'RETRY' ), ( TaskState . FAILURE , 'FAILURE' ), ( TaskState . SUCCESS , 'SUCCESS' )) class-attribute content_object = GenericForeignKey () class-attribute content_type = models . ForeignKey ( ContentType , null = True , on_delete = models . SET_NULL ) class-attribute creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) class-attribute The user who started the task. name = models . CharField ( max_length = 255 ) class-attribute object_id = models . UUIDField ( null = True ) class-attribute objects = TaskManager () class-attribute state = models . CharField ( choices = STATES , default = TaskState . PENDING , max_length = 255 ) class-attribute task_id = models . UUIDField ( primary_key = True ) class-attribute __str__ () Source code in tasks/models.py 86 87 def __str__ ( self ): return ' %s : %s ' % ( self . task_id , dict ( self . STATES )[ self . state ]) can_view ( user ) Source code in tasks/models.py 100 101 102 103 def can_view ( self , user : User ): return user . is_superuser or self . creator == user or ( self . content_object and self . content_object . can_view ( user ) ) delete ( using = None , keep_parents = False ) Source code in tasks/models.py 97 98 def delete ( self , using = None , keep_parents = False ): return super () . delete ( using , keep_parents ) result () property Source code in tasks/models.py 89 90 91 @property def result ( self ): return ModelAsyncResult ( self . task_id ) short_id () property Source code in tasks/models.py 93 94 95 @property def short_id ( self ): return str ( self . task_id )[: 6 ] TaskFilterMixin Bases: object Source code in tasks/models.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class TaskFilterMixin ( object ): objects : models . Manager () def pending ( self ): return self . objects . filter ( state = TaskState . PENDING ) def started ( self ): return self . objects . filter ( state = TaskState . STARTED ) def retrying ( self ): return self . objects . filter ( state = TaskState . RETRY ) def failed ( self ): return self . objects . filter ( state = TaskState . FAILURE ) def successful ( self ): return self . objects . filter ( state = TaskState . SUCCESS ) def running ( self ): return self . objects . filter ( Q ( state = TaskState . PENDING ) | Q ( state = TaskState . STARTED ) | Q ( state = TaskState . RETRY )) def ready ( self ): return self . objects . filter ( Q ( state = TaskState . FAILURE ) | Q ( state = TaskState . SUCCESS )) objects : models . Manager () class-attribute failed () Source code in tasks/models.py 39 40 def failed ( self ): return self . objects . filter ( state = TaskState . FAILURE ) pending () Source code in tasks/models.py 30 31 def pending ( self ): return self . objects . filter ( state = TaskState . PENDING ) ready () Source code in tasks/models.py 50 51 52 def ready ( self ): return self . objects . filter ( Q ( state = TaskState . FAILURE ) | Q ( state = TaskState . SUCCESS )) retrying () Source code in tasks/models.py 36 37 def retrying ( self ): return self . objects . filter ( state = TaskState . RETRY ) running () Source code in tasks/models.py 45 46 47 48 def running ( self ): return self . objects . filter ( Q ( state = TaskState . PENDING ) | Q ( state = TaskState . STARTED ) | Q ( state = TaskState . RETRY )) started () Source code in tasks/models.py 33 34 def started ( self ): return self . objects . filter ( state = TaskState . STARTED ) successful () Source code in tasks/models.py 42 43 def successful ( self ): return self . objects . filter ( state = TaskState . SUCCESS ) TaskManager Bases: TaskFilterMixin , models . Manager Source code in tasks/models.py 59 60 61 62 63 class TaskManager ( TaskFilterMixin , models . Manager ): use_for_related_fields = True def get_queryset ( self ): return TaskQuerySet ( self . model , using = self . _db ) use_for_related_fields = True class-attribute get_queryset () Source code in tasks/models.py 62 63 def get_queryset ( self ): return TaskQuerySet ( self . model , using = self . _db ) TaskMetaFilterMixin Bases: object Source code in tasks/models.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class TaskMetaFilterMixin ( object ): objects : models . Manager () def with_tasks ( self ): return self . objects . filter ( tasks__state__isnull = False ) def with_pending_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . PENDING ) def with_started_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . STARTED ) def with_retrying_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . RETRY ) def with_failed_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . FAILURE ) def with_successful_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . SUCCESS ) def with_running_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY )) def with_ready_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS )) def without_tasks ( self ): return self . objects . exclude ( tasks__state__isnull = False ) def without_pending_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . PENDING ) def without_started_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . STARTED ) def without_retrying_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . RETRY ) def without_failed_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . FAILURE ) def without_successful_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . SUCCESS ) def without_running_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY )) def without_ready_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS )) objects : models . Manager () class-attribute with_failed_tasks () Source code in tasks/models.py 127 128 def with_failed_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . FAILURE ) with_pending_tasks () Source code in tasks/models.py 118 119 def with_pending_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . PENDING ) with_ready_tasks () Source code in tasks/models.py 138 139 140 def with_ready_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS )) with_retrying_tasks () Source code in tasks/models.py 124 125 def with_retrying_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . RETRY ) with_running_tasks () Source code in tasks/models.py 133 134 135 136 def with_running_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY )) with_started_tasks () Source code in tasks/models.py 121 122 def with_started_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . STARTED ) with_successful_tasks () Source code in tasks/models.py 130 131 def with_successful_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . SUCCESS ) with_tasks () Source code in tasks/models.py 115 116 def with_tasks ( self ): return self . objects . filter ( tasks__state__isnull = False ) without_failed_tasks () Source code in tasks/models.py 154 155 def without_failed_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . FAILURE ) without_pending_tasks () Source code in tasks/models.py 145 146 def without_pending_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . PENDING ) without_ready_tasks () Source code in tasks/models.py 165 166 167 def without_ready_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS )) without_retrying_tasks () Source code in tasks/models.py 151 152 def without_retrying_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . RETRY ) without_running_tasks () Source code in tasks/models.py 160 161 162 163 def without_running_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY )) without_started_tasks () Source code in tasks/models.py 148 149 def without_started_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . STARTED ) without_successful_tasks () Source code in tasks/models.py 157 158 def without_successful_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . SUCCESS ) without_tasks () Source code in tasks/models.py 142 143 def without_tasks ( self ): return self . objects . exclude ( tasks__state__isnull = False ) TaskMetaManager Bases: TaskMetaFilterMixin , models . Manager Source code in tasks/models.py 174 175 176 177 178 class TaskMetaManager ( TaskMetaFilterMixin , models . Manager ): use_for_related_fields = True def get_queryset ( self ): return TaskMetaQuerySet ( self . model , using = self . _db ) use_for_related_fields = True class-attribute get_queryset () Source code in tasks/models.py 177 178 def get_queryset ( self ): return TaskMetaQuerySet ( self . model , using = self . _db ) TaskMetaQuerySet Bases: TaskMetaFilterMixin , QuerySet Source code in tasks/models.py 170 171 class TaskMetaQuerySet ( TaskMetaFilterMixin , QuerySet ): pass TaskMixin Bases: models . Model Source code in tasks/models.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 class TaskMixin ( models . Model ): tasks = CustomGenericRelation ( Task ) objects = TaskMetaManager () class Meta : abstract = True @property def has_running_task ( self ): return self . tasks . running () . exists () @property def has_ready_task ( self ): return self . tasks . ready () . exists () def apply_async ( self , task_fn , * args , name = None , creator = None , ** kwargs ): if 'task_id' in kwargs : task_id = kwargs [ 'task_id' ] else : task_id = kwargs [ 'task_id' ] = uuid () if name is None : name = task_fn . __name__ forget_if_ready ( AsyncResult ( task_id )) try : task = Task . objects . get ( task_id = task_id ) task . content_object = self task . name = name task . creator = creator except Task . DoesNotExist : task = Task ( task_id = task_id , content_object = self , name = name ) task . creator = creator task . save () return task_fn . apply_async ( * args , ** kwargs ) def get_task_results ( self ): return map ( lambda x : x . result , self . tasks . all ()) def get_task_result ( self , task_id ): return self . tasks . get ( task_id = task_id ) . result def clear_task_results ( self ): for task_result in self . get_task_results (): forget_if_ready ( task_result ) def clear_task_result ( self , task_id ): task_result = self . get_task_result ( task_id ) forget_if_ready ( task_result ) objects = TaskMetaManager () class-attribute tasks = CustomGenericRelation ( Task ) class-attribute Meta Source code in tasks/models.py 192 193 class Meta : abstract = True abstract = True class-attribute apply_async ( task_fn , * args , name = None , creator = None , ** kwargs ) Source code in tasks/models.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def apply_async ( self , task_fn , * args , name = None , creator = None , ** kwargs ): if 'task_id' in kwargs : task_id = kwargs [ 'task_id' ] else : task_id = kwargs [ 'task_id' ] = uuid () if name is None : name = task_fn . __name__ forget_if_ready ( AsyncResult ( task_id )) try : task = Task . objects . get ( task_id = task_id ) task . content_object = self task . name = name task . creator = creator except Task . DoesNotExist : task = Task ( task_id = task_id , content_object = self , name = name ) task . creator = creator task . save () return task_fn . apply_async ( * args , ** kwargs ) clear_task_result ( task_id ) Source code in tasks/models.py 235 236 237 def clear_task_result ( self , task_id ): task_result = self . get_task_result ( task_id ) forget_if_ready ( task_result ) clear_task_results () Source code in tasks/models.py 231 232 233 def clear_task_results ( self ): for task_result in self . get_task_results (): forget_if_ready ( task_result ) get_task_result ( task_id ) Source code in tasks/models.py 228 229 def get_task_result ( self , task_id ): return self . tasks . get ( task_id = task_id ) . result get_task_results () Source code in tasks/models.py 225 226 def get_task_results ( self ): return map ( lambda x : x . result , self . tasks . all ()) has_ready_task () property Source code in tasks/models.py 199 200 201 @property def has_ready_task ( self ): return self . tasks . ready () . exists () has_running_task () property Source code in tasks/models.py 195 196 197 @property def has_running_task ( self ): return self . tasks . running () . exists () TaskQuerySet Bases: TaskFilterMixin , QuerySet Source code in tasks/models.py 55 56 class TaskQuerySet ( TaskFilterMixin , QuerySet ): pass TaskState Bases: object Source code in tasks/models.py 15 16 17 18 19 20 21 22 23 24 class TaskState ( object ): PENDING = 'PENDING' STARTED = 'STARTED' RETRY = 'RETRY' FAILURE = 'FAILURE' SUCCESS = 'SUCCESS' @classmethod def lookup ( cls , state ): return getattr ( cls , state ) FAILURE = 'FAILURE' class-attribute PENDING = 'PENDING' class-attribute RETRY = 'RETRY' class-attribute STARTED = 'STARTED' class-attribute SUCCESS = 'SUCCESS' class-attribute lookup ( state ) classmethod Source code in tasks/models.py 22 23 24 @classmethod def lookup ( cls , state ): return getattr ( cls , state ) forget_if_ready ( async_result ) Source code in tasks/models.py 240 241 242 def forget_if_ready ( async_result ): if async_result and async_result . ready (): async_result . forget ()","title":"models"},{"location":"reference/tasks/models/#tasks.models.CustomGenericRelation","text":"Bases: GenericRelation Source code in tasks/models.py 181 182 183 class CustomGenericRelation ( GenericRelation ): def bulk_related_objects ( self , objs , using = \"default\" ): return []","title":"CustomGenericRelation"},{"location":"reference/tasks/models/#tasks.models.CustomGenericRelation.bulk_related_objects","text":"Source code in tasks/models.py 182 183 def bulk_related_objects ( self , objs , using = \"default\" ): return []","title":"bulk_related_objects()"},{"location":"reference/tasks/models/#tasks.models.ModelAsyncResult","text":"Bases: AsyncResult Source code in tasks/models.py 106 107 108 109 class ModelAsyncResult ( AsyncResult ): def forget ( self ): Task . objects . filter ( task_id = self . id ) . delete () return super ( ModelAsyncResult , self ) . forget ()","title":"ModelAsyncResult"},{"location":"reference/tasks/models/#tasks.models.ModelAsyncResult.forget","text":"Source code in tasks/models.py 107 108 109 def forget ( self ): Task . objects . filter ( task_id = self . id ) . delete () return super ( ModelAsyncResult , self ) . forget ()","title":"forget()"},{"location":"reference/tasks/models/#tasks.models.Task","text":"Bases: TimeStampMixin Source code in tasks/models.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class Task ( TimeStampMixin ): STATES = ( ( TaskState . PENDING , 'PENDING' ), ( TaskState . STARTED , 'STARTED' ), ( TaskState . RETRY , 'RETRY' ), ( TaskState . FAILURE , 'FAILURE' ), ( TaskState . SUCCESS , 'SUCCESS' ), ) task_id = models . UUIDField ( primary_key = True ) state = models . CharField ( choices = STATES , default = TaskState . PENDING , max_length = 255 ) object_id = models . UUIDField ( null = True ) content_object = GenericForeignKey () content_type = models . ForeignKey ( ContentType , null = True , on_delete = models . SET_NULL ) name = models . CharField ( max_length = 255 ) creator = models . ForeignKey ( settings . AUTH_USER_MODEL , on_delete = models . SET_NULL , null = True ) \"\"\"The user who started the task.\"\"\" objects = TaskManager () def __str__ ( self ): return ' %s : %s ' % ( self . task_id , dict ( self . STATES )[ self . state ]) @property def result ( self ): return ModelAsyncResult ( self . task_id ) @property def short_id ( self ): return str ( self . task_id )[: 6 ] def delete ( self , using = None , keep_parents = False ): return super () . delete ( using , keep_parents ) def can_view ( self , user : User ): return user . is_superuser or self . creator == user or ( self . content_object and self . content_object . can_view ( user ) )","title":"Task"},{"location":"reference/tasks/models/#tasks.models.Task.STATES","text":"","title":"STATES"},{"location":"reference/tasks/models/#tasks.models.Task.content_object","text":"","title":"content_object"},{"location":"reference/tasks/models/#tasks.models.Task.content_type","text":"","title":"content_type"},{"location":"reference/tasks/models/#tasks.models.Task.creator","text":"The user who started the task.","title":"creator"},{"location":"reference/tasks/models/#tasks.models.Task.name","text":"","title":"name"},{"location":"reference/tasks/models/#tasks.models.Task.object_id","text":"","title":"object_id"},{"location":"reference/tasks/models/#tasks.models.Task.objects","text":"","title":"objects"},{"location":"reference/tasks/models/#tasks.models.Task.state","text":"","title":"state"},{"location":"reference/tasks/models/#tasks.models.Task.task_id","text":"","title":"task_id"},{"location":"reference/tasks/models/#tasks.models.Task.__str__","text":"Source code in tasks/models.py 86 87 def __str__ ( self ): return ' %s : %s ' % ( self . task_id , dict ( self . STATES )[ self . state ])","title":"__str__()"},{"location":"reference/tasks/models/#tasks.models.Task.can_view","text":"Source code in tasks/models.py 100 101 102 103 def can_view ( self , user : User ): return user . is_superuser or self . creator == user or ( self . content_object and self . content_object . can_view ( user ) )","title":"can_view()"},{"location":"reference/tasks/models/#tasks.models.Task.delete","text":"Source code in tasks/models.py 97 98 def delete ( self , using = None , keep_parents = False ): return super () . delete ( using , keep_parents )","title":"delete()"},{"location":"reference/tasks/models/#tasks.models.Task.result","text":"Source code in tasks/models.py 89 90 91 @property def result ( self ): return ModelAsyncResult ( self . task_id )","title":"result()"},{"location":"reference/tasks/models/#tasks.models.Task.short_id","text":"Source code in tasks/models.py 93 94 95 @property def short_id ( self ): return str ( self . task_id )[: 6 ]","title":"short_id()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin","text":"Bases: object Source code in tasks/models.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class TaskFilterMixin ( object ): objects : models . Manager () def pending ( self ): return self . objects . filter ( state = TaskState . PENDING ) def started ( self ): return self . objects . filter ( state = TaskState . STARTED ) def retrying ( self ): return self . objects . filter ( state = TaskState . RETRY ) def failed ( self ): return self . objects . filter ( state = TaskState . FAILURE ) def successful ( self ): return self . objects . filter ( state = TaskState . SUCCESS ) def running ( self ): return self . objects . filter ( Q ( state = TaskState . PENDING ) | Q ( state = TaskState . STARTED ) | Q ( state = TaskState . RETRY )) def ready ( self ): return self . objects . filter ( Q ( state = TaskState . FAILURE ) | Q ( state = TaskState . SUCCESS ))","title":"TaskFilterMixin"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.objects","text":"","title":"objects"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.failed","text":"Source code in tasks/models.py 39 40 def failed ( self ): return self . objects . filter ( state = TaskState . FAILURE )","title":"failed()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.pending","text":"Source code in tasks/models.py 30 31 def pending ( self ): return self . objects . filter ( state = TaskState . PENDING )","title":"pending()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.ready","text":"Source code in tasks/models.py 50 51 52 def ready ( self ): return self . objects . filter ( Q ( state = TaskState . FAILURE ) | Q ( state = TaskState . SUCCESS ))","title":"ready()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.retrying","text":"Source code in tasks/models.py 36 37 def retrying ( self ): return self . objects . filter ( state = TaskState . RETRY )","title":"retrying()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.running","text":"Source code in tasks/models.py 45 46 47 48 def running ( self ): return self . objects . filter ( Q ( state = TaskState . PENDING ) | Q ( state = TaskState . STARTED ) | Q ( state = TaskState . RETRY ))","title":"running()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.started","text":"Source code in tasks/models.py 33 34 def started ( self ): return self . objects . filter ( state = TaskState . STARTED )","title":"started()"},{"location":"reference/tasks/models/#tasks.models.TaskFilterMixin.successful","text":"Source code in tasks/models.py 42 43 def successful ( self ): return self . objects . filter ( state = TaskState . SUCCESS )","title":"successful()"},{"location":"reference/tasks/models/#tasks.models.TaskManager","text":"Bases: TaskFilterMixin , models . Manager Source code in tasks/models.py 59 60 61 62 63 class TaskManager ( TaskFilterMixin , models . Manager ): use_for_related_fields = True def get_queryset ( self ): return TaskQuerySet ( self . model , using = self . _db )","title":"TaskManager"},{"location":"reference/tasks/models/#tasks.models.TaskManager.use_for_related_fields","text":"","title":"use_for_related_fields"},{"location":"reference/tasks/models/#tasks.models.TaskManager.get_queryset","text":"Source code in tasks/models.py 62 63 def get_queryset ( self ): return TaskQuerySet ( self . model , using = self . _db )","title":"get_queryset()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin","text":"Bases: object Source code in tasks/models.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 class TaskMetaFilterMixin ( object ): objects : models . Manager () def with_tasks ( self ): return self . objects . filter ( tasks__state__isnull = False ) def with_pending_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . PENDING ) def with_started_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . STARTED ) def with_retrying_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . RETRY ) def with_failed_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . FAILURE ) def with_successful_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . SUCCESS ) def with_running_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY )) def with_ready_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS )) def without_tasks ( self ): return self . objects . exclude ( tasks__state__isnull = False ) def without_pending_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . PENDING ) def without_started_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . STARTED ) def without_retrying_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . RETRY ) def without_failed_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . FAILURE ) def without_successful_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . SUCCESS ) def without_running_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY )) def without_ready_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS ))","title":"TaskMetaFilterMixin"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.objects","text":"","title":"objects"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_failed_tasks","text":"Source code in tasks/models.py 127 128 def with_failed_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . FAILURE )","title":"with_failed_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_pending_tasks","text":"Source code in tasks/models.py 118 119 def with_pending_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . PENDING )","title":"with_pending_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_ready_tasks","text":"Source code in tasks/models.py 138 139 140 def with_ready_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS ))","title":"with_ready_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_retrying_tasks","text":"Source code in tasks/models.py 124 125 def with_retrying_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . RETRY )","title":"with_retrying_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_running_tasks","text":"Source code in tasks/models.py 133 134 135 136 def with_running_tasks ( self ): return self . objects . filter ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY ))","title":"with_running_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_started_tasks","text":"Source code in tasks/models.py 121 122 def with_started_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . STARTED )","title":"with_started_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_successful_tasks","text":"Source code in tasks/models.py 130 131 def with_successful_tasks ( self ): return self . objects . filter ( tasks__state = TaskState . SUCCESS )","title":"with_successful_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.with_tasks","text":"Source code in tasks/models.py 115 116 def with_tasks ( self ): return self . objects . filter ( tasks__state__isnull = False )","title":"with_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_failed_tasks","text":"Source code in tasks/models.py 154 155 def without_failed_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . FAILURE )","title":"without_failed_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_pending_tasks","text":"Source code in tasks/models.py 145 146 def without_pending_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . PENDING )","title":"without_pending_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_ready_tasks","text":"Source code in tasks/models.py 165 166 167 def without_ready_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . FAILURE ) | Q ( tasks__state = TaskState . SUCCESS ))","title":"without_ready_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_retrying_tasks","text":"Source code in tasks/models.py 151 152 def without_retrying_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . RETRY )","title":"without_retrying_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_running_tasks","text":"Source code in tasks/models.py 160 161 162 163 def without_running_tasks ( self ): return self . objects . exclude ( Q ( tasks__state = TaskState . PENDING ) | Q ( tasks__state = TaskState . STARTED ) | Q ( tasks__state = TaskState . RETRY ))","title":"without_running_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_started_tasks","text":"Source code in tasks/models.py 148 149 def without_started_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . STARTED )","title":"without_started_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_successful_tasks","text":"Source code in tasks/models.py 157 158 def without_successful_tasks ( self ): return self . objects . exclude ( tasks__state = TaskState . SUCCESS )","title":"without_successful_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaFilterMixin.without_tasks","text":"Source code in tasks/models.py 142 143 def without_tasks ( self ): return self . objects . exclude ( tasks__state__isnull = False )","title":"without_tasks()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaManager","text":"Bases: TaskMetaFilterMixin , models . Manager Source code in tasks/models.py 174 175 176 177 178 class TaskMetaManager ( TaskMetaFilterMixin , models . Manager ): use_for_related_fields = True def get_queryset ( self ): return TaskMetaQuerySet ( self . model , using = self . _db )","title":"TaskMetaManager"},{"location":"reference/tasks/models/#tasks.models.TaskMetaManager.use_for_related_fields","text":"","title":"use_for_related_fields"},{"location":"reference/tasks/models/#tasks.models.TaskMetaManager.get_queryset","text":"Source code in tasks/models.py 177 178 def get_queryset ( self ): return TaskMetaQuerySet ( self . model , using = self . _db )","title":"get_queryset()"},{"location":"reference/tasks/models/#tasks.models.TaskMetaQuerySet","text":"Bases: TaskMetaFilterMixin , QuerySet Source code in tasks/models.py 170 171 class TaskMetaQuerySet ( TaskMetaFilterMixin , QuerySet ): pass","title":"TaskMetaQuerySet"},{"location":"reference/tasks/models/#tasks.models.TaskMixin","text":"Bases: models . Model Source code in tasks/models.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 class TaskMixin ( models . Model ): tasks = CustomGenericRelation ( Task ) objects = TaskMetaManager () class Meta : abstract = True @property def has_running_task ( self ): return self . tasks . running () . exists () @property def has_ready_task ( self ): return self . tasks . ready () . exists () def apply_async ( self , task_fn , * args , name = None , creator = None , ** kwargs ): if 'task_id' in kwargs : task_id = kwargs [ 'task_id' ] else : task_id = kwargs [ 'task_id' ] = uuid () if name is None : name = task_fn . __name__ forget_if_ready ( AsyncResult ( task_id )) try : task = Task . objects . get ( task_id = task_id ) task . content_object = self task . name = name task . creator = creator except Task . DoesNotExist : task = Task ( task_id = task_id , content_object = self , name = name ) task . creator = creator task . save () return task_fn . apply_async ( * args , ** kwargs ) def get_task_results ( self ): return map ( lambda x : x . result , self . tasks . all ()) def get_task_result ( self , task_id ): return self . tasks . get ( task_id = task_id ) . result def clear_task_results ( self ): for task_result in self . get_task_results (): forget_if_ready ( task_result ) def clear_task_result ( self , task_id ): task_result = self . get_task_result ( task_id ) forget_if_ready ( task_result )","title":"TaskMixin"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.objects","text":"","title":"objects"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.tasks","text":"","title":"tasks"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.Meta","text":"Source code in tasks/models.py 192 193 class Meta : abstract = True","title":"Meta"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.Meta.abstract","text":"","title":"abstract"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.apply_async","text":"Source code in tasks/models.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def apply_async ( self , task_fn , * args , name = None , creator = None , ** kwargs ): if 'task_id' in kwargs : task_id = kwargs [ 'task_id' ] else : task_id = kwargs [ 'task_id' ] = uuid () if name is None : name = task_fn . __name__ forget_if_ready ( AsyncResult ( task_id )) try : task = Task . objects . get ( task_id = task_id ) task . content_object = self task . name = name task . creator = creator except Task . DoesNotExist : task = Task ( task_id = task_id , content_object = self , name = name ) task . creator = creator task . save () return task_fn . apply_async ( * args , ** kwargs )","title":"apply_async()"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.clear_task_result","text":"Source code in tasks/models.py 235 236 237 def clear_task_result ( self , task_id ): task_result = self . get_task_result ( task_id ) forget_if_ready ( task_result )","title":"clear_task_result()"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.clear_task_results","text":"Source code in tasks/models.py 231 232 233 def clear_task_results ( self ): for task_result in self . get_task_results (): forget_if_ready ( task_result )","title":"clear_task_results()"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.get_task_result","text":"Source code in tasks/models.py 228 229 def get_task_result ( self , task_id ): return self . tasks . get ( task_id = task_id ) . result","title":"get_task_result()"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.get_task_results","text":"Source code in tasks/models.py 225 226 def get_task_results ( self ): return map ( lambda x : x . result , self . tasks . all ())","title":"get_task_results()"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.has_ready_task","text":"Source code in tasks/models.py 199 200 201 @property def has_ready_task ( self ): return self . tasks . ready () . exists ()","title":"has_ready_task()"},{"location":"reference/tasks/models/#tasks.models.TaskMixin.has_running_task","text":"Source code in tasks/models.py 195 196 197 @property def has_running_task ( self ): return self . tasks . running () . exists ()","title":"has_running_task()"},{"location":"reference/tasks/models/#tasks.models.TaskQuerySet","text":"Bases: TaskFilterMixin , QuerySet Source code in tasks/models.py 55 56 class TaskQuerySet ( TaskFilterMixin , QuerySet ): pass","title":"TaskQuerySet"},{"location":"reference/tasks/models/#tasks.models.TaskState","text":"Bases: object Source code in tasks/models.py 15 16 17 18 19 20 21 22 23 24 class TaskState ( object ): PENDING = 'PENDING' STARTED = 'STARTED' RETRY = 'RETRY' FAILURE = 'FAILURE' SUCCESS = 'SUCCESS' @classmethod def lookup ( cls , state ): return getattr ( cls , state )","title":"TaskState"},{"location":"reference/tasks/models/#tasks.models.TaskState.FAILURE","text":"","title":"FAILURE"},{"location":"reference/tasks/models/#tasks.models.TaskState.PENDING","text":"","title":"PENDING"},{"location":"reference/tasks/models/#tasks.models.TaskState.RETRY","text":"","title":"RETRY"},{"location":"reference/tasks/models/#tasks.models.TaskState.STARTED","text":"","title":"STARTED"},{"location":"reference/tasks/models/#tasks.models.TaskState.SUCCESS","text":"","title":"SUCCESS"},{"location":"reference/tasks/models/#tasks.models.TaskState.lookup","text":"Source code in tasks/models.py 22 23 24 @classmethod def lookup ( cls , state ): return getattr ( cls , state )","title":"lookup()"},{"location":"reference/tasks/models/#tasks.models.forget_if_ready","text":"Source code in tasks/models.py 240 241 242 def forget_if_ready ( async_result ): if async_result and async_result . ready (): async_result . forget ()","title":"forget_if_ready()"},{"location":"reference/tasks/serializers/","text":"ContentTypeSerializer Bases: serializers . ModelSerializer Source code in tasks/serializers.py 8 9 10 11 class ContentTypeSerializer ( serializers . ModelSerializer ): class Meta : model = ContentType exclude = [] Meta Source code in tasks/serializers.py 9 10 11 class Meta : model = ContentType exclude = [] exclude = [] class-attribute model = ContentType class-attribute TaskSerializer Bases: serializers . ModelSerializer Source code in tasks/serializers.py 14 15 16 17 18 19 20 class TaskSerializer ( serializers . ModelSerializer ): content_type = ContentTypeSerializer ( read_only = True ) creator = ShortUserSerializer ( read_only = True ) class Meta : model = Task exclude = [] content_type = ContentTypeSerializer ( read_only = True ) class-attribute creator = ShortUserSerializer ( read_only = True ) class-attribute Meta Source code in tasks/serializers.py 18 19 20 class Meta : model = Task exclude = [] exclude = [] class-attribute model = Task class-attribute","title":"serializers"},{"location":"reference/tasks/serializers/#tasks.serializers.ContentTypeSerializer","text":"Bases: serializers . ModelSerializer Source code in tasks/serializers.py 8 9 10 11 class ContentTypeSerializer ( serializers . ModelSerializer ): class Meta : model = ContentType exclude = []","title":"ContentTypeSerializer"},{"location":"reference/tasks/serializers/#tasks.serializers.ContentTypeSerializer.Meta","text":"Source code in tasks/serializers.py 9 10 11 class Meta : model = ContentType exclude = []","title":"Meta"},{"location":"reference/tasks/serializers/#tasks.serializers.ContentTypeSerializer.Meta.exclude","text":"","title":"exclude"},{"location":"reference/tasks/serializers/#tasks.serializers.ContentTypeSerializer.Meta.model","text":"","title":"model"},{"location":"reference/tasks/serializers/#tasks.serializers.TaskSerializer","text":"Bases: serializers . ModelSerializer Source code in tasks/serializers.py 14 15 16 17 18 19 20 class TaskSerializer ( serializers . ModelSerializer ): content_type = ContentTypeSerializer ( read_only = True ) creator = ShortUserSerializer ( read_only = True ) class Meta : model = Task exclude = []","title":"TaskSerializer"},{"location":"reference/tasks/serializers/#tasks.serializers.TaskSerializer.content_type","text":"","title":"content_type"},{"location":"reference/tasks/serializers/#tasks.serializers.TaskSerializer.creator","text":"","title":"creator"},{"location":"reference/tasks/serializers/#tasks.serializers.TaskSerializer.Meta","text":"Source code in tasks/serializers.py 18 19 20 class Meta : model = Task exclude = []","title":"Meta"},{"location":"reference/tasks/serializers/#tasks.serializers.TaskSerializer.Meta.exclude","text":"","title":"exclude"},{"location":"reference/tasks/serializers/#tasks.serializers.TaskSerializer.Meta.model","text":"","title":"model"},{"location":"reference/tasks/signals/","text":"logger = get_logger () module-attribute handle_after_task_publish ( sender = None , body = None , headers = None , ** kwargs ) Source code in tasks/signals.py 13 14 15 16 17 18 19 20 21 @signals . after_task_publish . connect def handle_after_task_publish ( sender = None , body = None , headers = None , ** kwargs ): if headers and 'id' in headers : try : instance = Task . objects . get ( task_id = headers [ 'id' ]) instance . state = TaskState . PENDING instance . save () except ObjectDoesNotExist : pass handle_task_failure ( sender = None , task_id = None , ** kwargs ) Source code in tasks/signals.py 48 49 50 51 52 53 54 55 56 @signals . task_failure . connect def handle_task_failure ( sender = None , task_id = None , ** kwargs ): if task_id : try : instance = Task . objects . get ( task_id = task_id ) instance . state = TaskState . FAILURE instance . save () except ObjectDoesNotExist : pass handle_task_postrun ( sender = None , task_id = None , state = None , ** kwargs ) Source code in tasks/signals.py 37 38 39 40 41 42 43 44 45 @signals . task_postrun . connect def handle_task_postrun ( sender = None , task_id = None , state = None , ** kwargs ): if task_id and state : try : instance = Task . objects . get ( task_id = task_id ) instance . state = TaskState . lookup ( state ) instance . save () except ObjectDoesNotExist as e : logger . exception ( e ) handle_task_prerun ( sender = None , task_id = None , ** kwargs ) Source code in tasks/signals.py 24 25 26 27 28 29 30 31 32 33 34 @signals . task_prerun . connect def handle_task_prerun ( sender = None , task_id = None , ** kwargs ): if not task_id : return try : instance = Task . objects . get ( task_id = task_id ) instance . state = TaskState . STARTED instance . save () except ObjectDoesNotExist : pass handle_task_revoked ( sender = None , request = None , ** kwargs ) Source code in tasks/signals.py 59 60 61 62 63 64 65 66 @signals . task_revoked . connect def handle_task_revoked ( sender = None , request = None , ** kwargs ): if request and request . id : try : instance = Task . objects . get ( task_id = request . id ) instance . delete () except ObjectDoesNotExist : pass handle_task_update ( sender , instance , created , ** kwargs ) Source code in tasks/signals.py 69 70 71 72 73 74 @receiver ( post_save , sender = Task ) def handle_task_update ( sender , instance , created , ** kwargs ): send_to_group_sync ( str ( 'TASKS_GLOBAL' ), { 'type' : 'task_updated' , 'message' : str ( instance . task_id ) })","title":"signals"},{"location":"reference/tasks/signals/#tasks.signals.logger","text":"","title":"logger"},{"location":"reference/tasks/signals/#tasks.signals.handle_after_task_publish","text":"Source code in tasks/signals.py 13 14 15 16 17 18 19 20 21 @signals . after_task_publish . connect def handle_after_task_publish ( sender = None , body = None , headers = None , ** kwargs ): if headers and 'id' in headers : try : instance = Task . objects . get ( task_id = headers [ 'id' ]) instance . state = TaskState . PENDING instance . save () except ObjectDoesNotExist : pass","title":"handle_after_task_publish()"},{"location":"reference/tasks/signals/#tasks.signals.handle_task_failure","text":"Source code in tasks/signals.py 48 49 50 51 52 53 54 55 56 @signals . task_failure . connect def handle_task_failure ( sender = None , task_id = None , ** kwargs ): if task_id : try : instance = Task . objects . get ( task_id = task_id ) instance . state = TaskState . FAILURE instance . save () except ObjectDoesNotExist : pass","title":"handle_task_failure()"},{"location":"reference/tasks/signals/#tasks.signals.handle_task_postrun","text":"Source code in tasks/signals.py 37 38 39 40 41 42 43 44 45 @signals . task_postrun . connect def handle_task_postrun ( sender = None , task_id = None , state = None , ** kwargs ): if task_id and state : try : instance = Task . objects . get ( task_id = task_id ) instance . state = TaskState . lookup ( state ) instance . save () except ObjectDoesNotExist as e : logger . exception ( e )","title":"handle_task_postrun()"},{"location":"reference/tasks/signals/#tasks.signals.handle_task_prerun","text":"Source code in tasks/signals.py 24 25 26 27 28 29 30 31 32 33 34 @signals . task_prerun . connect def handle_task_prerun ( sender = None , task_id = None , ** kwargs ): if not task_id : return try : instance = Task . objects . get ( task_id = task_id ) instance . state = TaskState . STARTED instance . save () except ObjectDoesNotExist : pass","title":"handle_task_prerun()"},{"location":"reference/tasks/signals/#tasks.signals.handle_task_revoked","text":"Source code in tasks/signals.py 59 60 61 62 63 64 65 66 @signals . task_revoked . connect def handle_task_revoked ( sender = None , request = None , ** kwargs ): if request and request . id : try : instance = Task . objects . get ( task_id = request . id ) instance . delete () except ObjectDoesNotExist : pass","title":"handle_task_revoked()"},{"location":"reference/tasks/signals/#tasks.signals.handle_task_update","text":"Source code in tasks/signals.py 69 70 71 72 73 74 @receiver ( post_save , sender = Task ) def handle_task_update ( sender , instance , created , ** kwargs ): send_to_group_sync ( str ( 'TASKS_GLOBAL' ), { 'type' : 'task_updated' , 'message' : str ( instance . task_id ) })","title":"handle_task_update()"},{"location":"reference/tasks/utils/","text":"send_to_channel_sync ( cur , group_key , message , expire , channel = None ) Source code in tasks/utils.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def send_to_channel_sync ( cur , group_key , message , expire , channel = None ): if channel is None : channels = _retrieve_group_channels ( cur , group_key ) else : channels = [ channel ] values_str = b ',' . join ( cur . mogrify ( \"( %s , %s , (NOW() + INTERVAL ' %s seconds'))\" , ( channel , message , expire ) ) for channel in channels ) insert_message_sql = ( b 'INSERT INTO channels_postgres_message (channel, message, expire) VALUES ' + values_str ) cur . execute ( insert_message_sql ) send_to_group_sync ( group , message ) Source code in tasks/utils.py 38 39 40 41 42 43 44 45 46 47 48 def send_to_group_sync ( group , message ): channel_layer : PostgresChannelLayer = get_channel_layer () assert channel_layer . valid_group_name ( group ), \"Group name not valid\" group_key = channel_layer . _group_key ( group ) message = channel_layer . serialize ( message ) cur = connection . cursor () send_to_channel_sync ( cur , group_key , message , channel_layer . expiry )","title":"utils"},{"location":"reference/tasks/utils/#tasks.utils.send_to_channel_sync","text":"Source code in tasks/utils.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def send_to_channel_sync ( cur , group_key , message , expire , channel = None ): if channel is None : channels = _retrieve_group_channels ( cur , group_key ) else : channels = [ channel ] values_str = b ',' . join ( cur . mogrify ( \"( %s , %s , (NOW() + INTERVAL ' %s seconds'))\" , ( channel , message , expire ) ) for channel in channels ) insert_message_sql = ( b 'INSERT INTO channels_postgres_message (channel, message, expire) VALUES ' + values_str ) cur . execute ( insert_message_sql )","title":"send_to_channel_sync()"},{"location":"reference/tasks/utils/#tasks.utils.send_to_group_sync","text":"Source code in tasks/utils.py 38 39 40 41 42 43 44 45 46 47 48 def send_to_group_sync ( group , message ): channel_layer : PostgresChannelLayer = get_channel_layer () assert channel_layer . valid_group_name ( group ), \"Group name not valid\" group_key = channel_layer . _group_key ( group ) message = channel_layer . serialize ( message ) cur = connection . cursor () send_to_channel_sync ( cur , group_key , message , channel_layer . expiry )","title":"send_to_group_sync()"},{"location":"reference/tasks/views/","text":"ListFilter Bases: django_filters . Filter Source code in tasks/views.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class ListFilter ( django_filters . Filter ): def __init__ ( self , filter_value = lambda x : x , ** kwargs ): super ( ListFilter , self ) . __init__ ( ** kwargs ) self . filter_value_fn = filter_value def sanitize ( self , value_list ): return [ v for v in value_list if v != u '' ] def filter ( self , qs , value ): if value : values = value . split ( u \",\" ) values = self . sanitize ( values ) values = map ( self . filter_value_fn , values ) f = Q () for v in values : kwargs = { self . field_name : v } f = f | Q ( ** kwargs ) return qs . filter ( f ) else : return qs filter_value_fn = filter_value instance-attribute __init__ ( filter_value = lambda x : x , ** kwargs ) Source code in tasks/views.py 13 14 15 def __init__ ( self , filter_value = lambda x : x , ** kwargs ): super ( ListFilter , self ) . __init__ ( ** kwargs ) self . filter_value_fn = filter_value filter ( qs , value ) Source code in tasks/views.py 20 21 22 23 24 25 26 27 28 29 30 31 def filter ( self , qs , value ): if value : values = value . split ( u \",\" ) values = self . sanitize ( values ) values = map ( self . filter_value_fn , values ) f = Q () for v in values : kwargs = { self . field_name : v } f = f | Q ( ** kwargs ) return qs . filter ( f ) else : return qs sanitize ( value_list ) Source code in tasks/views.py 17 18 def sanitize ( self , value_list ): return [ v for v in value_list if v != u '' ] TaskFilter Bases: django_filters . FilterSet Source code in tasks/views.py 34 35 36 37 38 39 class TaskFilter ( django_filters . FilterSet ): state__in = ListFilter ( field_name = \"state\" ) class Meta : model = Task fields = [ 'state' , 'created_at' ] state__in = ListFilter ( field_name = 'state' ) class-attribute Meta Source code in tasks/views.py 37 38 39 class Meta : model = Task fields = [ 'state' , 'created_at' ] fields = [ 'state' , 'created_at' ] class-attribute model = Task class-attribute TaskViewSet Bases: viewsets . ModelViewSet API endpoint that allows users to be viewed or edited. Source code in tasks/views.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class TaskViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = Task . objects . all () serializer_class = TaskSerializer pagination_class = LimitOffsetPagination filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] search_fields = [ 'object_id' , 'name' , 'state' ] filter_class = TaskFilter def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) ) filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] class-attribute filter_class = TaskFilter class-attribute pagination_class = LimitOffsetPagination class-attribute queryset = Task . objects . all () class-attribute search_fields = [ 'object_id' , 'name' , 'state' ] class-attribute serializer_class = TaskSerializer class-attribute get_queryset () Source code in tasks/views.py 53 54 55 56 57 58 59 def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) )","title":"views"},{"location":"reference/tasks/views/#tasks.views.ListFilter","text":"Bases: django_filters . Filter Source code in tasks/views.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class ListFilter ( django_filters . Filter ): def __init__ ( self , filter_value = lambda x : x , ** kwargs ): super ( ListFilter , self ) . __init__ ( ** kwargs ) self . filter_value_fn = filter_value def sanitize ( self , value_list ): return [ v for v in value_list if v != u '' ] def filter ( self , qs , value ): if value : values = value . split ( u \",\" ) values = self . sanitize ( values ) values = map ( self . filter_value_fn , values ) f = Q () for v in values : kwargs = { self . field_name : v } f = f | Q ( ** kwargs ) return qs . filter ( f ) else : return qs","title":"ListFilter"},{"location":"reference/tasks/views/#tasks.views.ListFilter.filter_value_fn","text":"","title":"filter_value_fn"},{"location":"reference/tasks/views/#tasks.views.ListFilter.__init__","text":"Source code in tasks/views.py 13 14 15 def __init__ ( self , filter_value = lambda x : x , ** kwargs ): super ( ListFilter , self ) . __init__ ( ** kwargs ) self . filter_value_fn = filter_value","title":"__init__()"},{"location":"reference/tasks/views/#tasks.views.ListFilter.filter","text":"Source code in tasks/views.py 20 21 22 23 24 25 26 27 28 29 30 31 def filter ( self , qs , value ): if value : values = value . split ( u \",\" ) values = self . sanitize ( values ) values = map ( self . filter_value_fn , values ) f = Q () for v in values : kwargs = { self . field_name : v } f = f | Q ( ** kwargs ) return qs . filter ( f ) else : return qs","title":"filter()"},{"location":"reference/tasks/views/#tasks.views.ListFilter.sanitize","text":"Source code in tasks/views.py 17 18 def sanitize ( self , value_list ): return [ v for v in value_list if v != u '' ]","title":"sanitize()"},{"location":"reference/tasks/views/#tasks.views.TaskFilter","text":"Bases: django_filters . FilterSet Source code in tasks/views.py 34 35 36 37 38 39 class TaskFilter ( django_filters . FilterSet ): state__in = ListFilter ( field_name = \"state\" ) class Meta : model = Task fields = [ 'state' , 'created_at' ]","title":"TaskFilter"},{"location":"reference/tasks/views/#tasks.views.TaskFilter.state__in","text":"","title":"state__in"},{"location":"reference/tasks/views/#tasks.views.TaskFilter.Meta","text":"Source code in tasks/views.py 37 38 39 class Meta : model = Task fields = [ 'state' , 'created_at' ]","title":"Meta"},{"location":"reference/tasks/views/#tasks.views.TaskFilter.Meta.fields","text":"","title":"fields"},{"location":"reference/tasks/views/#tasks.views.TaskFilter.Meta.model","text":"","title":"model"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet","text":"Bases: viewsets . ModelViewSet API endpoint that allows users to be viewed or edited. Source code in tasks/views.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class TaskViewSet ( viewsets . ModelViewSet ): \"\"\" API endpoint that allows users to be viewed or edited. \"\"\" queryset = Task . objects . all () serializer_class = TaskSerializer pagination_class = LimitOffsetPagination filter_backends = [ DjangoFilterBackend , filters . SearchFilter , filters . OrderingFilter ] search_fields = [ 'object_id' , 'name' , 'state' ] filter_class = TaskFilter def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) )","title":"TaskViewSet"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.filter_backends","text":"","title":"filter_backends"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.filter_class","text":"","title":"filter_class"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.pagination_class","text":"","title":"pagination_class"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.queryset","text":"","title":"queryset"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.search_fields","text":"","title":"search_fields"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.serializer_class","text":"","title":"serializer_class"},{"location":"reference/tasks/views/#tasks.views.TaskViewSet.get_queryset","text":"Source code in tasks/views.py 53 54 55 56 57 58 59 def get_queryset ( self ): if self . request . user . is_superuser : return super () . get_queryset () return super () . get_queryset () . filter ( Q ( creator = self . request . user ) )","title":"get_queryset()"},{"location":"reference/tasks/management/","text":"","title":"management"},{"location":"reference/tasks/management/commands/","text":"","title":"commands"},{"location":"reference/tasks/management/commands/celery_worker/","text":"Command Bases: BaseCommand Source code in tasks/management/commands/celery_worker.py 15 16 17 18 class Command ( BaseCommand ): def handle ( self , * args , ** options ): print ( 'Starting celery worker with autoreload...' ) autoreload . run_with_reloader ( restart_celery ) handle ( * args , ** options ) Source code in tasks/management/commands/celery_worker.py 16 17 18 def handle ( self , * args , ** options ): print ( 'Starting celery worker with autoreload...' ) autoreload . run_with_reloader ( restart_celery ) restart_celery () Source code in tasks/management/commands/celery_worker.py 8 9 10 11 12 def restart_celery (): cmd = 'pkill -f \"celery worker\"' subprocess . call ( shlex . split ( cmd )) cmd = 'celery -A backend worker -l info' subprocess . call ( shlex . split ( cmd ))","title":"celery_worker"},{"location":"reference/tasks/management/commands/celery_worker/#tasks.management.commands.celery_worker.Command","text":"Bases: BaseCommand Source code in tasks/management/commands/celery_worker.py 15 16 17 18 class Command ( BaseCommand ): def handle ( self , * args , ** options ): print ( 'Starting celery worker with autoreload...' ) autoreload . run_with_reloader ( restart_celery )","title":"Command"},{"location":"reference/tasks/management/commands/celery_worker/#tasks.management.commands.celery_worker.Command.handle","text":"Source code in tasks/management/commands/celery_worker.py 16 17 18 def handle ( self , * args , ** options ): print ( 'Starting celery worker with autoreload...' ) autoreload . run_with_reloader ( restart_celery )","title":"handle()"},{"location":"reference/tasks/management/commands/celery_worker/#tasks.management.commands.celery_worker.restart_celery","text":"Source code in tasks/management/commands/celery_worker.py 8 9 10 11 12 def restart_celery (): cmd = 'pkill -f \"celery worker\"' subprocess . call ( shlex . split ( cmd )) cmd = 'celery -A backend worker -l info' subprocess . call ( shlex . split ( cmd ))","title":"restart_celery()"},{"location":"reference/users/","text":"","title":"users"},{"location":"reference/users/middleware/","text":"JwtAuthMiddleware Bases: BaseMiddleware Source code in users/middleware.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class JwtAuthMiddleware ( BaseMiddleware ): def __init__ ( self , inner ): self . inner = inner async def __call__ ( self , scope , receive , send ): # Close old database connections to prevent usage of timed out connections close_old_connections () # Get the token token = parse_qs ( scope [ \"query_string\" ] . decode ( \"utf8\" ))[ \"token\" ][ 0 ] # Try to authenticate the user try : # This will automatically validate the token and raise an error if token is invalid UntypedToken ( token ) except ( InvalidToken , TokenError ) as e : # Token is invalid print ( e ) return None else : # Then token is valid, decode it decoded_data = jwt_decode ( token , settings . SECRET_KEY , algorithms = [ \"HS256\" ]) # Get the user using ID scope [ \"user\" ] = await get_user ( validated_token = decoded_data ) return await super () . __call__ ( scope , receive , send ) inner = inner instance-attribute __call__ ( scope , receive , send ) async Source code in users/middleware.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 async def __call__ ( self , scope , receive , send ): # Close old database connections to prevent usage of timed out connections close_old_connections () # Get the token token = parse_qs ( scope [ \"query_string\" ] . decode ( \"utf8\" ))[ \"token\" ][ 0 ] # Try to authenticate the user try : # This will automatically validate the token and raise an error if token is invalid UntypedToken ( token ) except ( InvalidToken , TokenError ) as e : # Token is invalid print ( e ) return None else : # Then token is valid, decode it decoded_data = jwt_decode ( token , settings . SECRET_KEY , algorithms = [ \"HS256\" ]) # Get the user using ID scope [ \"user\" ] = await get_user ( validated_token = decoded_data ) return await super () . __call__ ( scope , receive , send ) __init__ ( inner ) Source code in users/middleware.py 26 27 def __init__ ( self , inner ): self . inner = inner JwtAuthMiddlewareStack ( inner ) Source code in users/middleware.py 53 54 def JwtAuthMiddlewareStack ( inner ): return JwtAuthMiddleware ( AuthMiddlewareStack ( inner )) get_user ( validated_token ) Source code in users/middleware.py 15 16 17 18 19 20 21 22 @database_sync_to_async def get_user ( validated_token ): try : user = get_user_model () . objects . get ( id = validated_token [ \"user_id\" ]) return user except User . DoesNotExist : return AnonymousUser ()","title":"middleware"},{"location":"reference/users/middleware/#users.middleware.JwtAuthMiddleware","text":"Bases: BaseMiddleware Source code in users/middleware.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class JwtAuthMiddleware ( BaseMiddleware ): def __init__ ( self , inner ): self . inner = inner async def __call__ ( self , scope , receive , send ): # Close old database connections to prevent usage of timed out connections close_old_connections () # Get the token token = parse_qs ( scope [ \"query_string\" ] . decode ( \"utf8\" ))[ \"token\" ][ 0 ] # Try to authenticate the user try : # This will automatically validate the token and raise an error if token is invalid UntypedToken ( token ) except ( InvalidToken , TokenError ) as e : # Token is invalid print ( e ) return None else : # Then token is valid, decode it decoded_data = jwt_decode ( token , settings . SECRET_KEY , algorithms = [ \"HS256\" ]) # Get the user using ID scope [ \"user\" ] = await get_user ( validated_token = decoded_data ) return await super () . __call__ ( scope , receive , send )","title":"JwtAuthMiddleware"},{"location":"reference/users/middleware/#users.middleware.JwtAuthMiddleware.inner","text":"","title":"inner"},{"location":"reference/users/middleware/#users.middleware.JwtAuthMiddleware.__call__","text":"Source code in users/middleware.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 async def __call__ ( self , scope , receive , send ): # Close old database connections to prevent usage of timed out connections close_old_connections () # Get the token token = parse_qs ( scope [ \"query_string\" ] . decode ( \"utf8\" ))[ \"token\" ][ 0 ] # Try to authenticate the user try : # This will automatically validate the token and raise an error if token is invalid UntypedToken ( token ) except ( InvalidToken , TokenError ) as e : # Token is invalid print ( e ) return None else : # Then token is valid, decode it decoded_data = jwt_decode ( token , settings . SECRET_KEY , algorithms = [ \"HS256\" ]) # Get the user using ID scope [ \"user\" ] = await get_user ( validated_token = decoded_data ) return await super () . __call__ ( scope , receive , send )","title":"__call__()"},{"location":"reference/users/middleware/#users.middleware.JwtAuthMiddleware.__init__","text":"Source code in users/middleware.py 26 27 def __init__ ( self , inner ): self . inner = inner","title":"__init__()"},{"location":"reference/users/middleware/#users.middleware.JwtAuthMiddlewareStack","text":"Source code in users/middleware.py 53 54 def JwtAuthMiddlewareStack ( inner ): return JwtAuthMiddleware ( AuthMiddlewareStack ( inner ))","title":"JwtAuthMiddlewareStack()"},{"location":"reference/users/middleware/#users.middleware.get_user","text":"Source code in users/middleware.py 15 16 17 18 19 20 21 22 @database_sync_to_async def get_user ( validated_token ): try : user = get_user_model () . objects . get ( id = validated_token [ \"user_id\" ]) return user except User . DoesNotExist : return AnonymousUser ()","title":"get_user()"},{"location":"reference/users/mixins/","text":"OwnableMixin Source code in users/mixins.py 4 5 6 7 8 9 class OwnableMixin : def can_edit ( self , user : User ): return user . is_superuser def can_view ( self , user : User ): return user . is_superuser can_edit ( user ) Source code in users/mixins.py 5 6 def can_edit ( self , user : User ): return user . is_superuser can_view ( user ) Source code in users/mixins.py 8 9 def can_view ( self , user : User ): return user . is_superuser","title":"mixins"},{"location":"reference/users/mixins/#users.mixins.OwnableMixin","text":"Source code in users/mixins.py 4 5 6 7 8 9 class OwnableMixin : def can_edit ( self , user : User ): return user . is_superuser def can_view ( self , user : User ): return user . is_superuser","title":"OwnableMixin"},{"location":"reference/users/mixins/#users.mixins.OwnableMixin.can_edit","text":"Source code in users/mixins.py 5 6 def can_edit ( self , user : User ): return user . is_superuser","title":"can_edit()"},{"location":"reference/users/mixins/#users.mixins.OwnableMixin.can_view","text":"Source code in users/mixins.py 8 9 def can_view ( self , user : User ): return user . is_superuser","title":"can_view()"},{"location":"reference/users/permissions/","text":"IsOwner Bases: permissions . BasePermission Custom permission to only allow owners of an object to edit it. Source code in users/permissions.py 4 5 6 7 8 9 10 11 12 13 class IsOwner ( permissions . BasePermission ): \"\"\" Custom permission to only allow owners of an object to edit it. \"\"\" def has_permission ( self , request , view ): return request . user and request . user . is_authenticated def has_object_permission ( self , request , view , obj ): return obj . creator == request . user or request . user . is_superuser has_object_permission ( request , view , obj ) Source code in users/permissions.py 12 13 def has_object_permission ( self , request , view , obj ): return obj . creator == request . user or request . user . is_superuser has_permission ( request , view ) Source code in users/permissions.py 9 10 def has_permission ( self , request , view ): return request . user and request . user . is_authenticated","title":"permissions"},{"location":"reference/users/permissions/#users.permissions.IsOwner","text":"Bases: permissions . BasePermission Custom permission to only allow owners of an object to edit it. Source code in users/permissions.py 4 5 6 7 8 9 10 11 12 13 class IsOwner ( permissions . BasePermission ): \"\"\" Custom permission to only allow owners of an object to edit it. \"\"\" def has_permission ( self , request , view ): return request . user and request . user . is_authenticated def has_object_permission ( self , request , view , obj ): return obj . creator == request . user or request . user . is_superuser","title":"IsOwner"},{"location":"reference/users/permissions/#users.permissions.IsOwner.has_object_permission","text":"Source code in users/permissions.py 12 13 def has_object_permission ( self , request , view , obj ): return obj . creator == request . user or request . user . is_superuser","title":"has_object_permission()"},{"location":"reference/users/permissions/#users.permissions.IsOwner.has_permission","text":"Source code in users/permissions.py 9 10 def has_permission ( self , request , view ): return request . user and request . user . is_authenticated","title":"has_permission()"},{"location":"reference/users/serializers/","text":"MyTokenObtainPairSerializer Bases: TokenObtainPairSerializer Source code in users/serializers.py 18 19 20 21 22 23 24 25 26 27 28 29 class MyTokenObtainPairSerializer ( TokenObtainPairSerializer ): @classmethod def get_token ( cls , user ): token = super () . get_token ( user ) # Add custom claims token [ 'username' ] = user . username token [ 'email' ] = user . email token [ 'name' ] = f ' { user . first_name } { user . last_name } ' if user . first_name else user . username token [ 'group' ] = user . groups . first () . name if user . groups . first () else 'Researcher' token [ 'user_id' ] = user . id return token get_token ( user ) classmethod Source code in users/serializers.py 19 20 21 22 23 24 25 26 27 28 29 @classmethod def get_token ( cls , user ): token = super () . get_token ( user ) # Add custom claims token [ 'username' ] = user . username token [ 'email' ] = user . email token [ 'name' ] = f ' { user . first_name } { user . last_name } ' if user . first_name else user . username token [ 'group' ] = user . groups . first () . name if user . groups . first () else 'Researcher' token [ 'user_id' ] = user . id return token ShortUserSerializer Bases: serializers . ModelSerializer Source code in users/serializers.py 12 13 14 15 class ShortUserSerializer ( serializers . ModelSerializer ): class Meta : model = User fields = [ 'username' , 'id' ] Meta Source code in users/serializers.py 13 14 15 class Meta : model = User fields = [ 'username' , 'id' ] fields = [ 'username' , 'id' ] class-attribute model = User class-attribute UserSerializer Bases: serializers . HyperlinkedModelSerializer Source code in users/serializers.py 6 7 8 9 class UserSerializer ( serializers . HyperlinkedModelSerializer ): class Meta : model = User fields = [ 'username' , 'email' ] Meta Source code in users/serializers.py 7 8 9 class Meta : model = User fields = [ 'username' , 'email' ] fields = [ 'username' , 'email' ] class-attribute model = User class-attribute","title":"serializers"},{"location":"reference/users/serializers/#users.serializers.MyTokenObtainPairSerializer","text":"Bases: TokenObtainPairSerializer Source code in users/serializers.py 18 19 20 21 22 23 24 25 26 27 28 29 class MyTokenObtainPairSerializer ( TokenObtainPairSerializer ): @classmethod def get_token ( cls , user ): token = super () . get_token ( user ) # Add custom claims token [ 'username' ] = user . username token [ 'email' ] = user . email token [ 'name' ] = f ' { user . first_name } { user . last_name } ' if user . first_name else user . username token [ 'group' ] = user . groups . first () . name if user . groups . first () else 'Researcher' token [ 'user_id' ] = user . id return token","title":"MyTokenObtainPairSerializer"},{"location":"reference/users/serializers/#users.serializers.MyTokenObtainPairSerializer.get_token","text":"Source code in users/serializers.py 19 20 21 22 23 24 25 26 27 28 29 @classmethod def get_token ( cls , user ): token = super () . get_token ( user ) # Add custom claims token [ 'username' ] = user . username token [ 'email' ] = user . email token [ 'name' ] = f ' { user . first_name } { user . last_name } ' if user . first_name else user . username token [ 'group' ] = user . groups . first () . name if user . groups . first () else 'Researcher' token [ 'user_id' ] = user . id return token","title":"get_token()"},{"location":"reference/users/serializers/#users.serializers.ShortUserSerializer","text":"Bases: serializers . ModelSerializer Source code in users/serializers.py 12 13 14 15 class ShortUserSerializer ( serializers . ModelSerializer ): class Meta : model = User fields = [ 'username' , 'id' ]","title":"ShortUserSerializer"},{"location":"reference/users/serializers/#users.serializers.ShortUserSerializer.Meta","text":"Source code in users/serializers.py 13 14 15 class Meta : model = User fields = [ 'username' , 'id' ]","title":"Meta"},{"location":"reference/users/serializers/#users.serializers.ShortUserSerializer.Meta.fields","text":"","title":"fields"},{"location":"reference/users/serializers/#users.serializers.ShortUserSerializer.Meta.model","text":"","title":"model"},{"location":"reference/users/serializers/#users.serializers.UserSerializer","text":"Bases: serializers . HyperlinkedModelSerializer Source code in users/serializers.py 6 7 8 9 class UserSerializer ( serializers . HyperlinkedModelSerializer ): class Meta : model = User fields = [ 'username' , 'email' ]","title":"UserSerializer"},{"location":"reference/users/serializers/#users.serializers.UserSerializer.Meta","text":"Source code in users/serializers.py 7 8 9 class Meta : model = User fields = [ 'username' , 'email' ]","title":"Meta"},{"location":"reference/users/serializers/#users.serializers.UserSerializer.Meta.fields","text":"","title":"fields"},{"location":"reference/users/serializers/#users.serializers.UserSerializer.Meta.model","text":"","title":"model"},{"location":"reference/users/views/","text":"MyTokenObtainPairView Bases: TokenObtainPairView Source code in users/views.py 5 6 class MyTokenObtainPairView ( TokenObtainPairView ): serializer_class = MyTokenObtainPairSerializer serializer_class = MyTokenObtainPairSerializer class-attribute","title":"views"},{"location":"reference/users/views/#users.views.MyTokenObtainPairView","text":"Bases: TokenObtainPairView Source code in users/views.py 5 6 class MyTokenObtainPairView ( TokenObtainPairView ): serializer_class = MyTokenObtainPairSerializer","title":"MyTokenObtainPairView"},{"location":"reference/users/views/#users.views.MyTokenObtainPairView.serializer_class","text":"","title":"serializer_class"}]}